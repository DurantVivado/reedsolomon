// Code generated by command: go run gen.go -out galois_gen_amd64.s -stubs galois_gen_amd64.go. DO NOT EDIT.

// +build !appengine
// +build !noasm
// +build gc

// func mulAvxTwo_1x1(low [1][16]byte, high [1][16]byte, in [1][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x1(SB), $0-80
	// Loading all tables to registers
	// Full registers estimated 6 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+40(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x1_end
	MOVQ         out_0_base+56(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+16(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVQ         in_0_base+32(FP), DX
	MOVQ         $0x0000000f, BX
	MOVQ         BX, X3
	VPBROADCASTB X3, Y3
	XORQ         BX, BX

mulAvxTwo_1x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(BX*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y3, Y4, Y4
	VPAND   Y3, Y5, Y5
	VPSHUFB Y4, Y1, Y4
	VPSHUFB Y5, Y2, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(BX*1)

	// Prepare for next loop
	ADDQ $0x20, BX
	DECQ AX
	JNZ  mulAvxTwo_1x1_loop
	VZEROUPPER

mulAvxTwo_1x1_end:
	RET

// func mulAvxTwo_1x2(low [2][16]byte, high [2][16]byte, in [1][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x2(SB), $0-136
	// Loading all tables to registers
	// Full registers estimated 11 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+72(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x2_end
	MOVQ         out_0_base+88(FP), CX
	MOVQ         out_1_base+112(FP), DX
	MOVOU        low_0+0(FP), X2
	MOVOU        high_0+32(FP), X3
	VINSERTI128  $0x01, X2, Y2, Y2
	VINSERTI128  $0x01, X3, Y3, Y3
	MOVOU        low_1+16(FP), X4
	MOVOU        high_1+48(FP), X5
	VINSERTI128  $0x01, X4, Y4, Y4
	VINSERTI128  $0x01, X5, Y5, Y5
	MOVQ         in_0_base+64(FP), BX
	MOVQ         $0x0000000f, BP
	MOVQ         BP, X6
	VPBROADCASTB X6, Y6
	XORQ         BP, BP

mulAvxTwo_1x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(BP*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VPSHUFB Y9, Y2, Y7
	VPSHUFB Y10, Y3, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VPSHUFB Y9, Y4, Y7
	VPSHUFB Y10, Y5, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(BP*1)
	VMOVDQU Y1, (DX)(BP*1)

	// Prepare for next loop
	ADDQ $0x20, BP
	DECQ AX
	JNZ  mulAvxTwo_1x2_loop
	VZEROUPPER

mulAvxTwo_1x2_end:
	RET

// func mulAvxTwo_1x3(low [3][16]byte, high [3][16]byte, in [1][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x3(SB), $0-192
	// Loading all tables to registers
	// Full registers estimated 14 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x3_end
	MOVQ         out_0_base+120(FP), CX
	MOVQ         out_1_base+144(FP), DX
	MOVQ         out_2_base+168(FP), BX
	MOVOU        low_0+0(FP), X3
	MOVOU        high_0+48(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_1+16(FP), X5
	MOVOU        high_1+64(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_2+32(FP), X7
	MOVOU        high_2+80(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVQ         in_0_base+96(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X9
	VPBROADCASTB X9, Y9
	XORQ         SI, SI

mulAvxTwo_1x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(SI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VPSHUFB Y12, Y3, Y10
	VPSHUFB Y13, Y4, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VPSHUFB Y12, Y5, Y10
	VPSHUFB Y13, Y6, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VPSHUFB Y12, Y7, Y10
	VPSHUFB Y13, Y8, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(SI*1)
	VMOVDQU Y1, (DX)(SI*1)
	VMOVDQU Y2, (BX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_1x3_loop
	VZEROUPPER

mulAvxTwo_1x3_end:
	RET

// func mulAvxTwo_1x4(low [8][16]byte, high [8][16]byte, in [1][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x4(SB), $0-376
	// Loading no tables to registers
	// Full registers estimated 17 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+264(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x4_end
	MOVQ         out_0_base+280(FP), CX
	MOVQ         out_1_base+304(FP), DX
	MOVQ         out_2_base+328(FP), BX
	MOVQ         out_3_base+352(FP), BP
	MOVQ         in_0_base+256(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X4
	VPBROADCASTB X4, Y4
	XORQ         DI, DI

mulAvxTwo_1x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(DI*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+128(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+160(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+192(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+224(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(DI*1)
	VMOVDQU Y1, (DX)(DI*1)
	VMOVDQU Y2, (BX)(DI*1)
	VMOVDQU Y3, (BP)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_1x4_loop
	VZEROUPPER

mulAvxTwo_1x4_end:
	RET

// func mulAvxTwo_1x5(low [10][16]byte, high [10][16]byte, in [1][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x5(SB), $0-464
	// Loading no tables to registers
	// Full registers estimated 20 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+328(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x5_end
	MOVQ         out_0_base+344(FP), CX
	MOVQ         out_1_base+368(FP), DX
	MOVQ         out_2_base+392(FP), BX
	MOVQ         out_3_base+416(FP), BP
	MOVQ         out_4_base+440(FP), SI
	MOVQ         in_0_base+320(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X5
	VPBROADCASTB X5, Y5
	XORQ         R8, R8

mulAvxTwo_1x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R8*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+160(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+192(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+224(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+256(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+288(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R8*1)
	VMOVDQU Y1, (DX)(R8*1)
	VMOVDQU Y2, (BX)(R8*1)
	VMOVDQU Y3, (BP)(R8*1)
	VMOVDQU Y4, (SI)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_1x5_loop
	VZEROUPPER

mulAvxTwo_1x5_end:
	RET

// func mulAvxTwo_1x6(low [12][16]byte, high [12][16]byte, in [1][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x6(SB), $0-552
	// Loading no tables to registers
	// Full registers estimated 23 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+392(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x6_end
	MOVQ         out_0_base+408(FP), CX
	MOVQ         out_1_base+432(FP), DX
	MOVQ         out_2_base+456(FP), BX
	MOVQ         out_3_base+480(FP), BP
	MOVQ         out_4_base+504(FP), SI
	MOVQ         out_5_base+528(FP), DI
	MOVQ         in_0_base+384(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X6
	VPBROADCASTB X6, Y6
	XORQ         R9, R9

mulAvxTwo_1x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R9*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+192(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+224(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+256(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+288(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+320(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+352(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R9*1)
	VMOVDQU Y1, (DX)(R9*1)
	VMOVDQU Y2, (BX)(R9*1)
	VMOVDQU Y3, (BP)(R9*1)
	VMOVDQU Y4, (SI)(R9*1)
	VMOVDQU Y5, (DI)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_1x6_loop
	VZEROUPPER

mulAvxTwo_1x6_end:
	RET

// func mulAvxTwo_1x7(low [14][16]byte, high [14][16]byte, in [1][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x7(SB), $0-640
	// Loading no tables to registers
	// Full registers estimated 26 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+456(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x7_end
	MOVQ         out_0_base+472(FP), CX
	MOVQ         out_1_base+496(FP), DX
	MOVQ         out_2_base+520(FP), BX
	MOVQ         out_3_base+544(FP), BP
	MOVQ         out_4_base+568(FP), SI
	MOVQ         out_5_base+592(FP), DI
	MOVQ         out_6_base+616(FP), R8
	MOVQ         in_0_base+448(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X7
	VPBROADCASTB X7, Y7
	XORQ         R10, R10

mulAvxTwo_1x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (R9)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+224(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+256(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+288(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+320(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+352(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+384(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+416(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	VMOVDQU Y0, (CX)(R10*1)
	VMOVDQU Y1, (DX)(R10*1)
	VMOVDQU Y2, (BX)(R10*1)
	VMOVDQU Y3, (BP)(R10*1)
	VMOVDQU Y4, (SI)(R10*1)
	VMOVDQU Y5, (DI)(R10*1)
	VMOVDQU Y6, (R8)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_1x7_loop
	VZEROUPPER

mulAvxTwo_1x7_end:
	RET

// func mulAvxTwo_1x8(low [16][16]byte, high [16][16]byte, in [1][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x8(SB), $0-728
	// Loading no tables to registers
	// Full registers estimated 29 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+520(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x8_end
	MOVQ         out_0_base+536(FP), CX
	MOVQ         out_1_base+560(FP), DX
	MOVQ         out_2_base+584(FP), BX
	MOVQ         out_3_base+608(FP), BP
	MOVQ         out_4_base+632(FP), SI
	MOVQ         out_5_base+656(FP), DI
	MOVQ         out_6_base+680(FP), R8
	MOVQ         out_7_base+704(FP), R9
	MOVQ         in_0_base+512(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X8
	VPBROADCASTB X8, Y8
	XORQ         R11, R11

mulAvxTwo_1x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (R10)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+256(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+288(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+320(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+352(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+384(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+416(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+448(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+480(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)
	VMOVDQU Y2, (BX)(R11*1)
	VMOVDQU Y3, (BP)(R11*1)
	VMOVDQU Y4, (SI)(R11*1)
	VMOVDQU Y5, (DI)(R11*1)
	VMOVDQU Y6, (R8)(R11*1)
	VMOVDQU Y7, (R9)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_1x8_loop
	VZEROUPPER

mulAvxTwo_1x8_end:
	RET

// func mulAvxTwo_1x9(low [18][16]byte, high [18][16]byte, in [1][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x9(SB), $0-816
	// Loading no tables to registers
	// Full registers estimated 32 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+584(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x9_end
	MOVQ         out_0_base+600(FP), CX
	MOVQ         out_1_base+624(FP), DX
	MOVQ         out_2_base+648(FP), BX
	MOVQ         out_3_base+672(FP), BP
	MOVQ         out_4_base+696(FP), SI
	MOVQ         out_5_base+720(FP), DI
	MOVQ         out_6_base+744(FP), R8
	MOVQ         out_7_base+768(FP), R9
	MOVQ         out_8_base+792(FP), R10
	MOVQ         in_0_base+576(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X9
	VPBROADCASTB X9, Y9
	XORQ         R12, R12

mulAvxTwo_1x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (R11)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+288(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+320(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+352(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+384(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+416(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+448(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+480(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+512(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+544(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)
	VMOVDQU Y3, (BP)(R12*1)
	VMOVDQU Y4, (SI)(R12*1)
	VMOVDQU Y5, (DI)(R12*1)
	VMOVDQU Y6, (R8)(R12*1)
	VMOVDQU Y7, (R9)(R12*1)
	VMOVDQU Y8, (R10)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_1x9_loop
	VZEROUPPER

mulAvxTwo_1x9_end:
	RET

// func mulAvxTwo_1x10(low [20][16]byte, high [20][16]byte, in [1][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x10(SB), $0-904
	// Loading no tables to registers
	// Full registers estimated 35 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+648(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x10_end
	MOVQ         out_0_base+664(FP), CX
	MOVQ         out_1_base+688(FP), DX
	MOVQ         out_2_base+712(FP), BX
	MOVQ         out_3_base+736(FP), BP
	MOVQ         out_4_base+760(FP), SI
	MOVQ         out_5_base+784(FP), DI
	MOVQ         out_6_base+808(FP), R8
	MOVQ         out_7_base+832(FP), R9
	MOVQ         out_8_base+856(FP), R10
	MOVQ         out_9_base+880(FP), R11
	MOVQ         in_0_base+640(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X10
	VPBROADCASTB X10, Y10
	XORQ         R13, R13

mulAvxTwo_1x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (R12)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+480(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+512(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+544(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+576(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+608(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)
	VMOVDQU Y4, (SI)(R13*1)
	VMOVDQU Y5, (DI)(R13*1)
	VMOVDQU Y6, (R8)(R13*1)
	VMOVDQU Y7, (R9)(R13*1)
	VMOVDQU Y8, (R10)(R13*1)
	VMOVDQU Y9, (R11)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_1x10_loop
	VZEROUPPER

mulAvxTwo_1x10_end:
	RET

// func mulAvxTwo_2x1(low [2][16]byte, high [2][16]byte, in [2][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x1(SB), $0-136
	// Loading all tables to registers
	// Full registers estimated 8 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+72(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x1_end
	MOVQ         out_0_base+112(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+32(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+48(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVQ         in_0_base+64(FP), DX
	MOVQ         in_1_base+88(FP), BX
	MOVQ         $0x0000000f, BP
	MOVQ         BP, X5
	VPBROADCASTB X5, Y5
	XORQ         BP, BP

mulAvxTwo_2x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(BP*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y5, Y6, Y6
	VPAND   Y5, Y7, Y7
	VPSHUFB Y6, Y1, Y6
	VPSHUFB Y7, Y2, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(BP*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y5, Y6, Y6
	VPAND   Y5, Y7, Y7
	VPSHUFB Y6, Y3, Y6
	VPSHUFB Y7, Y4, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(BP*1)

	// Prepare for next loop
	ADDQ $0x20, BP
	DECQ AX
	JNZ  mulAvxTwo_2x1_loop
	VZEROUPPER

mulAvxTwo_2x1_end:
	RET

// func mulAvxTwo_2x2(low [4][16]byte, high [4][16]byte, in [2][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x2(SB), $0-224
	// Loading all tables to registers
	// Full registers estimated 15 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+136(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x2_end
	MOVQ         out_0_base+176(FP), CX
	MOVQ         out_1_base+200(FP), DX
	MOVOU        low_0+0(FP), X2
	MOVOU        high_0+64(FP), X3
	VINSERTI128  $0x01, X2, Y2, Y2
	VINSERTI128  $0x01, X3, Y3, Y3
	MOVOU        low_1+16(FP), X4
	MOVOU        high_1+80(FP), X5
	VINSERTI128  $0x01, X4, Y4, Y4
	VINSERTI128  $0x01, X5, Y5, Y5
	MOVOU        low_2+32(FP), X6
	MOVOU        high_2+96(FP), X7
	VINSERTI128  $0x01, X6, Y6, Y6
	VINSERTI128  $0x01, X7, Y7, Y7
	MOVOU        low_3+48(FP), X8
	MOVOU        high_3+112(FP), X9
	VINSERTI128  $0x01, X8, Y8, Y8
	VINSERTI128  $0x01, X9, Y9, Y9
	MOVQ         in_0_base+128(FP), BX
	MOVQ         in_1_base+152(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X10
	VPBROADCASTB X10, Y10
	XORQ         SI, SI

mulAvxTwo_2x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VPSHUFB Y13, Y2, Y11
	VPSHUFB Y14, Y3, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VPSHUFB Y13, Y4, Y11
	VPSHUFB Y14, Y5, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VPSHUFB Y13, Y6, Y11
	VPSHUFB Y14, Y7, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VPSHUFB Y13, Y8, Y11
	VPSHUFB Y14, Y9, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(SI*1)
	VMOVDQU Y1, (DX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_2x2_loop
	VZEROUPPER

mulAvxTwo_2x2_end:
	RET

// func mulAvxTwo_2x3(low [12][16]byte, high [12][16]byte, in [2][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x3(SB), $0-504
	// Loading no tables to registers
	// Full registers estimated 20 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+392(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x3_end
	MOVQ         out_0_base+432(FP), CX
	MOVQ         out_1_base+456(FP), DX
	MOVQ         out_2_base+480(FP), BX
	MOVQ         in_0_base+384(FP), BP
	MOVQ         in_1_base+408(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X3
	VPBROADCASTB X3, Y3
	XORQ         DI, DI

mulAvxTwo_2x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(DI*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+192(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+224(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+256(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(DI*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+288(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+320(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+352(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(DI*1)
	VMOVDQU Y1, (DX)(DI*1)
	VMOVDQU Y2, (BX)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_2x3_loop
	VZEROUPPER

mulAvxTwo_2x3_end:
	RET

// func mulAvxTwo_2x4(low [16][16]byte, high [16][16]byte, in [2][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x4(SB), $0-656
	// Loading no tables to registers
	// Full registers estimated 25 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+520(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x4_end
	MOVQ         out_0_base+560(FP), CX
	MOVQ         out_1_base+584(FP), DX
	MOVQ         out_2_base+608(FP), BX
	MOVQ         out_3_base+632(FP), BP
	MOVQ         in_0_base+512(FP), SI
	MOVQ         in_1_base+536(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X4
	VPBROADCASTB X4, Y4
	XORQ         R8, R8

mulAvxTwo_2x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R8*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+256(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+288(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+320(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+352(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R8*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+384(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+416(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+448(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+480(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R8*1)
	VMOVDQU Y1, (DX)(R8*1)
	VMOVDQU Y2, (BX)(R8*1)
	VMOVDQU Y3, (BP)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_2x4_loop
	VZEROUPPER

mulAvxTwo_2x4_end:
	RET

// func mulAvxTwo_2x5(low [20][16]byte, high [20][16]byte, in [2][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x5(SB), $0-808
	// Loading no tables to registers
	// Full registers estimated 30 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+648(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x5_end
	MOVQ         out_0_base+688(FP), CX
	MOVQ         out_1_base+712(FP), DX
	MOVQ         out_2_base+736(FP), BX
	MOVQ         out_3_base+760(FP), BP
	MOVQ         out_4_base+784(FP), SI
	MOVQ         in_0_base+640(FP), DI
	MOVQ         in_1_base+664(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X5
	VPBROADCASTB X5, Y5
	XORQ         R9, R9

mulAvxTwo_2x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R9*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+320(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+352(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+384(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+416(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+448(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R9*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+480(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+512(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+544(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+576(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+608(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R9*1)
	VMOVDQU Y1, (DX)(R9*1)
	VMOVDQU Y2, (BX)(R9*1)
	VMOVDQU Y3, (BP)(R9*1)
	VMOVDQU Y4, (SI)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_2x5_loop
	VZEROUPPER

mulAvxTwo_2x5_end:
	RET

// func mulAvxTwo_2x6(low [24][16]byte, high [24][16]byte, in [2][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x6(SB), $0-960
	// Loading no tables to registers
	// Full registers estimated 35 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+776(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x6_end
	MOVQ         out_0_base+816(FP), CX
	MOVQ         out_1_base+840(FP), DX
	MOVQ         out_2_base+864(FP), BX
	MOVQ         out_3_base+888(FP), BP
	MOVQ         out_4_base+912(FP), SI
	MOVQ         out_5_base+936(FP), DI
	MOVQ         in_0_base+768(FP), R8
	MOVQ         in_1_base+792(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X6
	VPBROADCASTB X6, Y6
	XORQ         R10, R10

mulAvxTwo_2x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+384(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+416(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+448(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+480(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+512(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+544(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (R9)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+576(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+608(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+640(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+672(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+704(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+736(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R10*1)
	VMOVDQU Y1, (DX)(R10*1)
	VMOVDQU Y2, (BX)(R10*1)
	VMOVDQU Y3, (BP)(R10*1)
	VMOVDQU Y4, (SI)(R10*1)
	VMOVDQU Y5, (DI)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_2x6_loop
	VZEROUPPER

mulAvxTwo_2x6_end:
	RET

// func mulAvxTwo_2x7(low [28][16]byte, high [28][16]byte, in [2][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x7(SB), $0-1112
	// Loading no tables to registers
	// Full registers estimated 40 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+904(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x7_end
	MOVQ         out_0_base+944(FP), CX
	MOVQ         out_1_base+968(FP), DX
	MOVQ         out_2_base+992(FP), BX
	MOVQ         out_3_base+1016(FP), BP
	MOVQ         out_4_base+1040(FP), SI
	MOVQ         out_5_base+1064(FP), DI
	MOVQ         out_6_base+1088(FP), R8
	MOVQ         in_0_base+896(FP), R9
	MOVQ         in_1_base+920(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X7
	VPBROADCASTB X7, Y7
	XORQ         R11, R11

mulAvxTwo_2x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (R9)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+448(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+480(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+512(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+544(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+576(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+608(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+640(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (R10)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+672(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+704(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+736(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+768(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+800(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+832(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+864(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)
	VMOVDQU Y2, (BX)(R11*1)
	VMOVDQU Y3, (BP)(R11*1)
	VMOVDQU Y4, (SI)(R11*1)
	VMOVDQU Y5, (DI)(R11*1)
	VMOVDQU Y6, (R8)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_2x7_loop
	VZEROUPPER

mulAvxTwo_2x7_end:
	RET

// func mulAvxTwo_2x8(low [32][16]byte, high [32][16]byte, in [2][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x8(SB), $0-1264
	// Loading no tables to registers
	// Full registers estimated 45 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1032(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x8_end
	MOVQ         out_0_base+1072(FP), CX
	MOVQ         out_1_base+1096(FP), DX
	MOVQ         out_2_base+1120(FP), BX
	MOVQ         out_3_base+1144(FP), BP
	MOVQ         out_4_base+1168(FP), SI
	MOVQ         out_5_base+1192(FP), DI
	MOVQ         out_6_base+1216(FP), R8
	MOVQ         out_7_base+1240(FP), R9
	MOVQ         in_0_base+1024(FP), R10
	MOVQ         in_1_base+1048(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X8
	VPBROADCASTB X8, Y8
	XORQ         R12, R12

mulAvxTwo_2x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (R10)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+512(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+544(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+576(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+608(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+640(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+672(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+704(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+736(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (R11)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+768(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+800(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+832(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+864(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+896(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+928(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+960(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+992(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)
	VMOVDQU Y3, (BP)(R12*1)
	VMOVDQU Y4, (SI)(R12*1)
	VMOVDQU Y5, (DI)(R12*1)
	VMOVDQU Y6, (R8)(R12*1)
	VMOVDQU Y7, (R9)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_2x8_loop
	VZEROUPPER

mulAvxTwo_2x8_end:
	RET

// func mulAvxTwo_2x9(low [36][16]byte, high [36][16]byte, in [2][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x9(SB), $0-1416
	// Loading no tables to registers
	// Full registers estimated 50 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1160(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x9_end
	MOVQ         out_0_base+1200(FP), CX
	MOVQ         out_1_base+1224(FP), DX
	MOVQ         out_2_base+1248(FP), BX
	MOVQ         out_3_base+1272(FP), BP
	MOVQ         out_4_base+1296(FP), SI
	MOVQ         out_5_base+1320(FP), DI
	MOVQ         out_6_base+1344(FP), R8
	MOVQ         out_7_base+1368(FP), R9
	MOVQ         out_8_base+1392(FP), R10
	MOVQ         in_0_base+1152(FP), R11
	MOVQ         in_1_base+1176(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X9
	VPBROADCASTB X9, Y9
	XORQ         R13, R13

mulAvxTwo_2x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (R11)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+576(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+608(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+640(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+672(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+704(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+736(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+768(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+800(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+832(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (R12)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+864(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+896(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+928(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+960(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+992(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+1024(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+1056(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+1088(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+1120(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)
	VMOVDQU Y4, (SI)(R13*1)
	VMOVDQU Y5, (DI)(R13*1)
	VMOVDQU Y6, (R8)(R13*1)
	VMOVDQU Y7, (R9)(R13*1)
	VMOVDQU Y8, (R10)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_2x9_loop
	VZEROUPPER

mulAvxTwo_2x9_end:
	RET

// func mulAvxTwo_2x10(low [40][16]byte, high [40][16]byte, in [2][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x10(SB), $0-1568
	// Loading no tables to registers
	// Full registers estimated 55 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1288(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x10_end
	MOVQ         out_0_base+1328(FP), CX
	MOVQ         out_1_base+1352(FP), DX
	MOVQ         out_2_base+1376(FP), BX
	MOVQ         out_3_base+1400(FP), BP
	MOVQ         out_4_base+1424(FP), SI
	MOVQ         out_5_base+1448(FP), DI
	MOVQ         out_6_base+1472(FP), R8
	MOVQ         out_7_base+1496(FP), R9
	MOVQ         out_8_base+1520(FP), R10
	MOVQ         out_9_base+1544(FP), R11
	MOVQ         in_0_base+1280(FP), R12
	MOVQ         in_1_base+1304(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X10
	VPBROADCASTB X10, Y10
	XORQ         R14, R14

mulAvxTwo_2x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (R12)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+640(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+672(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+704(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+736(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+768(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+800(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+832(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+864(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+896(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+928(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (R13)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+1024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+1056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+1088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+1120(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+1152(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+1184(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+1216(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+1248(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)
	VMOVDQU Y4, (SI)(R14*1)
	VMOVDQU Y5, (DI)(R14*1)
	VMOVDQU Y6, (R8)(R14*1)
	VMOVDQU Y7, (R9)(R14*1)
	VMOVDQU Y8, (R10)(R14*1)
	VMOVDQU Y9, (R11)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_2x10_loop
	VZEROUPPER

mulAvxTwo_2x10_end:
	RET

// func mulAvxTwo_3x1(low [3][16]byte, high [3][16]byte, in [3][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x1(SB), $0-192
	// Loading all tables to registers
	// Full registers estimated 10 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x1_end
	MOVQ         out_0_base+168(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+48(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+64(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+80(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVQ         in_0_base+96(FP), DX
	MOVQ         in_1_base+120(FP), BX
	MOVQ         in_2_base+144(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X7
	VPBROADCASTB X7, Y7
	XORQ         SI, SI

mulAvxTwo_3x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y1, Y8
	VPSHUFB Y9, Y2, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y3, Y8
	VPSHUFB Y9, Y4, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y5, Y8
	VPSHUFB Y9, Y6, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_3x1_loop
	VZEROUPPER

mulAvxTwo_3x1_end:
	RET

// func mulAvxTwo_3x2(low [12][16]byte, high [12][16]byte, in [3][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x2(SB), $0-504
	// Loading no tables to registers
	// Full registers estimated 19 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+392(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x2_end
	MOVQ         out_0_base+456(FP), CX
	MOVQ         out_1_base+480(FP), DX
	MOVQ         in_0_base+384(FP), BX
	MOVQ         in_1_base+408(FP), BP
	MOVQ         in_2_base+432(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X2
	VPBROADCASTB X2, Y2
	XORQ         DI, DI

mulAvxTwo_3x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(DI*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+192(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+224(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(DI*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+256(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+288(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(DI*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+320(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+352(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(DI*1)
	VMOVDQU Y1, (DX)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_3x2_loop
	VZEROUPPER

mulAvxTwo_3x2_end:
	RET

// func mulAvxTwo_3x3(low [18][16]byte, high [18][16]byte, in [3][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x3(SB), $0-720
	// Loading no tables to registers
	// Full registers estimated 26 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+584(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x3_end
	MOVQ         out_0_base+648(FP), CX
	MOVQ         out_1_base+672(FP), DX
	MOVQ         out_2_base+696(FP), BX
	MOVQ         in_0_base+576(FP), BP
	MOVQ         in_1_base+600(FP), SI
	MOVQ         in_2_base+624(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X3
	VPBROADCASTB X3, Y3
	XORQ         R8, R8

mulAvxTwo_3x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R8*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+288(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+320(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+352(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R8*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+384(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+416(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+448(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R8*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+480(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+512(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+544(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R8*1)
	VMOVDQU Y1, (DX)(R8*1)
	VMOVDQU Y2, (BX)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_3x3_loop
	VZEROUPPER

mulAvxTwo_3x3_end:
	RET

// func mulAvxTwo_3x4(low [24][16]byte, high [24][16]byte, in [3][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x4(SB), $0-936
	// Loading no tables to registers
	// Full registers estimated 33 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+776(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x4_end
	MOVQ         out_0_base+840(FP), CX
	MOVQ         out_1_base+864(FP), DX
	MOVQ         out_2_base+888(FP), BX
	MOVQ         out_3_base+912(FP), BP
	MOVQ         in_0_base+768(FP), SI
	MOVQ         in_1_base+792(FP), DI
	MOVQ         in_2_base+816(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X4
	VPBROADCASTB X4, Y4
	XORQ         R9, R9

mulAvxTwo_3x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R9*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+384(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+416(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+448(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+480(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R9*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+512(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+544(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+576(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+608(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R9*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+640(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+672(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+704(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+736(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R9*1)
	VMOVDQU Y1, (DX)(R9*1)
	VMOVDQU Y2, (BX)(R9*1)
	VMOVDQU Y3, (BP)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_3x4_loop
	VZEROUPPER

mulAvxTwo_3x4_end:
	RET

// func mulAvxTwo_3x5(low [30][16]byte, high [30][16]byte, in [3][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x5(SB), $0-1152
	// Loading no tables to registers
	// Full registers estimated 40 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+968(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x5_end
	MOVQ         out_0_base+1032(FP), CX
	MOVQ         out_1_base+1056(FP), DX
	MOVQ         out_2_base+1080(FP), BX
	MOVQ         out_3_base+1104(FP), BP
	MOVQ         out_4_base+1128(FP), SI
	MOVQ         in_0_base+960(FP), DI
	MOVQ         in_1_base+984(FP), R8
	MOVQ         in_2_base+1008(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X5
	VPBROADCASTB X5, Y5
	XORQ         R10, R10

mulAvxTwo_3x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R10*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+480(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+512(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+544(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+576(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+608(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R10*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+640(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+672(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+704(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+736(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+768(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (R9)(R10*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+800(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+832(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+864(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+896(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+928(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R10*1)
	VMOVDQU Y1, (DX)(R10*1)
	VMOVDQU Y2, (BX)(R10*1)
	VMOVDQU Y3, (BP)(R10*1)
	VMOVDQU Y4, (SI)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_3x5_loop
	VZEROUPPER

mulAvxTwo_3x5_end:
	RET

// func mulAvxTwo_3x6(low [36][16]byte, high [36][16]byte, in [3][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x6(SB), $0-1368
	// Loading no tables to registers
	// Full registers estimated 47 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1160(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x6_end
	MOVQ         out_0_base+1224(FP), CX
	MOVQ         out_1_base+1248(FP), DX
	MOVQ         out_2_base+1272(FP), BX
	MOVQ         out_3_base+1296(FP), BP
	MOVQ         out_4_base+1320(FP), SI
	MOVQ         out_5_base+1344(FP), DI
	MOVQ         in_0_base+1152(FP), R8
	MOVQ         in_1_base+1176(FP), R9
	MOVQ         in_2_base+1200(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X6
	VPBROADCASTB X6, Y6
	XORQ         R11, R11

mulAvxTwo_3x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+576(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+608(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+640(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+672(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+704(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+736(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (R9)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+768(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+800(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+832(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+864(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+896(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+928(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (R10)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+960(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+992(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+1024(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+1056(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+1088(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+1120(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)
	VMOVDQU Y2, (BX)(R11*1)
	VMOVDQU Y3, (BP)(R11*1)
	VMOVDQU Y4, (SI)(R11*1)
	VMOVDQU Y5, (DI)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_3x6_loop
	VZEROUPPER

mulAvxTwo_3x6_end:
	RET

// func mulAvxTwo_3x7(low [42][16]byte, high [42][16]byte, in [3][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x7(SB), $0-1584
	// Loading no tables to registers
	// Full registers estimated 54 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1352(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x7_end
	MOVQ         out_0_base+1416(FP), CX
	MOVQ         out_1_base+1440(FP), DX
	MOVQ         out_2_base+1464(FP), BX
	MOVQ         out_3_base+1488(FP), BP
	MOVQ         out_4_base+1512(FP), SI
	MOVQ         out_5_base+1536(FP), DI
	MOVQ         out_6_base+1560(FP), R8
	MOVQ         in_0_base+1344(FP), R9
	MOVQ         in_1_base+1368(FP), R10
	MOVQ         in_2_base+1392(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X7
	VPBROADCASTB X7, Y7
	XORQ         R12, R12

mulAvxTwo_3x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (R9)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+672(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+704(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+736(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+768(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+800(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+832(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+864(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (R10)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+896(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+928(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+960(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+992(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+1024(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+1056(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+1088(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (R11)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+1120(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+1152(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+1184(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+1216(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+1248(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+1280(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+1312(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)
	VMOVDQU Y3, (BP)(R12*1)
	VMOVDQU Y4, (SI)(R12*1)
	VMOVDQU Y5, (DI)(R12*1)
	VMOVDQU Y6, (R8)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_3x7_loop
	VZEROUPPER

mulAvxTwo_3x7_end:
	RET

// func mulAvxTwo_3x8(low [48][16]byte, high [48][16]byte, in [3][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x8(SB), $0-1800
	// Loading no tables to registers
	// Full registers estimated 61 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1544(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x8_end
	MOVQ         out_0_base+1608(FP), CX
	MOVQ         out_1_base+1632(FP), DX
	MOVQ         out_2_base+1656(FP), BX
	MOVQ         out_3_base+1680(FP), BP
	MOVQ         out_4_base+1704(FP), SI
	MOVQ         out_5_base+1728(FP), DI
	MOVQ         out_6_base+1752(FP), R8
	MOVQ         out_7_base+1776(FP), R9
	MOVQ         in_0_base+1536(FP), R10
	MOVQ         in_1_base+1560(FP), R11
	MOVQ         in_2_base+1584(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X8
	VPBROADCASTB X8, Y8
	XORQ         R13, R13

mulAvxTwo_3x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (R10)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+768(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+800(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+832(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+864(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+896(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+928(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+960(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+992(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (R11)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+1024(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+1056(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+1088(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+1120(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+1152(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+1184(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+1216(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+1248(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (R12)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+1280(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+1312(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+1344(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+1376(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+1408(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+1440(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+1472(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+1504(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)
	VMOVDQU Y4, (SI)(R13*1)
	VMOVDQU Y5, (DI)(R13*1)
	VMOVDQU Y6, (R8)(R13*1)
	VMOVDQU Y7, (R9)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_3x8_loop
	VZEROUPPER

mulAvxTwo_3x8_end:
	RET

// func mulAvxTwo_3x9(low [54][16]byte, high [54][16]byte, in [3][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x9(SB), $0-2016
	// Loading no tables to registers
	// Full registers estimated 68 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1736(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x9_end
	MOVQ         out_0_base+1800(FP), CX
	MOVQ         out_1_base+1824(FP), DX
	MOVQ         out_2_base+1848(FP), BX
	MOVQ         out_3_base+1872(FP), BP
	MOVQ         out_4_base+1896(FP), SI
	MOVQ         out_5_base+1920(FP), DI
	MOVQ         out_6_base+1944(FP), R8
	MOVQ         out_7_base+1968(FP), R9
	MOVQ         out_8_base+1992(FP), R10
	MOVQ         in_0_base+1728(FP), R11
	MOVQ         in_1_base+1752(FP), R12
	MOVQ         in_2_base+1776(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X9
	VPBROADCASTB X9, Y9
	XORQ         R14, R14

mulAvxTwo_3x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (R11)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+864(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+896(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+928(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+960(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+992(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+1024(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+1056(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+1088(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+1120(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (R12)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+1152(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+1184(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+1216(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+1248(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+1280(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+1312(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+1344(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+1376(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+1408(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (R13)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+1440(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+1472(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+1504(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+1536(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+1568(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+1600(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+1632(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+1664(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+1696(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)
	VMOVDQU Y4, (SI)(R14*1)
	VMOVDQU Y5, (DI)(R14*1)
	VMOVDQU Y6, (R8)(R14*1)
	VMOVDQU Y7, (R9)(R14*1)
	VMOVDQU Y8, (R10)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_3x9_loop
	VZEROUPPER

mulAvxTwo_3x9_end:
	RET

// func mulAvxTwo_3x10(low [60][16]byte, high [60][16]byte, in [3][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x10(SB), $0-2232
	// Loading no tables to registers
	// Full registers estimated 75 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1928(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x10_end
	MOVQ         out_0_base+1992(FP), CX
	MOVQ         out_1_base+2016(FP), DX
	MOVQ         out_2_base+2040(FP), BX
	MOVQ         out_3_base+2064(FP), BP
	MOVQ         out_4_base+2088(FP), SI
	MOVQ         out_5_base+2112(FP), DI
	MOVQ         out_6_base+2136(FP), R8
	MOVQ         out_7_base+2160(FP), R9
	MOVQ         out_8_base+2184(FP), R10
	MOVQ         out_9_base+2208(FP), R11
	MOVQ         in_0_base+1920(FP), R12
	MOVQ         in_1_base+1944(FP), R13
	MOVQ         in_2_base+1968(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X10
	VPBROADCASTB X10, Y10
	XORQ         R15, R15

mulAvxTwo_3x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (R12)(R15*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+1024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+1056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+1088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+1120(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+1152(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+1184(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+1216(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+1248(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (R13)(R15*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+1280(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+1312(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+1344(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+1376(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+1408(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+1440(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+1472(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+1504(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+1536(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+1568(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (R14)(R15*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+1600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+1632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+1664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+1696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+1728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+1760(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+1792(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+1824(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+1856(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+1888(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)
	VMOVDQU Y4, (SI)(R15*1)
	VMOVDQU Y5, (DI)(R15*1)
	VMOVDQU Y6, (R8)(R15*1)
	VMOVDQU Y7, (R9)(R15*1)
	VMOVDQU Y8, (R10)(R15*1)
	VMOVDQU Y9, (R11)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_3x10_loop
	VZEROUPPER

mulAvxTwo_3x10_end:
	RET

// func mulAvxTwo_4x1(low [4][16]byte, high [4][16]byte, in [4][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x1(SB), $0-248
	// Loading all tables to registers
	// Full registers estimated 12 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+136(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x1_end
	MOVQ         out_0_base+224(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+64(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+80(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+96(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+112(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVQ         in_0_base+128(FP), DX
	MOVQ         in_1_base+152(FP), BX
	MOVQ         in_2_base+176(FP), BP
	MOVQ         in_3_base+200(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X9
	VPBROADCASTB X9, Y9
	XORQ         DI, DI

mulAvxTwo_4x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y1, Y10
	VPSHUFB Y11, Y2, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y3, Y10
	VPSHUFB Y11, Y4, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y5, Y10
	VPSHUFB Y11, Y6, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y7, Y10
	VPSHUFB Y11, Y8, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_4x1_loop
	VZEROUPPER

mulAvxTwo_4x1_end:
	RET

// func mulAvxTwo_4x2(low [16][16]byte, high [16][16]byte, in [4][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x2(SB), $0-656
	// Loading no tables to registers
	// Full registers estimated 23 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+520(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x2_end
	MOVQ         out_0_base+608(FP), CX
	MOVQ         out_1_base+632(FP), DX
	MOVQ         in_0_base+512(FP), BX
	MOVQ         in_1_base+536(FP), BP
	MOVQ         in_2_base+560(FP), SI
	MOVQ         in_3_base+584(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X2
	VPBROADCASTB X2, Y2
	XORQ         R8, R8

mulAvxTwo_4x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R8*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+256(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+288(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R8*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+320(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+352(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R8*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+384(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+416(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R8*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+448(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+480(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R8*1)
	VMOVDQU Y1, (DX)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_4x2_loop
	VZEROUPPER

mulAvxTwo_4x2_end:
	RET

// func mulAvxTwo_4x3(low [24][16]byte, high [24][16]byte, in [4][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x3(SB), $0-936
	// Loading no tables to registers
	// Full registers estimated 32 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+776(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x3_end
	MOVQ         out_0_base+864(FP), CX
	MOVQ         out_1_base+888(FP), DX
	MOVQ         out_2_base+912(FP), BX
	MOVQ         in_0_base+768(FP), BP
	MOVQ         in_1_base+792(FP), SI
	MOVQ         in_2_base+816(FP), DI
	MOVQ         in_3_base+840(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X3
	VPBROADCASTB X3, Y3
	XORQ         R9, R9

mulAvxTwo_4x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R9*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+384(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+416(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+448(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R9*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+480(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+512(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+544(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R9*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+576(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+608(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+640(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R9*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+672(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+704(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+736(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R9*1)
	VMOVDQU Y1, (DX)(R9*1)
	VMOVDQU Y2, (BX)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_4x3_loop
	VZEROUPPER

mulAvxTwo_4x3_end:
	RET

// func mulAvxTwo_4x4(low [32][16]byte, high [32][16]byte, in [4][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x4(SB), $0-1216
	// Loading no tables to registers
	// Full registers estimated 41 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1032(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x4_end
	MOVQ         out_0_base+1120(FP), CX
	MOVQ         out_1_base+1144(FP), DX
	MOVQ         out_2_base+1168(FP), BX
	MOVQ         out_3_base+1192(FP), BP
	MOVQ         in_0_base+1024(FP), SI
	MOVQ         in_1_base+1048(FP), DI
	MOVQ         in_2_base+1072(FP), R8
	MOVQ         in_3_base+1096(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X4
	VPBROADCASTB X4, Y4
	XORQ         R10, R10

mulAvxTwo_4x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R10*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+512(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+544(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+576(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+608(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R10*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+640(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+672(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+704(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+736(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R10*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+768(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+800(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+832(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+864(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (R9)(R10*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+896(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+928(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+960(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+992(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R10*1)
	VMOVDQU Y1, (DX)(R10*1)
	VMOVDQU Y2, (BX)(R10*1)
	VMOVDQU Y3, (BP)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_4x4_loop
	VZEROUPPER

mulAvxTwo_4x4_end:
	RET

// func mulAvxTwo_4x5(low [40][16]byte, high [40][16]byte, in [4][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x5(SB), $0-1496
	// Loading no tables to registers
	// Full registers estimated 50 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1288(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x5_end
	MOVQ         out_0_base+1376(FP), CX
	MOVQ         out_1_base+1400(FP), DX
	MOVQ         out_2_base+1424(FP), BX
	MOVQ         out_3_base+1448(FP), BP
	MOVQ         out_4_base+1472(FP), SI
	MOVQ         in_0_base+1280(FP), DI
	MOVQ         in_1_base+1304(FP), R8
	MOVQ         in_2_base+1328(FP), R9
	MOVQ         in_3_base+1352(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X5
	VPBROADCASTB X5, Y5
	XORQ         R11, R11

mulAvxTwo_4x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+640(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+672(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+704(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+736(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+768(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+800(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+832(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+864(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+896(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+928(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (R9)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+960(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+992(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1024(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+1056(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+1088(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (R10)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+1120(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+1152(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+1184(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+1216(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+1248(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)
	VMOVDQU Y2, (BX)(R11*1)
	VMOVDQU Y3, (BP)(R11*1)
	VMOVDQU Y4, (SI)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_4x5_loop
	VZEROUPPER

mulAvxTwo_4x5_end:
	RET

// func mulAvxTwo_4x6(low [48][16]byte, high [48][16]byte, in [4][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x6(SB), $0-1776
	// Loading no tables to registers
	// Full registers estimated 59 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1544(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x6_end
	MOVQ         out_0_base+1632(FP), CX
	MOVQ         out_1_base+1656(FP), DX
	MOVQ         out_2_base+1680(FP), BX
	MOVQ         out_3_base+1704(FP), BP
	MOVQ         out_4_base+1728(FP), SI
	MOVQ         out_5_base+1752(FP), DI
	MOVQ         in_0_base+1536(FP), R8
	MOVQ         in_1_base+1560(FP), R9
	MOVQ         in_2_base+1584(FP), R10
	MOVQ         in_3_base+1608(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X6
	VPBROADCASTB X6, Y6
	XORQ         R12, R12

mulAvxTwo_4x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+768(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+800(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+832(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+864(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+896(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+928(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (R9)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+960(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+992(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+1024(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+1056(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+1088(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+1120(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (R10)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+1152(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+1184(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+1216(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+1248(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+1280(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+1312(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (R11)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+1344(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+1376(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+1408(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+1440(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+1472(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+1504(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)
	VMOVDQU Y3, (BP)(R12*1)
	VMOVDQU Y4, (SI)(R12*1)
	VMOVDQU Y5, (DI)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_4x6_loop
	VZEROUPPER

mulAvxTwo_4x6_end:
	RET

// func mulAvxTwo_4x7(low [56][16]byte, high [56][16]byte, in [4][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x7(SB), $0-2056
	// Loading no tables to registers
	// Full registers estimated 68 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1800(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x7_end
	MOVQ         out_0_base+1888(FP), CX
	MOVQ         out_1_base+1912(FP), DX
	MOVQ         out_2_base+1936(FP), BX
	MOVQ         out_3_base+1960(FP), BP
	MOVQ         out_4_base+1984(FP), SI
	MOVQ         out_5_base+2008(FP), DI
	MOVQ         out_6_base+2032(FP), R8
	MOVQ         in_0_base+1792(FP), R9
	MOVQ         in_1_base+1816(FP), R10
	MOVQ         in_2_base+1840(FP), R11
	MOVQ         in_3_base+1864(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X7
	VPBROADCASTB X7, Y7
	XORQ         R13, R13

mulAvxTwo_4x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (R9)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+896(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+928(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+960(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+992(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+1024(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+1056(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+1088(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (R10)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+1120(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+1152(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+1184(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+1216(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+1248(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+1280(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+1312(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (R11)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+1344(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+1376(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+1408(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+1440(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+1472(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+1504(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+1536(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (R12)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+1568(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+1600(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+1632(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+1664(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+1696(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+1728(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+1760(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)
	VMOVDQU Y4, (SI)(R13*1)
	VMOVDQU Y5, (DI)(R13*1)
	VMOVDQU Y6, (R8)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_4x7_loop
	VZEROUPPER

mulAvxTwo_4x7_end:
	RET

// func mulAvxTwo_4x8(low [64][16]byte, high [64][16]byte, in [4][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x8(SB), $0-2336
	// Loading no tables to registers
	// Full registers estimated 77 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2056(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x8_end
	MOVQ         out_0_base+2144(FP), CX
	MOVQ         out_1_base+2168(FP), DX
	MOVQ         out_2_base+2192(FP), BX
	MOVQ         out_3_base+2216(FP), BP
	MOVQ         out_4_base+2240(FP), SI
	MOVQ         out_5_base+2264(FP), DI
	MOVQ         out_6_base+2288(FP), R8
	MOVQ         out_7_base+2312(FP), R9
	MOVQ         in_0_base+2048(FP), R10
	MOVQ         in_1_base+2072(FP), R11
	MOVQ         in_2_base+2096(FP), R12
	MOVQ         in_3_base+2120(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X8
	VPBROADCASTB X8, Y8
	XORQ         R14, R14

mulAvxTwo_4x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (R10)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+1024(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+1056(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+1088(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+1120(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+1152(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+1184(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+1216(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+1248(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (R11)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+1280(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+1312(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+1344(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+1376(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+1408(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+1440(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+1472(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+1504(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (R12)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+1536(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+1568(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+1600(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+1632(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+1664(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+1696(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+1728(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+1760(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (R13)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+1792(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+1824(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+1856(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+1888(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+1920(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+1952(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+1984(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+2016(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)
	VMOVDQU Y4, (SI)(R14*1)
	VMOVDQU Y5, (DI)(R14*1)
	VMOVDQU Y6, (R8)(R14*1)
	VMOVDQU Y7, (R9)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_4x8_loop
	VZEROUPPER

mulAvxTwo_4x8_end:
	RET

// func mulAvxTwo_4x9(low [72][16]byte, high [72][16]byte, in [4][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x9(SB), $0-2616
	// Loading no tables to registers
	// Full registers estimated 86 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2312(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x9_end
	MOVQ         out_0_base+2400(FP), CX
	MOVQ         out_1_base+2424(FP), DX
	MOVQ         out_2_base+2448(FP), BX
	MOVQ         out_3_base+2472(FP), BP
	MOVQ         out_4_base+2496(FP), SI
	MOVQ         out_5_base+2520(FP), DI
	MOVQ         out_6_base+2544(FP), R8
	MOVQ         out_7_base+2568(FP), R9
	MOVQ         out_8_base+2592(FP), R10
	MOVQ         in_0_base+2304(FP), R11
	MOVQ         in_1_base+2328(FP), R12
	MOVQ         in_2_base+2352(FP), R13
	MOVQ         in_3_base+2376(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X9
	VPBROADCASTB X9, Y9
	XORQ         R15, R15

mulAvxTwo_4x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (R11)(R15*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+1152(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+1184(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+1216(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+1248(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+1280(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+1312(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+1344(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+1376(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+1408(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (R12)(R15*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+1440(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+1472(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+1504(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+1536(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+1568(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+1600(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+1632(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+1664(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+1696(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (R13)(R15*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+1728(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+1760(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+1792(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+1824(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+1856(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+1888(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+1920(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+1952(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+1984(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (R14)(R15*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+2016(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+2048(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+2080(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+2112(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+2144(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+2176(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+2208(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+2240(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+2272(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)
	VMOVDQU Y4, (SI)(R15*1)
	VMOVDQU Y5, (DI)(R15*1)
	VMOVDQU Y6, (R8)(R15*1)
	VMOVDQU Y7, (R9)(R15*1)
	VMOVDQU Y8, (R10)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_4x9_loop
	VZEROUPPER

mulAvxTwo_4x9_end:
	RET

// func mulAvxTwo_4x10(low [80][16]byte, high [80][16]byte, in [4][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x10(SB), $0-2896
	// Loading no tables to registers
	// Full registers estimated 95 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2568(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x10_end
	MOVQ         in_0_base+2560(FP), CX
	MOVQ         in_1_base+2584(FP), DX
	MOVQ         in_2_base+2608(FP), BX
	MOVQ         in_3_base+2632(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X10
	VPBROADCASTB X10, Y10
	XORQ         SI, SI

mulAvxTwo_4x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+1280(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+1312(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+1344(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+1376(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+1408(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+1440(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+1472(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+1504(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+1536(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+1568(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+1600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+1632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+1664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+1696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+1728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+1760(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+1792(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+1824(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+1856(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+1888(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+1920(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+1952(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+1984(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+2016(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+2048(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+2080(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+2112(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+2144(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+2176(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+2208(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+2240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+2272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+2304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+2336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+2368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+2400(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+2432(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+2464(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+2496(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+2528(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+2656(FP), DI
	VMOVDQU Y0, (DI)(SI*1)
	MOVQ    out_1_base+2680(FP), DI
	VMOVDQU Y1, (DI)(SI*1)
	MOVQ    out_2_base+2704(FP), DI
	VMOVDQU Y2, (DI)(SI*1)
	MOVQ    out_3_base+2728(FP), DI
	VMOVDQU Y3, (DI)(SI*1)
	MOVQ    out_4_base+2752(FP), DI
	VMOVDQU Y4, (DI)(SI*1)
	MOVQ    out_5_base+2776(FP), DI
	VMOVDQU Y5, (DI)(SI*1)
	MOVQ    out_6_base+2800(FP), DI
	VMOVDQU Y6, (DI)(SI*1)
	MOVQ    out_7_base+2824(FP), DI
	VMOVDQU Y7, (DI)(SI*1)
	MOVQ    out_8_base+2848(FP), DI
	VMOVDQU Y8, (DI)(SI*1)
	MOVQ    out_9_base+2872(FP), DI
	VMOVDQU Y9, (DI)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_4x10_loop
	VZEROUPPER

mulAvxTwo_4x10_end:
	RET

// func mulAvxTwo_5x1(low [5][16]byte, high [5][16]byte, in [5][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x1(SB), $0-304
	// Loading all tables to registers
	// Full registers estimated 14 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+168(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x1_end
	MOVQ         out_0_base+280(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+80(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+96(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+112(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+128(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVOU        low_4+64(FP), X9
	MOVOU        high_4+144(FP), X10
	VINSERTI128  $0x01, X9, Y9, Y9
	VINSERTI128  $0x01, X10, Y10, Y10
	MOVQ         in_0_base+160(FP), DX
	MOVQ         in_1_base+184(FP), BX
	MOVQ         in_2_base+208(FP), BP
	MOVQ         in_3_base+232(FP), SI
	MOVQ         in_4_base+256(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X11
	VPBROADCASTB X11, Y11
	XORQ         R8, R8

mulAvxTwo_5x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y1, Y12
	VPSHUFB Y13, Y2, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y3, Y12
	VPSHUFB Y13, Y4, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y5, Y12
	VPSHUFB Y13, Y6, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y7, Y12
	VPSHUFB Y13, Y8, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y9, Y12
	VPSHUFB Y13, Y10, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_5x1_loop
	VZEROUPPER

mulAvxTwo_5x1_end:
	RET

// func mulAvxTwo_5x2(low [20][16]byte, high [20][16]byte, in [5][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x2(SB), $0-808
	// Loading no tables to registers
	// Full registers estimated 27 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+648(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x2_end
	MOVQ         out_0_base+760(FP), CX
	MOVQ         out_1_base+784(FP), DX
	MOVQ         in_0_base+640(FP), BX
	MOVQ         in_1_base+664(FP), BP
	MOVQ         in_2_base+688(FP), SI
	MOVQ         in_3_base+712(FP), DI
	MOVQ         in_4_base+736(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X2
	VPBROADCASTB X2, Y2
	XORQ         R9, R9

mulAvxTwo_5x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R9*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+320(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+352(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R9*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+384(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+416(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R9*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+448(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+480(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R9*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+512(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+544(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R9*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+576(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+608(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R9*1)
	VMOVDQU Y1, (DX)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_5x2_loop
	VZEROUPPER

mulAvxTwo_5x2_end:
	RET

// func mulAvxTwo_5x3(low [30][16]byte, high [30][16]byte, in [5][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x3(SB), $0-1152
	// Loading no tables to registers
	// Full registers estimated 38 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+968(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x3_end
	MOVQ         out_0_base+1080(FP), CX
	MOVQ         out_1_base+1104(FP), DX
	MOVQ         out_2_base+1128(FP), BX
	MOVQ         in_0_base+960(FP), BP
	MOVQ         in_1_base+984(FP), SI
	MOVQ         in_2_base+1008(FP), DI
	MOVQ         in_3_base+1032(FP), R8
	MOVQ         in_4_base+1056(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X3
	VPBROADCASTB X3, Y3
	XORQ         R10, R10

mulAvxTwo_5x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R10*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+480(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+512(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+544(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R10*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+576(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+608(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+640(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R10*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+672(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+704(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+736(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R10*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+768(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+800(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+832(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (R9)(R10*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+864(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+896(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+928(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R10*1)
	VMOVDQU Y1, (DX)(R10*1)
	VMOVDQU Y2, (BX)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_5x3_loop
	VZEROUPPER

mulAvxTwo_5x3_end:
	RET

// func mulAvxTwo_5x4(low [40][16]byte, high [40][16]byte, in [5][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x4(SB), $0-1496
	// Loading no tables to registers
	// Full registers estimated 49 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1288(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x4_end
	MOVQ         out_0_base+1400(FP), CX
	MOVQ         out_1_base+1424(FP), DX
	MOVQ         out_2_base+1448(FP), BX
	MOVQ         out_3_base+1472(FP), BP
	MOVQ         in_0_base+1280(FP), SI
	MOVQ         in_1_base+1304(FP), DI
	MOVQ         in_2_base+1328(FP), R8
	MOVQ         in_3_base+1352(FP), R9
	MOVQ         in_4_base+1376(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X4
	VPBROADCASTB X4, Y4
	XORQ         R11, R11

mulAvxTwo_5x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R11*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+640(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+672(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+704(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+736(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R11*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+768(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+800(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+832(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+864(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R11*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+896(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+928(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+960(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+992(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (R9)(R11*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1024(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1056(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1088(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1120(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (R10)(R11*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1152(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1184(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1216(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+1248(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)
	VMOVDQU Y2, (BX)(R11*1)
	VMOVDQU Y3, (BP)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_5x4_loop
	VZEROUPPER

mulAvxTwo_5x4_end:
	RET

// func mulAvxTwo_5x5(low [50][16]byte, high [50][16]byte, in [5][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x5(SB), $0-1840
	// Loading no tables to registers
	// Full registers estimated 60 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1608(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x5_end
	MOVQ         out_0_base+1720(FP), CX
	MOVQ         out_1_base+1744(FP), DX
	MOVQ         out_2_base+1768(FP), BX
	MOVQ         out_3_base+1792(FP), BP
	MOVQ         out_4_base+1816(FP), SI
	MOVQ         in_0_base+1600(FP), DI
	MOVQ         in_1_base+1624(FP), R8
	MOVQ         in_2_base+1648(FP), R9
	MOVQ         in_3_base+1672(FP), R10
	MOVQ         in_4_base+1696(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X5
	VPBROADCASTB X5, Y5
	XORQ         R12, R12

mulAvxTwo_5x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+800(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+832(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+864(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+896(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+928(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+960(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+992(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1024(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+1056(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+1088(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (R9)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+1120(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+1152(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1184(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+1216(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+1248(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (R10)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+1280(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+1312(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+1344(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+1376(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+1408(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (R11)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+1440(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+1472(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+1504(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+1536(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+1568(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)
	VMOVDQU Y3, (BP)(R12*1)
	VMOVDQU Y4, (SI)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_5x5_loop
	VZEROUPPER

mulAvxTwo_5x5_end:
	RET

// func mulAvxTwo_5x6(low [60][16]byte, high [60][16]byte, in [5][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x6(SB), $0-2184
	// Loading no tables to registers
	// Full registers estimated 71 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1928(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x6_end
	MOVQ         out_0_base+2040(FP), CX
	MOVQ         out_1_base+2064(FP), DX
	MOVQ         out_2_base+2088(FP), BX
	MOVQ         out_3_base+2112(FP), BP
	MOVQ         out_4_base+2136(FP), SI
	MOVQ         out_5_base+2160(FP), DI
	MOVQ         in_0_base+1920(FP), R8
	MOVQ         in_1_base+1944(FP), R9
	MOVQ         in_2_base+1968(FP), R10
	MOVQ         in_3_base+1992(FP), R11
	MOVQ         in_4_base+2016(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X6
	VPBROADCASTB X6, Y6
	XORQ         R13, R13

mulAvxTwo_5x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+960(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+992(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+1024(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+1056(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+1088(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+1120(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (R9)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+1152(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+1184(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+1216(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+1248(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+1280(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+1312(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (R10)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+1344(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+1376(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+1408(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+1440(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+1472(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+1504(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (R11)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+1536(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+1568(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+1600(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+1632(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+1664(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+1696(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (R12)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+1728(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+1760(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+1792(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+1824(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+1856(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+1888(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)
	VMOVDQU Y4, (SI)(R13*1)
	VMOVDQU Y5, (DI)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_5x6_loop
	VZEROUPPER

mulAvxTwo_5x6_end:
	RET

// func mulAvxTwo_5x7(low [70][16]byte, high [70][16]byte, in [5][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x7(SB), $0-2528
	// Loading no tables to registers
	// Full registers estimated 82 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2248(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x7_end
	MOVQ         out_0_base+2360(FP), CX
	MOVQ         out_1_base+2384(FP), DX
	MOVQ         out_2_base+2408(FP), BX
	MOVQ         out_3_base+2432(FP), BP
	MOVQ         out_4_base+2456(FP), SI
	MOVQ         out_5_base+2480(FP), DI
	MOVQ         out_6_base+2504(FP), R8
	MOVQ         in_0_base+2240(FP), R9
	MOVQ         in_1_base+2264(FP), R10
	MOVQ         in_2_base+2288(FP), R11
	MOVQ         in_3_base+2312(FP), R12
	MOVQ         in_4_base+2336(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X7
	VPBROADCASTB X7, Y7
	XORQ         R14, R14

mulAvxTwo_5x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (R9)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+1120(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+1152(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+1184(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+1216(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+1248(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+1280(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+1312(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (R10)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+1344(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+1376(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+1408(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+1440(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+1472(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+1504(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+1536(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (R11)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+1568(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+1600(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+1632(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+1664(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+1696(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+1728(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+1760(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (R12)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+1792(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+1824(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+1856(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+1888(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+1920(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+1952(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+1984(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (R13)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+2016(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+2048(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+2080(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+2112(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+2144(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+2176(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+2208(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)
	VMOVDQU Y4, (SI)(R14*1)
	VMOVDQU Y5, (DI)(R14*1)
	VMOVDQU Y6, (R8)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_5x7_loop
	VZEROUPPER

mulAvxTwo_5x7_end:
	RET

// func mulAvxTwo_5x8(low [80][16]byte, high [80][16]byte, in [5][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x8(SB), $0-2872
	// Loading no tables to registers
	// Full registers estimated 93 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2568(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x8_end
	MOVQ         out_0_base+2680(FP), CX
	MOVQ         out_1_base+2704(FP), DX
	MOVQ         out_2_base+2728(FP), BX
	MOVQ         out_3_base+2752(FP), BP
	MOVQ         out_4_base+2776(FP), SI
	MOVQ         out_5_base+2800(FP), DI
	MOVQ         out_6_base+2824(FP), R8
	MOVQ         out_7_base+2848(FP), R9
	MOVQ         in_0_base+2560(FP), R10
	MOVQ         in_1_base+2584(FP), R11
	MOVQ         in_2_base+2608(FP), R12
	MOVQ         in_3_base+2632(FP), R13
	MOVQ         in_4_base+2656(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X8
	VPBROADCASTB X8, Y8
	XORQ         R15, R15

mulAvxTwo_5x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (R10)(R15*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+1280(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+1312(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+1344(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+1376(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+1408(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+1440(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+1472(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+1504(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (R11)(R15*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+1536(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+1568(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+1600(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+1632(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+1664(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+1696(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+1728(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+1760(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (R12)(R15*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+1792(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+1824(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+1856(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+1888(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+1920(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+1952(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+1984(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+2016(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (R13)(R15*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+2048(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+2080(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+2112(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+2144(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+2176(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+2208(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+2240(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+2272(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (R14)(R15*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+2304(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+2336(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+2368(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+2400(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+2432(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+2464(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+2496(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+2528(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)
	VMOVDQU Y4, (SI)(R15*1)
	VMOVDQU Y5, (DI)(R15*1)
	VMOVDQU Y6, (R8)(R15*1)
	VMOVDQU Y7, (R9)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_5x8_loop
	VZEROUPPER

mulAvxTwo_5x8_end:
	RET

// func mulAvxTwo_5x9(low [90][16]byte, high [90][16]byte, in [5][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x9(SB), $0-3216
	// Loading no tables to registers
	// Full registers estimated 104 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2888(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x9_end
	MOVQ         in_0_base+2880(FP), CX
	MOVQ         in_1_base+2904(FP), DX
	MOVQ         in_2_base+2928(FP), BX
	MOVQ         in_3_base+2952(FP), BP
	MOVQ         in_4_base+2976(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X9
	VPBROADCASTB X9, Y9
	XORQ         DI, DI

mulAvxTwo_5x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(DI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+1440(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+1472(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+1504(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+1536(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+1568(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+1600(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+1632(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+1664(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+1696(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(DI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+1728(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+1760(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+1792(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+1824(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+1856(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+1888(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+1920(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+1952(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+1984(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(DI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+2016(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+2048(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+2080(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+2112(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+2144(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+2176(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+2208(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+2240(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+2272(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(DI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+2304(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+2336(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+2368(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+2400(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+2432(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+2464(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+2496(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+2528(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+2560(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(DI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+2592(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+2624(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+2656(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+2688(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+2720(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+2752(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+2784(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+2816(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+2848(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+3000(FP), R8
	VMOVDQU Y0, (R8)(DI*1)
	MOVQ    out_1_base+3024(FP), R8
	VMOVDQU Y1, (R8)(DI*1)
	MOVQ    out_2_base+3048(FP), R8
	VMOVDQU Y2, (R8)(DI*1)
	MOVQ    out_3_base+3072(FP), R8
	VMOVDQU Y3, (R8)(DI*1)
	MOVQ    out_4_base+3096(FP), R8
	VMOVDQU Y4, (R8)(DI*1)
	MOVQ    out_5_base+3120(FP), R8
	VMOVDQU Y5, (R8)(DI*1)
	MOVQ    out_6_base+3144(FP), R8
	VMOVDQU Y6, (R8)(DI*1)
	MOVQ    out_7_base+3168(FP), R8
	VMOVDQU Y7, (R8)(DI*1)
	MOVQ    out_8_base+3192(FP), R8
	VMOVDQU Y8, (R8)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_5x9_loop
	VZEROUPPER

mulAvxTwo_5x9_end:
	RET

// func mulAvxTwo_5x10(low [100][16]byte, high [100][16]byte, in [5][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x10(SB), $0-3560
	// Loading no tables to registers
	// Full registers estimated 115 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3208(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x10_end
	MOVQ         in_0_base+3200(FP), CX
	MOVQ         in_1_base+3224(FP), DX
	MOVQ         in_2_base+3248(FP), BX
	MOVQ         in_3_base+3272(FP), BP
	MOVQ         in_4_base+3296(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X10
	VPBROADCASTB X10, Y10
	XORQ         DI, DI

mulAvxTwo_5x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(DI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+1600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+1632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+1664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+1696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+1728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+1760(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+1792(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+1824(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+1856(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+1888(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(DI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+1920(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+1952(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+1984(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+2016(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+2048(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+2080(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+2112(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+2144(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+2176(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+2208(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(DI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+2240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+2272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+2304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+2336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+2368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+2400(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+2432(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+2464(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+2496(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+2528(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(DI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+2560(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+2592(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+2624(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+2656(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+2688(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+2720(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+2752(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+2784(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+2816(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+2848(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(DI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+2880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+2912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+2944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+2976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+3008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+3040(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+3072(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+3104(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+3136(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+3168(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+3320(FP), R8
	VMOVDQU Y0, (R8)(DI*1)
	MOVQ    out_1_base+3344(FP), R8
	VMOVDQU Y1, (R8)(DI*1)
	MOVQ    out_2_base+3368(FP), R8
	VMOVDQU Y2, (R8)(DI*1)
	MOVQ    out_3_base+3392(FP), R8
	VMOVDQU Y3, (R8)(DI*1)
	MOVQ    out_4_base+3416(FP), R8
	VMOVDQU Y4, (R8)(DI*1)
	MOVQ    out_5_base+3440(FP), R8
	VMOVDQU Y5, (R8)(DI*1)
	MOVQ    out_6_base+3464(FP), R8
	VMOVDQU Y6, (R8)(DI*1)
	MOVQ    out_7_base+3488(FP), R8
	VMOVDQU Y7, (R8)(DI*1)
	MOVQ    out_8_base+3512(FP), R8
	VMOVDQU Y8, (R8)(DI*1)
	MOVQ    out_9_base+3536(FP), R8
	VMOVDQU Y9, (R8)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_5x10_loop
	VZEROUPPER

mulAvxTwo_5x10_end:
	RET

// func mulAvxTwo_6x1(low [6][16]byte, high [6][16]byte, in [6][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x1(SB), $0-360
	// Loading all tables to registers
	// Full registers estimated 16 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+200(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x1_end
	MOVQ         out_0_base+336(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+96(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+112(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+128(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+144(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVOU        low_4+64(FP), X9
	MOVOU        high_4+160(FP), X10
	VINSERTI128  $0x01, X9, Y9, Y9
	VINSERTI128  $0x01, X10, Y10, Y10
	MOVOU        low_5+80(FP), X11
	MOVOU        high_5+176(FP), X12
	VINSERTI128  $0x01, X11, Y11, Y11
	VINSERTI128  $0x01, X12, Y12, Y12
	MOVQ         in_0_base+192(FP), DX
	MOVQ         in_1_base+216(FP), BX
	MOVQ         in_2_base+240(FP), BP
	MOVQ         in_3_base+264(FP), SI
	MOVQ         in_4_base+288(FP), DI
	MOVQ         in_5_base+312(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X13
	VPBROADCASTB X13, Y13
	XORQ         R9, R9

mulAvxTwo_6x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y1, Y14
	VPSHUFB Y15, Y2, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y3, Y14
	VPSHUFB Y15, Y4, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y5, Y14
	VPSHUFB Y15, Y6, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y7, Y14
	VPSHUFB Y15, Y8, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y9, Y14
	VPSHUFB Y15, Y10, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y11, Y14
	VPSHUFB Y15, Y12, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_6x1_loop
	VZEROUPPER

mulAvxTwo_6x1_end:
	RET

// func mulAvxTwo_6x2(low [24][16]byte, high [24][16]byte, in [6][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x2(SB), $0-960
	// Loading no tables to registers
	// Full registers estimated 31 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+776(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x2_end
	MOVQ         out_0_base+912(FP), CX
	MOVQ         out_1_base+936(FP), DX
	MOVQ         in_0_base+768(FP), BX
	MOVQ         in_1_base+792(FP), BP
	MOVQ         in_2_base+816(FP), SI
	MOVQ         in_3_base+840(FP), DI
	MOVQ         in_4_base+864(FP), R8
	MOVQ         in_5_base+888(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X2
	VPBROADCASTB X2, Y2
	XORQ         R10, R10

mulAvxTwo_6x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R10*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+384(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+416(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R10*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+448(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+480(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R10*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+512(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+544(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R10*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+576(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+608(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R10*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+640(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+672(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (R9)(R10*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+704(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+736(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R10*1)
	VMOVDQU Y1, (DX)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_6x2_loop
	VZEROUPPER

mulAvxTwo_6x2_end:
	RET

// func mulAvxTwo_6x3(low [36][16]byte, high [36][16]byte, in [6][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x3(SB), $0-1368
	// Loading no tables to registers
	// Full registers estimated 44 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1160(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x3_end
	MOVQ         out_0_base+1296(FP), CX
	MOVQ         out_1_base+1320(FP), DX
	MOVQ         out_2_base+1344(FP), BX
	MOVQ         in_0_base+1152(FP), BP
	MOVQ         in_1_base+1176(FP), SI
	MOVQ         in_2_base+1200(FP), DI
	MOVQ         in_3_base+1224(FP), R8
	MOVQ         in_4_base+1248(FP), R9
	MOVQ         in_5_base+1272(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X3
	VPBROADCASTB X3, Y3
	XORQ         R11, R11

mulAvxTwo_6x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R11*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+576(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+608(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+640(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R11*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+672(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+704(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+736(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R11*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+768(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+800(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+832(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R11*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+864(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+896(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+928(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (R9)(R11*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+960(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+992(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1024(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (R10)(R11*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1056(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1088(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1120(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)
	VMOVDQU Y2, (BX)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_6x3_loop
	VZEROUPPER

mulAvxTwo_6x3_end:
	RET

// func mulAvxTwo_6x4(low [48][16]byte, high [48][16]byte, in [6][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x4(SB), $0-1776
	// Loading no tables to registers
	// Full registers estimated 57 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1544(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x4_end
	MOVQ         out_0_base+1680(FP), CX
	MOVQ         out_1_base+1704(FP), DX
	MOVQ         out_2_base+1728(FP), BX
	MOVQ         out_3_base+1752(FP), BP
	MOVQ         in_0_base+1536(FP), SI
	MOVQ         in_1_base+1560(FP), DI
	MOVQ         in_2_base+1584(FP), R8
	MOVQ         in_3_base+1608(FP), R9
	MOVQ         in_4_base+1632(FP), R10
	MOVQ         in_5_base+1656(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X4
	VPBROADCASTB X4, Y4
	XORQ         R12, R12

mulAvxTwo_6x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+768(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+800(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+832(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+864(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+896(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+928(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+960(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+992(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1024(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1056(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1088(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1120(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (R9)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1152(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1184(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1216(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1248(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (R10)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1280(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1312(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1344(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+1376(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (R11)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+1408(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+1440(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+1472(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+1504(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)
	VMOVDQU Y3, (BP)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_6x4_loop
	VZEROUPPER

mulAvxTwo_6x4_end:
	RET

// func mulAvxTwo_6x5(low [60][16]byte, high [60][16]byte, in [6][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x5(SB), $0-2184
	// Loading no tables to registers
	// Full registers estimated 70 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1928(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x5_end
	MOVQ         out_0_base+2064(FP), CX
	MOVQ         out_1_base+2088(FP), DX
	MOVQ         out_2_base+2112(FP), BX
	MOVQ         out_3_base+2136(FP), BP
	MOVQ         out_4_base+2160(FP), SI
	MOVQ         in_0_base+1920(FP), DI
	MOVQ         in_1_base+1944(FP), R8
	MOVQ         in_2_base+1968(FP), R9
	MOVQ         in_3_base+1992(FP), R10
	MOVQ         in_4_base+2016(FP), R11
	MOVQ         in_5_base+2040(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X5
	VPBROADCASTB X5, Y5
	XORQ         R13, R13

mulAvxTwo_6x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+960(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+992(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1024(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+1056(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+1088(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+1120(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+1152(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1184(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+1216(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+1248(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (R9)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+1280(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+1312(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1344(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+1376(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+1408(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (R10)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+1440(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+1472(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+1504(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+1536(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+1568(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (R11)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+1600(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+1632(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+1664(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+1696(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+1728(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (R12)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+1760(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+1792(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+1824(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+1856(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+1888(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)
	VMOVDQU Y4, (SI)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_6x5_loop
	VZEROUPPER

mulAvxTwo_6x5_end:
	RET

// func mulAvxTwo_6x6(low [72][16]byte, high [72][16]byte, in [6][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x6(SB), $0-2592
	// Loading no tables to registers
	// Full registers estimated 83 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2312(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x6_end
	MOVQ         out_0_base+2448(FP), CX
	MOVQ         out_1_base+2472(FP), DX
	MOVQ         out_2_base+2496(FP), BX
	MOVQ         out_3_base+2520(FP), BP
	MOVQ         out_4_base+2544(FP), SI
	MOVQ         out_5_base+2568(FP), DI
	MOVQ         in_0_base+2304(FP), R8
	MOVQ         in_1_base+2328(FP), R9
	MOVQ         in_2_base+2352(FP), R10
	MOVQ         in_3_base+2376(FP), R11
	MOVQ         in_4_base+2400(FP), R12
	MOVQ         in_5_base+2424(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X6
	VPBROADCASTB X6, Y6
	XORQ         R14, R14

mulAvxTwo_6x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+1152(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+1184(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+1216(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+1248(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+1280(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+1312(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (R9)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+1344(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+1376(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+1408(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+1440(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+1472(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+1504(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (R10)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+1536(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+1568(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+1600(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+1632(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+1664(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+1696(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (R11)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+1728(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+1760(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+1792(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+1824(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+1856(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+1888(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (R12)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+1920(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+1952(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+1984(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+2016(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+2048(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+2080(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (R13)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+2112(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+2144(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+2176(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+2208(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+2240(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+2272(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)
	VMOVDQU Y4, (SI)(R14*1)
	VMOVDQU Y5, (DI)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_6x6_loop
	VZEROUPPER

mulAvxTwo_6x6_end:
	RET

// func mulAvxTwo_6x7(low [84][16]byte, high [84][16]byte, in [6][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x7(SB), $0-3000
	// Loading no tables to registers
	// Full registers estimated 96 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2696(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x7_end
	MOVQ         out_0_base+2832(FP), CX
	MOVQ         out_1_base+2856(FP), DX
	MOVQ         out_2_base+2880(FP), BX
	MOVQ         out_3_base+2904(FP), BP
	MOVQ         out_4_base+2928(FP), SI
	MOVQ         out_5_base+2952(FP), DI
	MOVQ         out_6_base+2976(FP), R8
	MOVQ         in_0_base+2688(FP), R9
	MOVQ         in_1_base+2712(FP), R10
	MOVQ         in_2_base+2736(FP), R11
	MOVQ         in_3_base+2760(FP), R12
	MOVQ         in_4_base+2784(FP), R13
	MOVQ         in_5_base+2808(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X7
	VPBROADCASTB X7, Y7
	XORQ         R15, R15

mulAvxTwo_6x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (R9)(R15*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+1344(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+1376(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+1408(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+1440(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+1472(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+1504(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+1536(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (R10)(R15*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+1568(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+1600(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+1632(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+1664(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+1696(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+1728(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+1760(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (R11)(R15*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+1792(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+1824(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+1856(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+1888(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+1920(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+1952(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+1984(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (R12)(R15*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+2016(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+2048(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+2080(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+2112(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+2144(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+2176(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+2208(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (R13)(R15*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+2240(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+2272(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+2304(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+2336(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+2368(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+2400(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+2432(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (R14)(R15*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+2464(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+2496(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+2528(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+2560(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+2592(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+2624(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+2656(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)
	VMOVDQU Y4, (SI)(R15*1)
	VMOVDQU Y5, (DI)(R15*1)
	VMOVDQU Y6, (R8)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_6x7_loop
	VZEROUPPER

mulAvxTwo_6x7_end:
	RET

// func mulAvxTwo_6x8(low [96][16]byte, high [96][16]byte, in [6][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x8(SB), $0-3408
	// Loading no tables to registers
	// Full registers estimated 109 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3080(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x8_end
	MOVQ         in_0_base+3072(FP), CX
	MOVQ         in_1_base+3096(FP), DX
	MOVQ         in_2_base+3120(FP), BX
	MOVQ         in_3_base+3144(FP), BP
	MOVQ         in_4_base+3168(FP), SI
	MOVQ         in_5_base+3192(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X8
	VPBROADCASTB X8, Y8
	XORQ         R8, R8

mulAvxTwo_6x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R8*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+1536(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+1568(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+1600(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+1632(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+1664(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+1696(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+1728(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+1760(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R8*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+1792(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+1824(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+1856(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+1888(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+1920(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+1952(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+1984(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+2016(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R8*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+2048(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+2080(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+2112(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+2144(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+2176(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+2208(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+2240(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+2272(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R8*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+2304(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+2336(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+2368(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+2400(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+2432(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+2464(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+2496(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+2528(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R8*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+2560(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+2592(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+2624(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+2656(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+2688(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+2720(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+2752(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+2784(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R8*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+2816(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+2848(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+2880(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+2912(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+2944(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+2976(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+3008(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+3040(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+3216(FP), R9
	VMOVDQU Y0, (R9)(R8*1)
	MOVQ    out_1_base+3240(FP), R9
	VMOVDQU Y1, (R9)(R8*1)
	MOVQ    out_2_base+3264(FP), R9
	VMOVDQU Y2, (R9)(R8*1)
	MOVQ    out_3_base+3288(FP), R9
	VMOVDQU Y3, (R9)(R8*1)
	MOVQ    out_4_base+3312(FP), R9
	VMOVDQU Y4, (R9)(R8*1)
	MOVQ    out_5_base+3336(FP), R9
	VMOVDQU Y5, (R9)(R8*1)
	MOVQ    out_6_base+3360(FP), R9
	VMOVDQU Y6, (R9)(R8*1)
	MOVQ    out_7_base+3384(FP), R9
	VMOVDQU Y7, (R9)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_6x8_loop
	VZEROUPPER

mulAvxTwo_6x8_end:
	RET

// func mulAvxTwo_6x9(low [108][16]byte, high [108][16]byte, in [6][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x9(SB), $0-3816
	// Loading no tables to registers
	// Full registers estimated 122 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3464(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x9_end
	MOVQ         in_0_base+3456(FP), CX
	MOVQ         in_1_base+3480(FP), DX
	MOVQ         in_2_base+3504(FP), BX
	MOVQ         in_3_base+3528(FP), BP
	MOVQ         in_4_base+3552(FP), SI
	MOVQ         in_5_base+3576(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X9
	VPBROADCASTB X9, Y9
	XORQ         R8, R8

mulAvxTwo_6x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+1728(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+1760(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+1792(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+1824(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+1856(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+1888(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+1920(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+1952(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+1984(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+2016(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+2048(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+2080(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+2112(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+2144(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+2176(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+2208(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+2240(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+2272(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+2304(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+2336(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+2368(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+2400(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+2432(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+2464(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+2496(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+2528(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+2560(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+2592(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+2624(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+2656(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+2688(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+2720(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+2752(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+2784(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+2816(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+2848(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+2880(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+2912(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+2944(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+2976(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+3008(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+3040(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+3072(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+3104(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+3136(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+3168(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+3200(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+3232(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+3264(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+3296(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+3328(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+3360(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+3392(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+3424(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+3600(FP), R9
	VMOVDQU Y0, (R9)(R8*1)
	MOVQ    out_1_base+3624(FP), R9
	VMOVDQU Y1, (R9)(R8*1)
	MOVQ    out_2_base+3648(FP), R9
	VMOVDQU Y2, (R9)(R8*1)
	MOVQ    out_3_base+3672(FP), R9
	VMOVDQU Y3, (R9)(R8*1)
	MOVQ    out_4_base+3696(FP), R9
	VMOVDQU Y4, (R9)(R8*1)
	MOVQ    out_5_base+3720(FP), R9
	VMOVDQU Y5, (R9)(R8*1)
	MOVQ    out_6_base+3744(FP), R9
	VMOVDQU Y6, (R9)(R8*1)
	MOVQ    out_7_base+3768(FP), R9
	VMOVDQU Y7, (R9)(R8*1)
	MOVQ    out_8_base+3792(FP), R9
	VMOVDQU Y8, (R9)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_6x9_loop
	VZEROUPPER

mulAvxTwo_6x9_end:
	RET

// func mulAvxTwo_6x10(low [120][16]byte, high [120][16]byte, in [6][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x10(SB), $0-4224
	// Loading no tables to registers
	// Full registers estimated 135 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3848(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x10_end
	MOVQ         in_0_base+3840(FP), CX
	MOVQ         in_1_base+3864(FP), DX
	MOVQ         in_2_base+3888(FP), BX
	MOVQ         in_3_base+3912(FP), BP
	MOVQ         in_4_base+3936(FP), SI
	MOVQ         in_5_base+3960(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X10
	VPBROADCASTB X10, Y10
	XORQ         R8, R8

mulAvxTwo_6x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R8*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+1920(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+1952(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+1984(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+2016(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+2048(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+2080(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+2112(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+2144(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+2176(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+2208(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R8*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+2240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+2272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+2304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+2336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+2368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+2400(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+2432(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+2464(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+2496(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+2528(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R8*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+2560(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+2592(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+2624(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+2656(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+2688(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+2720(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+2752(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+2784(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+2816(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+2848(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R8*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+2880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+2912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+2944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+2976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+3008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+3040(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+3072(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+3104(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+3136(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+3168(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R8*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+3200(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+3232(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+3264(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+3296(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+3328(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+3360(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+3392(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+3424(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+3456(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+3488(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R8*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+3520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+3552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+3584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+3616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+3648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+3680(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+3712(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+3744(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+3776(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+3808(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+3984(FP), R9
	VMOVDQU Y0, (R9)(R8*1)
	MOVQ    out_1_base+4008(FP), R9
	VMOVDQU Y1, (R9)(R8*1)
	MOVQ    out_2_base+4032(FP), R9
	VMOVDQU Y2, (R9)(R8*1)
	MOVQ    out_3_base+4056(FP), R9
	VMOVDQU Y3, (R9)(R8*1)
	MOVQ    out_4_base+4080(FP), R9
	VMOVDQU Y4, (R9)(R8*1)
	MOVQ    out_5_base+4104(FP), R9
	VMOVDQU Y5, (R9)(R8*1)
	MOVQ    out_6_base+4128(FP), R9
	VMOVDQU Y6, (R9)(R8*1)
	MOVQ    out_7_base+4152(FP), R9
	VMOVDQU Y7, (R9)(R8*1)
	MOVQ    out_8_base+4176(FP), R9
	VMOVDQU Y8, (R9)(R8*1)
	MOVQ    out_9_base+4200(FP), R9
	VMOVDQU Y9, (R9)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_6x10_loop
	VZEROUPPER

mulAvxTwo_6x10_end:
	RET

// func mulAvxTwo_7x1(low [14][16]byte, high [14][16]byte, in [7][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x1(SB), $0-640
	// Loading no tables to registers
	// Full registers estimated 18 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+456(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x1_end
	MOVQ         out_0_base+616(FP), CX
	MOVQ         in_0_base+448(FP), DX
	MOVQ         in_1_base+472(FP), BX
	MOVQ         in_2_base+496(FP), BP
	MOVQ         in_3_base+520(FP), SI
	MOVQ         in_4_base+544(FP), DI
	MOVQ         in_5_base+568(FP), R8
	MOVQ         in_6_base+592(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X1
	VPBROADCASTB X1, Y1
	XORQ         R10, R10

mulAvxTwo_7x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_0+0(FP), Y2
	VMOVDQU high_0+224(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_2+32(FP), Y2
	VMOVDQU high_2+256(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_4+64(FP), Y2
	VMOVDQU high_4+288(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_6+96(FP), Y2
	VMOVDQU high_6+320(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_8+128(FP), Y2
	VMOVDQU high_8+352(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_10+160(FP), Y2
	VMOVDQU high_10+384(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 6 to 1 outputs
	VMOVDQU (R9)(R10*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_12+192(FP), Y2
	VMOVDQU high_12+416(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_7x1_loop
	VZEROUPPER

mulAvxTwo_7x1_end:
	RET

// func mulAvxTwo_7x2(low [28][16]byte, high [28][16]byte, in [7][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x2(SB), $0-1112
	// Loading no tables to registers
	// Full registers estimated 35 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+904(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x2_end
	MOVQ         out_0_base+1064(FP), CX
	MOVQ         out_1_base+1088(FP), DX
	MOVQ         in_0_base+896(FP), BX
	MOVQ         in_1_base+920(FP), BP
	MOVQ         in_2_base+944(FP), SI
	MOVQ         in_3_base+968(FP), DI
	MOVQ         in_4_base+992(FP), R8
	MOVQ         in_5_base+1016(FP), R9
	MOVQ         in_6_base+1040(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X2
	VPBROADCASTB X2, Y2
	XORQ         R11, R11

mulAvxTwo_7x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+448(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+480(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+512(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+544(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+576(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+608(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+640(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+672(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+704(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+736(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (R9)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+768(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+800(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 6 to 2 outputs
	VMOVDQU (R10)(R11*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_24+384(FP), Y3
	VMOVDQU high_24+832(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_26+416(FP), Y3
	VMOVDQU high_26+864(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R11*1)
	VMOVDQU Y1, (DX)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_7x2_loop
	VZEROUPPER

mulAvxTwo_7x2_end:
	RET

// func mulAvxTwo_7x3(low [42][16]byte, high [42][16]byte, in [7][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x3(SB), $0-1584
	// Loading no tables to registers
	// Full registers estimated 50 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1352(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x3_end
	MOVQ         out_0_base+1512(FP), CX
	MOVQ         out_1_base+1536(FP), DX
	MOVQ         out_2_base+1560(FP), BX
	MOVQ         in_0_base+1344(FP), BP
	MOVQ         in_1_base+1368(FP), SI
	MOVQ         in_2_base+1392(FP), DI
	MOVQ         in_3_base+1416(FP), R8
	MOVQ         in_4_base+1440(FP), R9
	MOVQ         in_5_base+1464(FP), R10
	MOVQ         in_6_base+1488(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X3
	VPBROADCASTB X3, Y3
	XORQ         R12, R12

mulAvxTwo_7x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+672(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+704(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+736(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+768(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+800(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+832(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+864(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+896(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+928(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+960(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+992(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+1024(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (R9)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+1056(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+1088(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1120(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (R10)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1152(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1184(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1216(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 6 to 3 outputs
	VMOVDQU (R11)(R12*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_36+576(FP), Y4
	VMOVDQU high_36+1248(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_38+608(FP), Y4
	VMOVDQU high_38+1280(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_40+640(FP), Y4
	VMOVDQU high_40+1312(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)
	VMOVDQU Y2, (BX)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_7x3_loop
	VZEROUPPER

mulAvxTwo_7x3_end:
	RET

// func mulAvxTwo_7x4(low [56][16]byte, high [56][16]byte, in [7][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x4(SB), $0-2056
	// Loading no tables to registers
	// Full registers estimated 65 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1800(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x4_end
	MOVQ         out_0_base+1960(FP), CX
	MOVQ         out_1_base+1984(FP), DX
	MOVQ         out_2_base+2008(FP), BX
	MOVQ         out_3_base+2032(FP), BP
	MOVQ         in_0_base+1792(FP), SI
	MOVQ         in_1_base+1816(FP), DI
	MOVQ         in_2_base+1840(FP), R8
	MOVQ         in_3_base+1864(FP), R9
	MOVQ         in_4_base+1888(FP), R10
	MOVQ         in_5_base+1912(FP), R11
	MOVQ         in_6_base+1936(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X4
	VPBROADCASTB X4, Y4
	XORQ         R13, R13

mulAvxTwo_7x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+896(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+928(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+960(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+992(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+1024(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+1056(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+1088(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+1120(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1152(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1184(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1216(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1248(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (R9)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1280(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1312(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1344(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1376(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (R10)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1408(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1440(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1472(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+1504(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (R11)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+1536(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+1568(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+1600(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+1632(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 6 to 4 outputs
	VMOVDQU (R12)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_48+768(FP), Y5
	VMOVDQU high_48+1664(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_50+800(FP), Y5
	VMOVDQU high_50+1696(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_52+832(FP), Y5
	VMOVDQU high_52+1728(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_54+864(FP), Y5
	VMOVDQU high_54+1760(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)
	VMOVDQU Y3, (BP)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_7x4_loop
	VZEROUPPER

mulAvxTwo_7x4_end:
	RET

// func mulAvxTwo_7x5(low [70][16]byte, high [70][16]byte, in [7][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x5(SB), $0-2528
	// Loading no tables to registers
	// Full registers estimated 80 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2248(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x5_end
	MOVQ         out_0_base+2408(FP), CX
	MOVQ         out_1_base+2432(FP), DX
	MOVQ         out_2_base+2456(FP), BX
	MOVQ         out_3_base+2480(FP), BP
	MOVQ         out_4_base+2504(FP), SI
	MOVQ         in_0_base+2240(FP), DI
	MOVQ         in_1_base+2264(FP), R8
	MOVQ         in_2_base+2288(FP), R9
	MOVQ         in_3_base+2312(FP), R10
	MOVQ         in_4_base+2336(FP), R11
	MOVQ         in_5_base+2360(FP), R12
	MOVQ         in_6_base+2384(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X5
	VPBROADCASTB X5, Y5
	XORQ         R14, R14

mulAvxTwo_7x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+1120(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+1152(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1184(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+1216(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+1248(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+1280(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+1312(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1344(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+1376(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+1408(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (R9)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+1440(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+1472(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1504(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+1536(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+1568(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (R10)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+1600(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+1632(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+1664(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+1696(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+1728(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (R11)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+1760(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+1792(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+1824(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+1856(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+1888(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (R12)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+1920(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+1952(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+1984(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+2016(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+2048(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 6 to 5 outputs
	VMOVDQU (R13)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_60+960(FP), Y6
	VMOVDQU high_60+2080(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_62+992(FP), Y6
	VMOVDQU high_62+2112(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_64+1024(FP), Y6
	VMOVDQU high_64+2144(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_66+1056(FP), Y6
	VMOVDQU high_66+2176(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_68+1088(FP), Y6
	VMOVDQU high_68+2208(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)
	VMOVDQU Y4, (SI)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_7x5_loop
	VZEROUPPER

mulAvxTwo_7x5_end:
	RET

// func mulAvxTwo_7x6(low [84][16]byte, high [84][16]byte, in [7][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x6(SB), $0-3000
	// Loading no tables to registers
	// Full registers estimated 95 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2696(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x6_end
	MOVQ         out_0_base+2856(FP), CX
	MOVQ         out_1_base+2880(FP), DX
	MOVQ         out_2_base+2904(FP), BX
	MOVQ         out_3_base+2928(FP), BP
	MOVQ         out_4_base+2952(FP), SI
	MOVQ         out_5_base+2976(FP), DI
	MOVQ         in_0_base+2688(FP), R8
	MOVQ         in_1_base+2712(FP), R9
	MOVQ         in_2_base+2736(FP), R10
	MOVQ         in_3_base+2760(FP), R11
	MOVQ         in_4_base+2784(FP), R12
	MOVQ         in_5_base+2808(FP), R13
	MOVQ         in_6_base+2832(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X6
	VPBROADCASTB X6, Y6
	XORQ         R15, R15

mulAvxTwo_7x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (R8)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+1344(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+1376(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+1408(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+1440(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+1472(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+1504(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (R9)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+1536(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+1568(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+1600(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+1632(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+1664(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+1696(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (R10)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+1728(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+1760(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+1792(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+1824(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+1856(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+1888(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (R11)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+1920(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+1952(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+1984(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+2016(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+2048(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+2080(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (R12)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+2112(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+2144(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+2176(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+2208(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+2240(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+2272(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (R13)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+2304(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+2336(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+2368(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+2400(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+2432(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+2464(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 6 to 6 outputs
	VMOVDQU (R14)(R15*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_72+1152(FP), Y7
	VMOVDQU high_72+2496(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_74+1184(FP), Y7
	VMOVDQU high_74+2528(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_76+1216(FP), Y7
	VMOVDQU high_76+2560(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_78+1248(FP), Y7
	VMOVDQU high_78+2592(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_80+1280(FP), Y7
	VMOVDQU high_80+2624(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_82+1312(FP), Y7
	VMOVDQU high_82+2656(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)
	VMOVDQU Y4, (SI)(R15*1)
	VMOVDQU Y5, (DI)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_7x6_loop
	VZEROUPPER

mulAvxTwo_7x6_end:
	RET

// func mulAvxTwo_7x7(low [98][16]byte, high [98][16]byte, in [7][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x7(SB), $0-3472
	// Loading no tables to registers
	// Full registers estimated 110 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3144(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x7_end
	MOVQ         in_0_base+3136(FP), CX
	MOVQ         in_1_base+3160(FP), DX
	MOVQ         in_2_base+3184(FP), BX
	MOVQ         in_3_base+3208(FP), BP
	MOVQ         in_4_base+3232(FP), SI
	MOVQ         in_5_base+3256(FP), DI
	MOVQ         in_6_base+3280(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X7
	VPBROADCASTB X7, Y7
	XORQ         R9, R9

mulAvxTwo_7x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (CX)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+1568(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+1600(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+1632(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+1664(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+1696(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+1728(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+1760(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (DX)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+1792(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+1824(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+1856(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+1888(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+1920(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+1952(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+1984(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (BX)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+2016(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+2048(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+2080(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+2112(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+2144(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+2176(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+2208(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (BP)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+2240(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+2272(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+2304(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+2336(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+2368(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+2400(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+2432(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (SI)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+2464(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+2496(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+2528(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+2560(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+2592(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+2624(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+2656(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (DI)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+2688(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+2720(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+2752(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+2784(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+2816(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+2848(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+2880(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 6 to 7 outputs
	VMOVDQU (R8)(R9*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_84+1344(FP), Y8
	VMOVDQU high_84+2912(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_86+1376(FP), Y8
	VMOVDQU high_86+2944(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_88+1408(FP), Y8
	VMOVDQU high_88+2976(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_90+1440(FP), Y8
	VMOVDQU high_90+3008(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_92+1472(FP), Y8
	VMOVDQU high_92+3040(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_94+1504(FP), Y8
	VMOVDQU high_94+3072(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_96+1536(FP), Y8
	VMOVDQU high_96+3104(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	MOVQ    out_0_base+3304(FP), R10
	VMOVDQU Y0, (R10)(R9*1)
	MOVQ    out_1_base+3328(FP), R10
	VMOVDQU Y1, (R10)(R9*1)
	MOVQ    out_2_base+3352(FP), R10
	VMOVDQU Y2, (R10)(R9*1)
	MOVQ    out_3_base+3376(FP), R10
	VMOVDQU Y3, (R10)(R9*1)
	MOVQ    out_4_base+3400(FP), R10
	VMOVDQU Y4, (R10)(R9*1)
	MOVQ    out_5_base+3424(FP), R10
	VMOVDQU Y5, (R10)(R9*1)
	MOVQ    out_6_base+3448(FP), R10
	VMOVDQU Y6, (R10)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_7x7_loop
	VZEROUPPER

mulAvxTwo_7x7_end:
	RET

// func mulAvxTwo_7x8(low [112][16]byte, high [112][16]byte, in [7][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x8(SB), $0-3944
	// Loading no tables to registers
	// Full registers estimated 125 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3592(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x8_end
	MOVQ         in_0_base+3584(FP), CX
	MOVQ         in_1_base+3608(FP), DX
	MOVQ         in_2_base+3632(FP), BX
	MOVQ         in_3_base+3656(FP), BP
	MOVQ         in_4_base+3680(FP), SI
	MOVQ         in_5_base+3704(FP), DI
	MOVQ         in_6_base+3728(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X8
	VPBROADCASTB X8, Y8
	XORQ         R9, R9

mulAvxTwo_7x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+1792(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+1824(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+1856(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+1888(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+1920(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+1952(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+1984(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+2016(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+2048(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+2080(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+2112(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+2144(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+2176(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+2208(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+2240(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+2272(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+2304(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+2336(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+2368(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+2400(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+2432(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+2464(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+2496(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+2528(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+2560(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+2592(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+2624(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+2656(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+2688(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+2720(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+2752(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+2784(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+2816(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+2848(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+2880(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+2912(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+2944(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+2976(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+3008(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+3040(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+3072(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+3104(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+3136(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+3168(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+3200(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+3232(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+3264(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+3296(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 6 to 8 outputs
	VMOVDQU (R8)(R9*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_96+1536(FP), Y9
	VMOVDQU high_96+3328(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_98+1568(FP), Y9
	VMOVDQU high_98+3360(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_100+1600(FP), Y9
	VMOVDQU high_100+3392(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_102+1632(FP), Y9
	VMOVDQU high_102+3424(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_104+1664(FP), Y9
	VMOVDQU high_104+3456(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_106+1696(FP), Y9
	VMOVDQU high_106+3488(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_108+1728(FP), Y9
	VMOVDQU high_108+3520(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_110+1760(FP), Y9
	VMOVDQU high_110+3552(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+3752(FP), R10
	VMOVDQU Y0, (R10)(R9*1)
	MOVQ    out_1_base+3776(FP), R10
	VMOVDQU Y1, (R10)(R9*1)
	MOVQ    out_2_base+3800(FP), R10
	VMOVDQU Y2, (R10)(R9*1)
	MOVQ    out_3_base+3824(FP), R10
	VMOVDQU Y3, (R10)(R9*1)
	MOVQ    out_4_base+3848(FP), R10
	VMOVDQU Y4, (R10)(R9*1)
	MOVQ    out_5_base+3872(FP), R10
	VMOVDQU Y5, (R10)(R9*1)
	MOVQ    out_6_base+3896(FP), R10
	VMOVDQU Y6, (R10)(R9*1)
	MOVQ    out_7_base+3920(FP), R10
	VMOVDQU Y7, (R10)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_7x8_loop
	VZEROUPPER

mulAvxTwo_7x8_end:
	RET

// func mulAvxTwo_7x9(low [126][16]byte, high [126][16]byte, in [7][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x9(SB), $0-4416
	// Loading no tables to registers
	// Full registers estimated 140 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4040(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x9_end
	MOVQ         in_0_base+4032(FP), CX
	MOVQ         in_1_base+4056(FP), DX
	MOVQ         in_2_base+4080(FP), BX
	MOVQ         in_3_base+4104(FP), BP
	MOVQ         in_4_base+4128(FP), SI
	MOVQ         in_5_base+4152(FP), DI
	MOVQ         in_6_base+4176(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X9
	VPBROADCASTB X9, Y9
	XORQ         R9, R9

mulAvxTwo_7x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+2016(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+2048(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+2080(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+2112(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+2144(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+2176(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+2208(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+2240(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+2272(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+2304(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+2336(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+2368(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+2400(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+2432(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+2464(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+2496(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+2528(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+2560(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+2592(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+2624(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+2656(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+2688(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+2720(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+2752(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+2784(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+2816(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+2848(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+2880(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+2912(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+2944(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+2976(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+3008(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+3040(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+3072(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+3104(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+3136(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+3168(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+3200(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+3232(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+3264(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+3296(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+3328(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+3360(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+3392(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+3424(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+3456(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+3488(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+3520(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+3552(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+3584(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+3616(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+3648(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+3680(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+3712(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 6 to 9 outputs
	VMOVDQU (R8)(R9*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_108+1728(FP), Y10
	VMOVDQU high_108+3744(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_110+1760(FP), Y10
	VMOVDQU high_110+3776(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_112+1792(FP), Y10
	VMOVDQU high_112+3808(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_114+1824(FP), Y10
	VMOVDQU high_114+3840(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_116+1856(FP), Y10
	VMOVDQU high_116+3872(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_118+1888(FP), Y10
	VMOVDQU high_118+3904(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_120+1920(FP), Y10
	VMOVDQU high_120+3936(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_122+1952(FP), Y10
	VMOVDQU high_122+3968(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_124+1984(FP), Y10
	VMOVDQU high_124+4000(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+4200(FP), R10
	VMOVDQU Y0, (R10)(R9*1)
	MOVQ    out_1_base+4224(FP), R10
	VMOVDQU Y1, (R10)(R9*1)
	MOVQ    out_2_base+4248(FP), R10
	VMOVDQU Y2, (R10)(R9*1)
	MOVQ    out_3_base+4272(FP), R10
	VMOVDQU Y3, (R10)(R9*1)
	MOVQ    out_4_base+4296(FP), R10
	VMOVDQU Y4, (R10)(R9*1)
	MOVQ    out_5_base+4320(FP), R10
	VMOVDQU Y5, (R10)(R9*1)
	MOVQ    out_6_base+4344(FP), R10
	VMOVDQU Y6, (R10)(R9*1)
	MOVQ    out_7_base+4368(FP), R10
	VMOVDQU Y7, (R10)(R9*1)
	MOVQ    out_8_base+4392(FP), R10
	VMOVDQU Y8, (R10)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_7x9_loop
	VZEROUPPER

mulAvxTwo_7x9_end:
	RET

// func mulAvxTwo_7x10(low [140][16]byte, high [140][16]byte, in [7][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_7x10(SB), $0-4888
	// Loading no tables to registers
	// Full registers estimated 155 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4488(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_7x10_end
	MOVQ         in_0_base+4480(FP), CX
	MOVQ         in_1_base+4504(FP), DX
	MOVQ         in_2_base+4528(FP), BX
	MOVQ         in_3_base+4552(FP), BP
	MOVQ         in_4_base+4576(FP), SI
	MOVQ         in_5_base+4600(FP), DI
	MOVQ         in_6_base+4624(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X10
	VPBROADCASTB X10, Y10
	XORQ         R9, R9

mulAvxTwo_7x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+2240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+2272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+2304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+2336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+2368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+2400(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+2432(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+2464(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+2496(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+2528(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+2560(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+2592(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+2624(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+2656(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+2688(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+2720(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+2752(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+2784(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+2816(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+2848(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+2880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+2912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+2944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+2976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+3008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+3040(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+3072(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+3104(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+3136(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+3168(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+3200(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+3232(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+3264(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+3296(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+3328(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+3360(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+3392(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+3424(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+3456(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+3488(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+3520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+3552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+3584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+3616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+3648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+3680(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+3712(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+3744(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+3776(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+3808(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+3840(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+3872(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+3904(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+3936(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+3968(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+4000(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+4032(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+4064(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+4096(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+4128(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 6 to 10 outputs
	VMOVDQU (R8)(R9*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_120+1920(FP), Y11
	VMOVDQU high_120+4160(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_122+1952(FP), Y11
	VMOVDQU high_122+4192(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_124+1984(FP), Y11
	VMOVDQU high_124+4224(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_126+2016(FP), Y11
	VMOVDQU high_126+4256(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_128+2048(FP), Y11
	VMOVDQU high_128+4288(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_130+2080(FP), Y11
	VMOVDQU high_130+4320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_132+2112(FP), Y11
	VMOVDQU high_132+4352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_134+2144(FP), Y11
	VMOVDQU high_134+4384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_136+2176(FP), Y11
	VMOVDQU high_136+4416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_138+2208(FP), Y11
	VMOVDQU high_138+4448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+4648(FP), R10
	VMOVDQU Y0, (R10)(R9*1)
	MOVQ    out_1_base+4672(FP), R10
	VMOVDQU Y1, (R10)(R9*1)
	MOVQ    out_2_base+4696(FP), R10
	VMOVDQU Y2, (R10)(R9*1)
	MOVQ    out_3_base+4720(FP), R10
	VMOVDQU Y3, (R10)(R9*1)
	MOVQ    out_4_base+4744(FP), R10
	VMOVDQU Y4, (R10)(R9*1)
	MOVQ    out_5_base+4768(FP), R10
	VMOVDQU Y5, (R10)(R9*1)
	MOVQ    out_6_base+4792(FP), R10
	VMOVDQU Y6, (R10)(R9*1)
	MOVQ    out_7_base+4816(FP), R10
	VMOVDQU Y7, (R10)(R9*1)
	MOVQ    out_8_base+4840(FP), R10
	VMOVDQU Y8, (R10)(R9*1)
	MOVQ    out_9_base+4864(FP), R10
	VMOVDQU Y9, (R10)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_7x10_loop
	VZEROUPPER

mulAvxTwo_7x10_end:
	RET

// func mulAvxTwo_8x1(low [16][16]byte, high [16][16]byte, in [8][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x1(SB), $0-728
	// Loading no tables to registers
	// Full registers estimated 20 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+520(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x1_end
	MOVQ         out_0_base+704(FP), CX
	MOVQ         in_0_base+512(FP), DX
	MOVQ         in_1_base+536(FP), BX
	MOVQ         in_2_base+560(FP), BP
	MOVQ         in_3_base+584(FP), SI
	MOVQ         in_4_base+608(FP), DI
	MOVQ         in_5_base+632(FP), R8
	MOVQ         in_6_base+656(FP), R9
	MOVQ         in_7_base+680(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X1
	VPBROADCASTB X1, Y1
	XORQ         R11, R11

mulAvxTwo_8x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_0+0(FP), Y2
	VMOVDQU high_0+256(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_2+32(FP), Y2
	VMOVDQU high_2+288(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_4+64(FP), Y2
	VMOVDQU high_4+320(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_6+96(FP), Y2
	VMOVDQU high_6+352(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_8+128(FP), Y2
	VMOVDQU high_8+384(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_10+160(FP), Y2
	VMOVDQU high_10+416(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 6 to 1 outputs
	VMOVDQU (R9)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_12+192(FP), Y2
	VMOVDQU high_12+448(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 7 to 1 outputs
	VMOVDQU (R10)(R11*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_14+224(FP), Y2
	VMOVDQU high_14+480(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_8x1_loop
	VZEROUPPER

mulAvxTwo_8x1_end:
	RET

// func mulAvxTwo_8x2(low [32][16]byte, high [32][16]byte, in [8][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x2(SB), $0-1264
	// Loading no tables to registers
	// Full registers estimated 39 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1032(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x2_end
	MOVQ         out_0_base+1216(FP), CX
	MOVQ         out_1_base+1240(FP), DX
	MOVQ         in_0_base+1024(FP), BX
	MOVQ         in_1_base+1048(FP), BP
	MOVQ         in_2_base+1072(FP), SI
	MOVQ         in_3_base+1096(FP), DI
	MOVQ         in_4_base+1120(FP), R8
	MOVQ         in_5_base+1144(FP), R9
	MOVQ         in_6_base+1168(FP), R10
	MOVQ         in_7_base+1192(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X2
	VPBROADCASTB X2, Y2
	XORQ         R12, R12

mulAvxTwo_8x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+512(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+544(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+576(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+608(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+640(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+672(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+704(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+736(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+768(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+800(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (R9)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+832(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+864(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 6 to 2 outputs
	VMOVDQU (R10)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_24+384(FP), Y3
	VMOVDQU high_24+896(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_26+416(FP), Y3
	VMOVDQU high_26+928(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 7 to 2 outputs
	VMOVDQU (R11)(R12*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_28+448(FP), Y3
	VMOVDQU high_28+960(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_30+480(FP), Y3
	VMOVDQU high_30+992(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R12*1)
	VMOVDQU Y1, (DX)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_8x2_loop
	VZEROUPPER

mulAvxTwo_8x2_end:
	RET

// func mulAvxTwo_8x3(low [48][16]byte, high [48][16]byte, in [8][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x3(SB), $0-1800
	// Loading no tables to registers
	// Full registers estimated 56 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1544(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x3_end
	MOVQ         out_0_base+1728(FP), CX
	MOVQ         out_1_base+1752(FP), DX
	MOVQ         out_2_base+1776(FP), BX
	MOVQ         in_0_base+1536(FP), BP
	MOVQ         in_1_base+1560(FP), SI
	MOVQ         in_2_base+1584(FP), DI
	MOVQ         in_3_base+1608(FP), R8
	MOVQ         in_4_base+1632(FP), R9
	MOVQ         in_5_base+1656(FP), R10
	MOVQ         in_6_base+1680(FP), R11
	MOVQ         in_7_base+1704(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X3
	VPBROADCASTB X3, Y3
	XORQ         R13, R13

mulAvxTwo_8x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+768(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+800(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+832(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+864(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+896(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+928(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+960(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+992(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+1024(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+1056(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+1088(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+1120(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (R9)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+1152(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+1184(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1216(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (R10)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1248(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1280(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1312(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 6 to 3 outputs
	VMOVDQU (R11)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_36+576(FP), Y4
	VMOVDQU high_36+1344(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_38+608(FP), Y4
	VMOVDQU high_38+1376(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_40+640(FP), Y4
	VMOVDQU high_40+1408(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 7 to 3 outputs
	VMOVDQU (R12)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_42+672(FP), Y4
	VMOVDQU high_42+1440(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_44+704(FP), Y4
	VMOVDQU high_44+1472(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_46+736(FP), Y4
	VMOVDQU high_46+1504(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)
	VMOVDQU Y2, (BX)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_8x3_loop
	VZEROUPPER

mulAvxTwo_8x3_end:
	RET

// func mulAvxTwo_8x4(low [64][16]byte, high [64][16]byte, in [8][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x4(SB), $0-2336
	// Loading no tables to registers
	// Full registers estimated 73 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2056(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x4_end
	MOVQ         out_0_base+2240(FP), CX
	MOVQ         out_1_base+2264(FP), DX
	MOVQ         out_2_base+2288(FP), BX
	MOVQ         out_3_base+2312(FP), BP
	MOVQ         in_0_base+2048(FP), SI
	MOVQ         in_1_base+2072(FP), DI
	MOVQ         in_2_base+2096(FP), R8
	MOVQ         in_3_base+2120(FP), R9
	MOVQ         in_4_base+2144(FP), R10
	MOVQ         in_5_base+2168(FP), R11
	MOVQ         in_6_base+2192(FP), R12
	MOVQ         in_7_base+2216(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X4
	VPBROADCASTB X4, Y4
	XORQ         R14, R14

mulAvxTwo_8x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+1024(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+1056(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+1088(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+1120(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+1152(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+1184(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+1216(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+1248(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1280(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1312(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1344(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1376(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (R9)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1408(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1440(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1472(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1504(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (R10)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1536(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1568(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1600(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+1632(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (R11)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+1664(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+1696(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+1728(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+1760(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 6 to 4 outputs
	VMOVDQU (R12)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_48+768(FP), Y5
	VMOVDQU high_48+1792(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_50+800(FP), Y5
	VMOVDQU high_50+1824(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_52+832(FP), Y5
	VMOVDQU high_52+1856(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_54+864(FP), Y5
	VMOVDQU high_54+1888(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 7 to 4 outputs
	VMOVDQU (R13)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_56+896(FP), Y5
	VMOVDQU high_56+1920(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_58+928(FP), Y5
	VMOVDQU high_58+1952(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_60+960(FP), Y5
	VMOVDQU high_60+1984(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_62+992(FP), Y5
	VMOVDQU high_62+2016(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)
	VMOVDQU Y3, (BP)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_8x4_loop
	VZEROUPPER

mulAvxTwo_8x4_end:
	RET

// func mulAvxTwo_8x5(low [80][16]byte, high [80][16]byte, in [8][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x5(SB), $0-2872
	// Loading no tables to registers
	// Full registers estimated 90 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2568(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x5_end
	MOVQ         out_0_base+2752(FP), CX
	MOVQ         out_1_base+2776(FP), DX
	MOVQ         out_2_base+2800(FP), BX
	MOVQ         out_3_base+2824(FP), BP
	MOVQ         out_4_base+2848(FP), SI
	MOVQ         in_0_base+2560(FP), DI
	MOVQ         in_1_base+2584(FP), R8
	MOVQ         in_2_base+2608(FP), R9
	MOVQ         in_3_base+2632(FP), R10
	MOVQ         in_4_base+2656(FP), R11
	MOVQ         in_5_base+2680(FP), R12
	MOVQ         in_6_base+2704(FP), R13
	MOVQ         in_7_base+2728(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X5
	VPBROADCASTB X5, Y5
	XORQ         R15, R15

mulAvxTwo_8x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (DI)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+1280(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+1312(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1344(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+1376(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+1408(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (R8)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+1440(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+1472(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1504(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+1536(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+1568(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (R9)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+1600(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+1632(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1664(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+1696(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+1728(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (R10)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+1760(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+1792(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+1824(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+1856(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+1888(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (R11)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+1920(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+1952(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+1984(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+2016(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+2048(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (R12)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+2080(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+2112(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+2144(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+2176(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+2208(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 6 to 5 outputs
	VMOVDQU (R13)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_60+960(FP), Y6
	VMOVDQU high_60+2240(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_62+992(FP), Y6
	VMOVDQU high_62+2272(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_64+1024(FP), Y6
	VMOVDQU high_64+2304(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_66+1056(FP), Y6
	VMOVDQU high_66+2336(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_68+1088(FP), Y6
	VMOVDQU high_68+2368(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 7 to 5 outputs
	VMOVDQU (R14)(R15*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_70+1120(FP), Y6
	VMOVDQU high_70+2400(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_72+1152(FP), Y6
	VMOVDQU high_72+2432(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_74+1184(FP), Y6
	VMOVDQU high_74+2464(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_76+1216(FP), Y6
	VMOVDQU high_76+2496(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_78+1248(FP), Y6
	VMOVDQU high_78+2528(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)
	VMOVDQU Y4, (SI)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_8x5_loop
	VZEROUPPER

mulAvxTwo_8x5_end:
	RET

// func mulAvxTwo_8x6(low [96][16]byte, high [96][16]byte, in [8][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x6(SB), $0-3408
	// Loading no tables to registers
	// Full registers estimated 107 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3080(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x6_end
	MOVQ         in_0_base+3072(FP), CX
	MOVQ         in_1_base+3096(FP), DX
	MOVQ         in_2_base+3120(FP), BX
	MOVQ         in_3_base+3144(FP), BP
	MOVQ         in_4_base+3168(FP), SI
	MOVQ         in_5_base+3192(FP), DI
	MOVQ         in_6_base+3216(FP), R8
	MOVQ         in_7_base+3240(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X6
	VPBROADCASTB X6, Y6
	XORQ         R10, R10

mulAvxTwo_8x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (CX)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+1536(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+1568(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+1600(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+1632(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+1664(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+1696(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (DX)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+1728(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+1760(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+1792(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+1824(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+1856(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+1888(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (BX)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+1920(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+1952(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+1984(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+2016(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+2048(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+2080(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (BP)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+2112(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+2144(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+2176(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+2208(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+2240(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+2272(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (SI)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+2304(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+2336(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+2368(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+2400(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+2432(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+2464(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (DI)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+2496(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+2528(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+2560(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+2592(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+2624(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+2656(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 6 to 6 outputs
	VMOVDQU (R8)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_72+1152(FP), Y7
	VMOVDQU high_72+2688(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_74+1184(FP), Y7
	VMOVDQU high_74+2720(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_76+1216(FP), Y7
	VMOVDQU high_76+2752(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_78+1248(FP), Y7
	VMOVDQU high_78+2784(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_80+1280(FP), Y7
	VMOVDQU high_80+2816(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_82+1312(FP), Y7
	VMOVDQU high_82+2848(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 7 to 6 outputs
	VMOVDQU (R9)(R10*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_84+1344(FP), Y7
	VMOVDQU high_84+2880(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_86+1376(FP), Y7
	VMOVDQU high_86+2912(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_88+1408(FP), Y7
	VMOVDQU high_88+2944(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_90+1440(FP), Y7
	VMOVDQU high_90+2976(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_92+1472(FP), Y7
	VMOVDQU high_92+3008(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_94+1504(FP), Y7
	VMOVDQU high_94+3040(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	MOVQ    out_0_base+3264(FP), R11
	VMOVDQU Y0, (R11)(R10*1)
	MOVQ    out_1_base+3288(FP), R11
	VMOVDQU Y1, (R11)(R10*1)
	MOVQ    out_2_base+3312(FP), R11
	VMOVDQU Y2, (R11)(R10*1)
	MOVQ    out_3_base+3336(FP), R11
	VMOVDQU Y3, (R11)(R10*1)
	MOVQ    out_4_base+3360(FP), R11
	VMOVDQU Y4, (R11)(R10*1)
	MOVQ    out_5_base+3384(FP), R11
	VMOVDQU Y5, (R11)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_8x6_loop
	VZEROUPPER

mulAvxTwo_8x6_end:
	RET

// func mulAvxTwo_8x7(low [112][16]byte, high [112][16]byte, in [8][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x7(SB), $0-3944
	// Loading no tables to registers
	// Full registers estimated 124 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3592(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x7_end
	MOVQ         in_0_base+3584(FP), CX
	MOVQ         in_1_base+3608(FP), DX
	MOVQ         in_2_base+3632(FP), BX
	MOVQ         in_3_base+3656(FP), BP
	MOVQ         in_4_base+3680(FP), SI
	MOVQ         in_5_base+3704(FP), DI
	MOVQ         in_6_base+3728(FP), R8
	MOVQ         in_7_base+3752(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X7
	VPBROADCASTB X7, Y7
	XORQ         R10, R10

mulAvxTwo_8x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (CX)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+1792(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+1824(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+1856(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+1888(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+1920(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+1952(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+1984(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (DX)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+2016(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+2048(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+2080(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+2112(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+2144(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+2176(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+2208(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (BX)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+2240(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+2272(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+2304(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+2336(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+2368(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+2400(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+2432(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (BP)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+2464(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+2496(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+2528(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+2560(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+2592(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+2624(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+2656(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (SI)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+2688(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+2720(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+2752(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+2784(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+2816(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+2848(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+2880(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (DI)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+2912(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+2944(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+2976(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+3008(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+3040(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+3072(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+3104(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 6 to 7 outputs
	VMOVDQU (R8)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_84+1344(FP), Y8
	VMOVDQU high_84+3136(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_86+1376(FP), Y8
	VMOVDQU high_86+3168(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_88+1408(FP), Y8
	VMOVDQU high_88+3200(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_90+1440(FP), Y8
	VMOVDQU high_90+3232(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_92+1472(FP), Y8
	VMOVDQU high_92+3264(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_94+1504(FP), Y8
	VMOVDQU high_94+3296(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_96+1536(FP), Y8
	VMOVDQU high_96+3328(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 7 to 7 outputs
	VMOVDQU (R9)(R10*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_98+1568(FP), Y8
	VMOVDQU high_98+3360(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_100+1600(FP), Y8
	VMOVDQU high_100+3392(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_102+1632(FP), Y8
	VMOVDQU high_102+3424(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_104+1664(FP), Y8
	VMOVDQU high_104+3456(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_106+1696(FP), Y8
	VMOVDQU high_106+3488(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_108+1728(FP), Y8
	VMOVDQU high_108+3520(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_110+1760(FP), Y8
	VMOVDQU high_110+3552(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	MOVQ    out_0_base+3776(FP), R11
	VMOVDQU Y0, (R11)(R10*1)
	MOVQ    out_1_base+3800(FP), R11
	VMOVDQU Y1, (R11)(R10*1)
	MOVQ    out_2_base+3824(FP), R11
	VMOVDQU Y2, (R11)(R10*1)
	MOVQ    out_3_base+3848(FP), R11
	VMOVDQU Y3, (R11)(R10*1)
	MOVQ    out_4_base+3872(FP), R11
	VMOVDQU Y4, (R11)(R10*1)
	MOVQ    out_5_base+3896(FP), R11
	VMOVDQU Y5, (R11)(R10*1)
	MOVQ    out_6_base+3920(FP), R11
	VMOVDQU Y6, (R11)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_8x7_loop
	VZEROUPPER

mulAvxTwo_8x7_end:
	RET

// func mulAvxTwo_8x8(low [128][16]byte, high [128][16]byte, in [8][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x8(SB), $0-4480
	// Loading no tables to registers
	// Full registers estimated 141 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x8_end
	MOVQ         in_0_base+4096(FP), CX
	MOVQ         in_1_base+4120(FP), DX
	MOVQ         in_2_base+4144(FP), BX
	MOVQ         in_3_base+4168(FP), BP
	MOVQ         in_4_base+4192(FP), SI
	MOVQ         in_5_base+4216(FP), DI
	MOVQ         in_6_base+4240(FP), R8
	MOVQ         in_7_base+4264(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X8
	VPBROADCASTB X8, Y8
	XORQ         R10, R10

mulAvxTwo_8x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+2048(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+2080(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+2112(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+2144(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+2176(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+2208(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+2240(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+2272(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+2304(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+2336(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+2368(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+2400(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+2432(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+2464(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+2496(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+2528(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+2560(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+2592(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+2624(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+2656(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+2688(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+2720(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+2752(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+2784(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+2816(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+2848(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+2880(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+2912(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+2944(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+2976(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+3008(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+3040(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+3072(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+3104(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+3136(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+3168(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+3200(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+3232(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+3264(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+3296(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+3328(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+3360(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+3392(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+3424(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+3456(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+3488(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+3520(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+3552(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 6 to 8 outputs
	VMOVDQU (R8)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_96+1536(FP), Y9
	VMOVDQU high_96+3584(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_98+1568(FP), Y9
	VMOVDQU high_98+3616(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_100+1600(FP), Y9
	VMOVDQU high_100+3648(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_102+1632(FP), Y9
	VMOVDQU high_102+3680(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_104+1664(FP), Y9
	VMOVDQU high_104+3712(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_106+1696(FP), Y9
	VMOVDQU high_106+3744(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_108+1728(FP), Y9
	VMOVDQU high_108+3776(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_110+1760(FP), Y9
	VMOVDQU high_110+3808(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 7 to 8 outputs
	VMOVDQU (R9)(R10*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_112+1792(FP), Y9
	VMOVDQU high_112+3840(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_114+1824(FP), Y9
	VMOVDQU high_114+3872(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_116+1856(FP), Y9
	VMOVDQU high_116+3904(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_118+1888(FP), Y9
	VMOVDQU high_118+3936(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_120+1920(FP), Y9
	VMOVDQU high_120+3968(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_122+1952(FP), Y9
	VMOVDQU high_122+4000(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_124+1984(FP), Y9
	VMOVDQU high_124+4032(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_126+2016(FP), Y9
	VMOVDQU high_126+4064(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+4288(FP), R11
	VMOVDQU Y0, (R11)(R10*1)
	MOVQ    out_1_base+4312(FP), R11
	VMOVDQU Y1, (R11)(R10*1)
	MOVQ    out_2_base+4336(FP), R11
	VMOVDQU Y2, (R11)(R10*1)
	MOVQ    out_3_base+4360(FP), R11
	VMOVDQU Y3, (R11)(R10*1)
	MOVQ    out_4_base+4384(FP), R11
	VMOVDQU Y4, (R11)(R10*1)
	MOVQ    out_5_base+4408(FP), R11
	VMOVDQU Y5, (R11)(R10*1)
	MOVQ    out_6_base+4432(FP), R11
	VMOVDQU Y6, (R11)(R10*1)
	MOVQ    out_7_base+4456(FP), R11
	VMOVDQU Y7, (R11)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_8x8_loop
	VZEROUPPER

mulAvxTwo_8x8_end:
	RET

// func mulAvxTwo_8x9(low [144][16]byte, high [144][16]byte, in [8][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x9(SB), $0-5016
	// Loading no tables to registers
	// Full registers estimated 158 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4616(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x9_end
	MOVQ         in_0_base+4608(FP), CX
	MOVQ         in_1_base+4632(FP), DX
	MOVQ         in_2_base+4656(FP), BX
	MOVQ         in_3_base+4680(FP), BP
	MOVQ         in_4_base+4704(FP), SI
	MOVQ         in_5_base+4728(FP), DI
	MOVQ         in_6_base+4752(FP), R8
	MOVQ         in_7_base+4776(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X9
	VPBROADCASTB X9, Y9
	XORQ         R10, R10

mulAvxTwo_8x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+2304(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+2336(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+2368(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+2400(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+2432(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+2464(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+2496(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+2528(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+2560(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+2592(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+2624(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+2656(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+2688(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+2720(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+2752(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+2784(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+2816(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+2848(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+2880(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+2912(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+2944(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+2976(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+3008(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+3040(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+3072(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+3104(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+3136(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+3168(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+3200(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+3232(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+3264(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+3296(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+3328(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+3360(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+3392(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+3424(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+3456(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+3488(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+3520(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+3552(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+3584(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+3616(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+3648(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+3680(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+3712(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+3744(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+3776(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+3808(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+3840(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+3872(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+3904(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+3936(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+3968(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+4000(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 6 to 9 outputs
	VMOVDQU (R8)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_108+1728(FP), Y10
	VMOVDQU high_108+4032(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_110+1760(FP), Y10
	VMOVDQU high_110+4064(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_112+1792(FP), Y10
	VMOVDQU high_112+4096(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_114+1824(FP), Y10
	VMOVDQU high_114+4128(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_116+1856(FP), Y10
	VMOVDQU high_116+4160(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_118+1888(FP), Y10
	VMOVDQU high_118+4192(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_120+1920(FP), Y10
	VMOVDQU high_120+4224(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_122+1952(FP), Y10
	VMOVDQU high_122+4256(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_124+1984(FP), Y10
	VMOVDQU high_124+4288(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 7 to 9 outputs
	VMOVDQU (R9)(R10*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_126+2016(FP), Y10
	VMOVDQU high_126+4320(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_128+2048(FP), Y10
	VMOVDQU high_128+4352(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_130+2080(FP), Y10
	VMOVDQU high_130+4384(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_132+2112(FP), Y10
	VMOVDQU high_132+4416(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_134+2144(FP), Y10
	VMOVDQU high_134+4448(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_136+2176(FP), Y10
	VMOVDQU high_136+4480(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_138+2208(FP), Y10
	VMOVDQU high_138+4512(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_140+2240(FP), Y10
	VMOVDQU high_140+4544(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_142+2272(FP), Y10
	VMOVDQU high_142+4576(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+4800(FP), R11
	VMOVDQU Y0, (R11)(R10*1)
	MOVQ    out_1_base+4824(FP), R11
	VMOVDQU Y1, (R11)(R10*1)
	MOVQ    out_2_base+4848(FP), R11
	VMOVDQU Y2, (R11)(R10*1)
	MOVQ    out_3_base+4872(FP), R11
	VMOVDQU Y3, (R11)(R10*1)
	MOVQ    out_4_base+4896(FP), R11
	VMOVDQU Y4, (R11)(R10*1)
	MOVQ    out_5_base+4920(FP), R11
	VMOVDQU Y5, (R11)(R10*1)
	MOVQ    out_6_base+4944(FP), R11
	VMOVDQU Y6, (R11)(R10*1)
	MOVQ    out_7_base+4968(FP), R11
	VMOVDQU Y7, (R11)(R10*1)
	MOVQ    out_8_base+4992(FP), R11
	VMOVDQU Y8, (R11)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_8x9_loop
	VZEROUPPER

mulAvxTwo_8x9_end:
	RET

// func mulAvxTwo_8x10(low [160][16]byte, high [160][16]byte, in [8][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_8x10(SB), $0-5552
	// Loading no tables to registers
	// Full registers estimated 175 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5128(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_8x10_end
	MOVQ         in_0_base+5120(FP), CX
	MOVQ         in_1_base+5144(FP), DX
	MOVQ         in_2_base+5168(FP), BX
	MOVQ         in_3_base+5192(FP), BP
	MOVQ         in_4_base+5216(FP), SI
	MOVQ         in_5_base+5240(FP), DI
	MOVQ         in_6_base+5264(FP), R8
	MOVQ         in_7_base+5288(FP), R9
	MOVQ         $0x0000000f, R10
	MOVQ         R10, X10
	VPBROADCASTB X10, Y10
	XORQ         R10, R10

mulAvxTwo_8x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+2560(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+2592(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+2624(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+2656(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+2688(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+2720(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+2752(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+2784(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+2816(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+2848(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+2880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+2912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+2944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+2976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+3008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+3040(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+3072(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+3104(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+3136(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+3168(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+3200(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+3232(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+3264(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+3296(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+3328(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+3360(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+3392(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+3424(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+3456(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+3488(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+3520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+3552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+3584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+3616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+3648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+3680(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+3712(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+3744(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+3776(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+3808(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+3840(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+3872(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+3904(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+3936(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+3968(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+4000(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+4032(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+4064(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+4096(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+4128(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+4160(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+4192(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+4224(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+4256(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+4288(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+4320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+4352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+4384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+4416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+4448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 6 to 10 outputs
	VMOVDQU (R8)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_120+1920(FP), Y11
	VMOVDQU high_120+4480(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_122+1952(FP), Y11
	VMOVDQU high_122+4512(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_124+1984(FP), Y11
	VMOVDQU high_124+4544(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_126+2016(FP), Y11
	VMOVDQU high_126+4576(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_128+2048(FP), Y11
	VMOVDQU high_128+4608(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_130+2080(FP), Y11
	VMOVDQU high_130+4640(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_132+2112(FP), Y11
	VMOVDQU high_132+4672(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_134+2144(FP), Y11
	VMOVDQU high_134+4704(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_136+2176(FP), Y11
	VMOVDQU high_136+4736(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_138+2208(FP), Y11
	VMOVDQU high_138+4768(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 7 to 10 outputs
	VMOVDQU (R9)(R10*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_140+2240(FP), Y11
	VMOVDQU high_140+4800(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_142+2272(FP), Y11
	VMOVDQU high_142+4832(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_144+2304(FP), Y11
	VMOVDQU high_144+4864(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_146+2336(FP), Y11
	VMOVDQU high_146+4896(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_148+2368(FP), Y11
	VMOVDQU high_148+4928(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_150+2400(FP), Y11
	VMOVDQU high_150+4960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_152+2432(FP), Y11
	VMOVDQU high_152+4992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_154+2464(FP), Y11
	VMOVDQU high_154+5024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_156+2496(FP), Y11
	VMOVDQU high_156+5056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_158+2528(FP), Y11
	VMOVDQU high_158+5088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+5312(FP), R11
	VMOVDQU Y0, (R11)(R10*1)
	MOVQ    out_1_base+5336(FP), R11
	VMOVDQU Y1, (R11)(R10*1)
	MOVQ    out_2_base+5360(FP), R11
	VMOVDQU Y2, (R11)(R10*1)
	MOVQ    out_3_base+5384(FP), R11
	VMOVDQU Y3, (R11)(R10*1)
	MOVQ    out_4_base+5408(FP), R11
	VMOVDQU Y4, (R11)(R10*1)
	MOVQ    out_5_base+5432(FP), R11
	VMOVDQU Y5, (R11)(R10*1)
	MOVQ    out_6_base+5456(FP), R11
	VMOVDQU Y6, (R11)(R10*1)
	MOVQ    out_7_base+5480(FP), R11
	VMOVDQU Y7, (R11)(R10*1)
	MOVQ    out_8_base+5504(FP), R11
	VMOVDQU Y8, (R11)(R10*1)
	MOVQ    out_9_base+5528(FP), R11
	VMOVDQU Y9, (R11)(R10*1)

	// Prepare for next loop
	ADDQ $0x20, R10
	DECQ AX
	JNZ  mulAvxTwo_8x10_loop
	VZEROUPPER

mulAvxTwo_8x10_end:
	RET

// func mulAvxTwo_9x1(low [18][16]byte, high [18][16]byte, in [9][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x1(SB), $0-816
	// Loading no tables to registers
	// Full registers estimated 22 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+584(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x1_end
	MOVQ         out_0_base+792(FP), CX
	MOVQ         in_0_base+576(FP), DX
	MOVQ         in_1_base+600(FP), BX
	MOVQ         in_2_base+624(FP), BP
	MOVQ         in_3_base+648(FP), SI
	MOVQ         in_4_base+672(FP), DI
	MOVQ         in_5_base+696(FP), R8
	MOVQ         in_6_base+720(FP), R9
	MOVQ         in_7_base+744(FP), R10
	MOVQ         in_8_base+768(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X1
	VPBROADCASTB X1, Y1
	XORQ         R12, R12

mulAvxTwo_9x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_0+0(FP), Y2
	VMOVDQU high_0+288(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_2+32(FP), Y2
	VMOVDQU high_2+320(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_4+64(FP), Y2
	VMOVDQU high_4+352(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_6+96(FP), Y2
	VMOVDQU high_6+384(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_8+128(FP), Y2
	VMOVDQU high_8+416(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_10+160(FP), Y2
	VMOVDQU high_10+448(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 6 to 1 outputs
	VMOVDQU (R9)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_12+192(FP), Y2
	VMOVDQU high_12+480(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 7 to 1 outputs
	VMOVDQU (R10)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_14+224(FP), Y2
	VMOVDQU high_14+512(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 8 to 1 outputs
	VMOVDQU (R11)(R12*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_16+256(FP), Y2
	VMOVDQU high_16+544(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_9x1_loop
	VZEROUPPER

mulAvxTwo_9x1_end:
	RET

// func mulAvxTwo_9x2(low [36][16]byte, high [36][16]byte, in [9][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x2(SB), $0-1416
	// Loading no tables to registers
	// Full registers estimated 43 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1160(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x2_end
	MOVQ         out_0_base+1368(FP), CX
	MOVQ         out_1_base+1392(FP), DX
	MOVQ         in_0_base+1152(FP), BX
	MOVQ         in_1_base+1176(FP), BP
	MOVQ         in_2_base+1200(FP), SI
	MOVQ         in_3_base+1224(FP), DI
	MOVQ         in_4_base+1248(FP), R8
	MOVQ         in_5_base+1272(FP), R9
	MOVQ         in_6_base+1296(FP), R10
	MOVQ         in_7_base+1320(FP), R11
	MOVQ         in_8_base+1344(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X2
	VPBROADCASTB X2, Y2
	XORQ         R13, R13

mulAvxTwo_9x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+576(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+608(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+640(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+672(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+704(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+736(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+768(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+800(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+832(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+864(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (R9)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+896(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+928(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 6 to 2 outputs
	VMOVDQU (R10)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_24+384(FP), Y3
	VMOVDQU high_24+960(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_26+416(FP), Y3
	VMOVDQU high_26+992(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 7 to 2 outputs
	VMOVDQU (R11)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_28+448(FP), Y3
	VMOVDQU high_28+1024(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_30+480(FP), Y3
	VMOVDQU high_30+1056(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 8 to 2 outputs
	VMOVDQU (R12)(R13*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_32+512(FP), Y3
	VMOVDQU high_32+1088(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_34+544(FP), Y3
	VMOVDQU high_34+1120(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R13*1)
	VMOVDQU Y1, (DX)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_9x2_loop
	VZEROUPPER

mulAvxTwo_9x2_end:
	RET

// func mulAvxTwo_9x3(low [54][16]byte, high [54][16]byte, in [9][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x3(SB), $0-2016
	// Loading no tables to registers
	// Full registers estimated 62 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1736(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x3_end
	MOVQ         out_0_base+1944(FP), CX
	MOVQ         out_1_base+1968(FP), DX
	MOVQ         out_2_base+1992(FP), BX
	MOVQ         in_0_base+1728(FP), BP
	MOVQ         in_1_base+1752(FP), SI
	MOVQ         in_2_base+1776(FP), DI
	MOVQ         in_3_base+1800(FP), R8
	MOVQ         in_4_base+1824(FP), R9
	MOVQ         in_5_base+1848(FP), R10
	MOVQ         in_6_base+1872(FP), R11
	MOVQ         in_7_base+1896(FP), R12
	MOVQ         in_8_base+1920(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X3
	VPBROADCASTB X3, Y3
	XORQ         R14, R14

mulAvxTwo_9x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+864(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+896(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+928(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+960(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+992(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+1024(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+1056(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+1088(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+1120(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+1152(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+1184(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+1216(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (R9)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+1248(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+1280(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1312(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (R10)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1344(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1376(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1408(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 6 to 3 outputs
	VMOVDQU (R11)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_36+576(FP), Y4
	VMOVDQU high_36+1440(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_38+608(FP), Y4
	VMOVDQU high_38+1472(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_40+640(FP), Y4
	VMOVDQU high_40+1504(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 7 to 3 outputs
	VMOVDQU (R12)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_42+672(FP), Y4
	VMOVDQU high_42+1536(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_44+704(FP), Y4
	VMOVDQU high_44+1568(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_46+736(FP), Y4
	VMOVDQU high_46+1600(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 8 to 3 outputs
	VMOVDQU (R13)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_48+768(FP), Y4
	VMOVDQU high_48+1632(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_50+800(FP), Y4
	VMOVDQU high_50+1664(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_52+832(FP), Y4
	VMOVDQU high_52+1696(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)
	VMOVDQU Y2, (BX)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_9x3_loop
	VZEROUPPER

mulAvxTwo_9x3_end:
	RET

// func mulAvxTwo_9x4(low [72][16]byte, high [72][16]byte, in [9][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x4(SB), $0-2616
	// Loading no tables to registers
	// Full registers estimated 81 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2312(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x4_end
	MOVQ         out_0_base+2520(FP), CX
	MOVQ         out_1_base+2544(FP), DX
	MOVQ         out_2_base+2568(FP), BX
	MOVQ         out_3_base+2592(FP), BP
	MOVQ         in_0_base+2304(FP), SI
	MOVQ         in_1_base+2328(FP), DI
	MOVQ         in_2_base+2352(FP), R8
	MOVQ         in_3_base+2376(FP), R9
	MOVQ         in_4_base+2400(FP), R10
	MOVQ         in_5_base+2424(FP), R11
	MOVQ         in_6_base+2448(FP), R12
	MOVQ         in_7_base+2472(FP), R13
	MOVQ         in_8_base+2496(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X4
	VPBROADCASTB X4, Y4
	XORQ         R15, R15

mulAvxTwo_9x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (SI)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+1152(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+1184(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+1216(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+1248(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DI)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+1280(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+1312(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+1344(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+1376(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (R8)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1408(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1440(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1472(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1504(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (R9)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1536(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1568(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1600(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1632(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (R10)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1664(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1696(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1728(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+1760(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (R11)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+1792(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+1824(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+1856(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+1888(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 6 to 4 outputs
	VMOVDQU (R12)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_48+768(FP), Y5
	VMOVDQU high_48+1920(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_50+800(FP), Y5
	VMOVDQU high_50+1952(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_52+832(FP), Y5
	VMOVDQU high_52+1984(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_54+864(FP), Y5
	VMOVDQU high_54+2016(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 7 to 4 outputs
	VMOVDQU (R13)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_56+896(FP), Y5
	VMOVDQU high_56+2048(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_58+928(FP), Y5
	VMOVDQU high_58+2080(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_60+960(FP), Y5
	VMOVDQU high_60+2112(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_62+992(FP), Y5
	VMOVDQU high_62+2144(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 8 to 4 outputs
	VMOVDQU (R14)(R15*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_64+1024(FP), Y5
	VMOVDQU high_64+2176(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_66+1056(FP), Y5
	VMOVDQU high_66+2208(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_68+1088(FP), Y5
	VMOVDQU high_68+2240(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_70+1120(FP), Y5
	VMOVDQU high_70+2272(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)
	VMOVDQU Y3, (BP)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_9x4_loop
	VZEROUPPER

mulAvxTwo_9x4_end:
	RET

// func mulAvxTwo_9x5(low [90][16]byte, high [90][16]byte, in [9][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x5(SB), $0-3216
	// Loading no tables to registers
	// Full registers estimated 100 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2888(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x5_end
	MOVQ         in_0_base+2880(FP), CX
	MOVQ         in_1_base+2904(FP), DX
	MOVQ         in_2_base+2928(FP), BX
	MOVQ         in_3_base+2952(FP), BP
	MOVQ         in_4_base+2976(FP), SI
	MOVQ         in_5_base+3000(FP), DI
	MOVQ         in_6_base+3024(FP), R8
	MOVQ         in_7_base+3048(FP), R9
	MOVQ         in_8_base+3072(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X5
	VPBROADCASTB X5, Y5
	XORQ         R11, R11

mulAvxTwo_9x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (CX)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+1440(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+1472(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1504(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+1536(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+1568(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (DX)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+1600(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+1632(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1664(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+1696(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+1728(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (BX)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+1760(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+1792(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1824(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+1856(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+1888(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (BP)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+1920(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+1952(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+1984(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+2016(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+2048(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (SI)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+2080(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+2112(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+2144(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+2176(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+2208(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (DI)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+2240(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+2272(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+2304(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+2336(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+2368(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 6 to 5 outputs
	VMOVDQU (R8)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_60+960(FP), Y6
	VMOVDQU high_60+2400(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_62+992(FP), Y6
	VMOVDQU high_62+2432(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_64+1024(FP), Y6
	VMOVDQU high_64+2464(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_66+1056(FP), Y6
	VMOVDQU high_66+2496(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_68+1088(FP), Y6
	VMOVDQU high_68+2528(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 7 to 5 outputs
	VMOVDQU (R9)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_70+1120(FP), Y6
	VMOVDQU high_70+2560(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_72+1152(FP), Y6
	VMOVDQU high_72+2592(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_74+1184(FP), Y6
	VMOVDQU high_74+2624(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_76+1216(FP), Y6
	VMOVDQU high_76+2656(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_78+1248(FP), Y6
	VMOVDQU high_78+2688(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 8 to 5 outputs
	VMOVDQU (R10)(R11*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_80+1280(FP), Y6
	VMOVDQU high_80+2720(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_82+1312(FP), Y6
	VMOVDQU high_82+2752(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_84+1344(FP), Y6
	VMOVDQU high_84+2784(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_86+1376(FP), Y6
	VMOVDQU high_86+2816(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_88+1408(FP), Y6
	VMOVDQU high_88+2848(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	MOVQ    out_0_base+3096(FP), R12
	VMOVDQU Y0, (R12)(R11*1)
	MOVQ    out_1_base+3120(FP), R12
	VMOVDQU Y1, (R12)(R11*1)
	MOVQ    out_2_base+3144(FP), R12
	VMOVDQU Y2, (R12)(R11*1)
	MOVQ    out_3_base+3168(FP), R12
	VMOVDQU Y3, (R12)(R11*1)
	MOVQ    out_4_base+3192(FP), R12
	VMOVDQU Y4, (R12)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_9x5_loop
	VZEROUPPER

mulAvxTwo_9x5_end:
	RET

// func mulAvxTwo_9x6(low [108][16]byte, high [108][16]byte, in [9][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x6(SB), $0-3816
	// Loading no tables to registers
	// Full registers estimated 119 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3464(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x6_end
	MOVQ         in_0_base+3456(FP), CX
	MOVQ         in_1_base+3480(FP), DX
	MOVQ         in_2_base+3504(FP), BX
	MOVQ         in_3_base+3528(FP), BP
	MOVQ         in_4_base+3552(FP), SI
	MOVQ         in_5_base+3576(FP), DI
	MOVQ         in_6_base+3600(FP), R8
	MOVQ         in_7_base+3624(FP), R9
	MOVQ         in_8_base+3648(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X6
	VPBROADCASTB X6, Y6
	XORQ         R11, R11

mulAvxTwo_9x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (CX)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+1728(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+1760(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+1792(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+1824(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+1856(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+1888(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (DX)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+1920(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+1952(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+1984(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+2016(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+2048(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+2080(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (BX)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+2112(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+2144(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+2176(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+2208(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+2240(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+2272(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (BP)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+2304(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+2336(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+2368(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+2400(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+2432(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+2464(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (SI)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+2496(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+2528(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+2560(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+2592(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+2624(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+2656(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (DI)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+2688(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+2720(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+2752(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+2784(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+2816(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+2848(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 6 to 6 outputs
	VMOVDQU (R8)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_72+1152(FP), Y7
	VMOVDQU high_72+2880(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_74+1184(FP), Y7
	VMOVDQU high_74+2912(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_76+1216(FP), Y7
	VMOVDQU high_76+2944(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_78+1248(FP), Y7
	VMOVDQU high_78+2976(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_80+1280(FP), Y7
	VMOVDQU high_80+3008(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_82+1312(FP), Y7
	VMOVDQU high_82+3040(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 7 to 6 outputs
	VMOVDQU (R9)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_84+1344(FP), Y7
	VMOVDQU high_84+3072(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_86+1376(FP), Y7
	VMOVDQU high_86+3104(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_88+1408(FP), Y7
	VMOVDQU high_88+3136(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_90+1440(FP), Y7
	VMOVDQU high_90+3168(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_92+1472(FP), Y7
	VMOVDQU high_92+3200(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_94+1504(FP), Y7
	VMOVDQU high_94+3232(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 8 to 6 outputs
	VMOVDQU (R10)(R11*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_96+1536(FP), Y7
	VMOVDQU high_96+3264(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_98+1568(FP), Y7
	VMOVDQU high_98+3296(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_100+1600(FP), Y7
	VMOVDQU high_100+3328(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_102+1632(FP), Y7
	VMOVDQU high_102+3360(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_104+1664(FP), Y7
	VMOVDQU high_104+3392(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_106+1696(FP), Y7
	VMOVDQU high_106+3424(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	MOVQ    out_0_base+3672(FP), R12
	VMOVDQU Y0, (R12)(R11*1)
	MOVQ    out_1_base+3696(FP), R12
	VMOVDQU Y1, (R12)(R11*1)
	MOVQ    out_2_base+3720(FP), R12
	VMOVDQU Y2, (R12)(R11*1)
	MOVQ    out_3_base+3744(FP), R12
	VMOVDQU Y3, (R12)(R11*1)
	MOVQ    out_4_base+3768(FP), R12
	VMOVDQU Y4, (R12)(R11*1)
	MOVQ    out_5_base+3792(FP), R12
	VMOVDQU Y5, (R12)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_9x6_loop
	VZEROUPPER

mulAvxTwo_9x6_end:
	RET

// func mulAvxTwo_9x7(low [126][16]byte, high [126][16]byte, in [9][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x7(SB), $0-4416
	// Loading no tables to registers
	// Full registers estimated 138 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4040(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x7_end
	MOVQ         in_0_base+4032(FP), CX
	MOVQ         in_1_base+4056(FP), DX
	MOVQ         in_2_base+4080(FP), BX
	MOVQ         in_3_base+4104(FP), BP
	MOVQ         in_4_base+4128(FP), SI
	MOVQ         in_5_base+4152(FP), DI
	MOVQ         in_6_base+4176(FP), R8
	MOVQ         in_7_base+4200(FP), R9
	MOVQ         in_8_base+4224(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X7
	VPBROADCASTB X7, Y7
	XORQ         R11, R11

mulAvxTwo_9x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (CX)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+2016(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+2048(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+2080(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+2112(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+2144(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+2176(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+2208(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (DX)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+2240(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+2272(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+2304(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+2336(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+2368(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+2400(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+2432(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (BX)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+2464(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+2496(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+2528(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+2560(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+2592(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+2624(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+2656(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (BP)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+2688(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+2720(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+2752(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+2784(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+2816(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+2848(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+2880(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (SI)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+2912(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+2944(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+2976(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+3008(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+3040(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+3072(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+3104(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (DI)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+3136(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+3168(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+3200(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+3232(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+3264(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+3296(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+3328(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 6 to 7 outputs
	VMOVDQU (R8)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_84+1344(FP), Y8
	VMOVDQU high_84+3360(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_86+1376(FP), Y8
	VMOVDQU high_86+3392(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_88+1408(FP), Y8
	VMOVDQU high_88+3424(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_90+1440(FP), Y8
	VMOVDQU high_90+3456(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_92+1472(FP), Y8
	VMOVDQU high_92+3488(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_94+1504(FP), Y8
	VMOVDQU high_94+3520(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_96+1536(FP), Y8
	VMOVDQU high_96+3552(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 7 to 7 outputs
	VMOVDQU (R9)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_98+1568(FP), Y8
	VMOVDQU high_98+3584(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_100+1600(FP), Y8
	VMOVDQU high_100+3616(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_102+1632(FP), Y8
	VMOVDQU high_102+3648(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_104+1664(FP), Y8
	VMOVDQU high_104+3680(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_106+1696(FP), Y8
	VMOVDQU high_106+3712(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_108+1728(FP), Y8
	VMOVDQU high_108+3744(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_110+1760(FP), Y8
	VMOVDQU high_110+3776(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 8 to 7 outputs
	VMOVDQU (R10)(R11*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_112+1792(FP), Y8
	VMOVDQU high_112+3808(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_114+1824(FP), Y8
	VMOVDQU high_114+3840(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_116+1856(FP), Y8
	VMOVDQU high_116+3872(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_118+1888(FP), Y8
	VMOVDQU high_118+3904(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_120+1920(FP), Y8
	VMOVDQU high_120+3936(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_122+1952(FP), Y8
	VMOVDQU high_122+3968(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_124+1984(FP), Y8
	VMOVDQU high_124+4000(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	MOVQ    out_0_base+4248(FP), R12
	VMOVDQU Y0, (R12)(R11*1)
	MOVQ    out_1_base+4272(FP), R12
	VMOVDQU Y1, (R12)(R11*1)
	MOVQ    out_2_base+4296(FP), R12
	VMOVDQU Y2, (R12)(R11*1)
	MOVQ    out_3_base+4320(FP), R12
	VMOVDQU Y3, (R12)(R11*1)
	MOVQ    out_4_base+4344(FP), R12
	VMOVDQU Y4, (R12)(R11*1)
	MOVQ    out_5_base+4368(FP), R12
	VMOVDQU Y5, (R12)(R11*1)
	MOVQ    out_6_base+4392(FP), R12
	VMOVDQU Y6, (R12)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_9x7_loop
	VZEROUPPER

mulAvxTwo_9x7_end:
	RET

// func mulAvxTwo_9x8(low [144][16]byte, high [144][16]byte, in [9][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x8(SB), $0-5016
	// Loading no tables to registers
	// Full registers estimated 157 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4616(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x8_end
	MOVQ         in_0_base+4608(FP), CX
	MOVQ         in_1_base+4632(FP), DX
	MOVQ         in_2_base+4656(FP), BX
	MOVQ         in_3_base+4680(FP), BP
	MOVQ         in_4_base+4704(FP), SI
	MOVQ         in_5_base+4728(FP), DI
	MOVQ         in_6_base+4752(FP), R8
	MOVQ         in_7_base+4776(FP), R9
	MOVQ         in_8_base+4800(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X8
	VPBROADCASTB X8, Y8
	XORQ         R11, R11

mulAvxTwo_9x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+2304(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+2336(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+2368(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+2400(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+2432(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+2464(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+2496(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+2528(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+2560(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+2592(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+2624(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+2656(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+2688(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+2720(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+2752(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+2784(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+2816(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+2848(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+2880(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+2912(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+2944(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+2976(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+3008(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+3040(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+3072(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+3104(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+3136(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+3168(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+3200(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+3232(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+3264(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+3296(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+3328(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+3360(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+3392(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+3424(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+3456(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+3488(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+3520(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+3552(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+3584(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+3616(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+3648(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+3680(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+3712(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+3744(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+3776(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+3808(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 6 to 8 outputs
	VMOVDQU (R8)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_96+1536(FP), Y9
	VMOVDQU high_96+3840(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_98+1568(FP), Y9
	VMOVDQU high_98+3872(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_100+1600(FP), Y9
	VMOVDQU high_100+3904(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_102+1632(FP), Y9
	VMOVDQU high_102+3936(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_104+1664(FP), Y9
	VMOVDQU high_104+3968(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_106+1696(FP), Y9
	VMOVDQU high_106+4000(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_108+1728(FP), Y9
	VMOVDQU high_108+4032(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_110+1760(FP), Y9
	VMOVDQU high_110+4064(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 7 to 8 outputs
	VMOVDQU (R9)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_112+1792(FP), Y9
	VMOVDQU high_112+4096(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_114+1824(FP), Y9
	VMOVDQU high_114+4128(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_116+1856(FP), Y9
	VMOVDQU high_116+4160(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_118+1888(FP), Y9
	VMOVDQU high_118+4192(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_120+1920(FP), Y9
	VMOVDQU high_120+4224(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_122+1952(FP), Y9
	VMOVDQU high_122+4256(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_124+1984(FP), Y9
	VMOVDQU high_124+4288(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_126+2016(FP), Y9
	VMOVDQU high_126+4320(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 8 to 8 outputs
	VMOVDQU (R10)(R11*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_128+2048(FP), Y9
	VMOVDQU high_128+4352(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_130+2080(FP), Y9
	VMOVDQU high_130+4384(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_132+2112(FP), Y9
	VMOVDQU high_132+4416(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_134+2144(FP), Y9
	VMOVDQU high_134+4448(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_136+2176(FP), Y9
	VMOVDQU high_136+4480(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_138+2208(FP), Y9
	VMOVDQU high_138+4512(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_140+2240(FP), Y9
	VMOVDQU high_140+4544(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_142+2272(FP), Y9
	VMOVDQU high_142+4576(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+4824(FP), R12
	VMOVDQU Y0, (R12)(R11*1)
	MOVQ    out_1_base+4848(FP), R12
	VMOVDQU Y1, (R12)(R11*1)
	MOVQ    out_2_base+4872(FP), R12
	VMOVDQU Y2, (R12)(R11*1)
	MOVQ    out_3_base+4896(FP), R12
	VMOVDQU Y3, (R12)(R11*1)
	MOVQ    out_4_base+4920(FP), R12
	VMOVDQU Y4, (R12)(R11*1)
	MOVQ    out_5_base+4944(FP), R12
	VMOVDQU Y5, (R12)(R11*1)
	MOVQ    out_6_base+4968(FP), R12
	VMOVDQU Y6, (R12)(R11*1)
	MOVQ    out_7_base+4992(FP), R12
	VMOVDQU Y7, (R12)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_9x8_loop
	VZEROUPPER

mulAvxTwo_9x8_end:
	RET

// func mulAvxTwo_9x9(low [162][16]byte, high [162][16]byte, in [9][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x9(SB), $0-5616
	// Loading no tables to registers
	// Full registers estimated 176 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5192(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x9_end
	MOVQ         in_0_base+5184(FP), CX
	MOVQ         in_1_base+5208(FP), DX
	MOVQ         in_2_base+5232(FP), BX
	MOVQ         in_3_base+5256(FP), BP
	MOVQ         in_4_base+5280(FP), SI
	MOVQ         in_5_base+5304(FP), DI
	MOVQ         in_6_base+5328(FP), R8
	MOVQ         in_7_base+5352(FP), R9
	MOVQ         in_8_base+5376(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X9
	VPBROADCASTB X9, Y9
	XORQ         R11, R11

mulAvxTwo_9x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+2592(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+2624(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+2656(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+2688(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+2720(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+2752(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+2784(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+2816(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+2848(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+2880(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+2912(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+2944(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+2976(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+3008(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+3040(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+3072(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+3104(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+3136(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+3168(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+3200(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+3232(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+3264(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+3296(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+3328(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+3360(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+3392(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+3424(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+3456(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+3488(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+3520(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+3552(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+3584(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+3616(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+3648(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+3680(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+3712(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+3744(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+3776(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+3808(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+3840(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+3872(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+3904(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+3936(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+3968(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+4000(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+4032(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+4064(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+4096(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+4128(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+4160(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+4192(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+4224(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+4256(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+4288(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 6 to 9 outputs
	VMOVDQU (R8)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_108+1728(FP), Y10
	VMOVDQU high_108+4320(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_110+1760(FP), Y10
	VMOVDQU high_110+4352(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_112+1792(FP), Y10
	VMOVDQU high_112+4384(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_114+1824(FP), Y10
	VMOVDQU high_114+4416(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_116+1856(FP), Y10
	VMOVDQU high_116+4448(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_118+1888(FP), Y10
	VMOVDQU high_118+4480(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_120+1920(FP), Y10
	VMOVDQU high_120+4512(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_122+1952(FP), Y10
	VMOVDQU high_122+4544(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_124+1984(FP), Y10
	VMOVDQU high_124+4576(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 7 to 9 outputs
	VMOVDQU (R9)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_126+2016(FP), Y10
	VMOVDQU high_126+4608(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_128+2048(FP), Y10
	VMOVDQU high_128+4640(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_130+2080(FP), Y10
	VMOVDQU high_130+4672(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_132+2112(FP), Y10
	VMOVDQU high_132+4704(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_134+2144(FP), Y10
	VMOVDQU high_134+4736(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_136+2176(FP), Y10
	VMOVDQU high_136+4768(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_138+2208(FP), Y10
	VMOVDQU high_138+4800(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_140+2240(FP), Y10
	VMOVDQU high_140+4832(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_142+2272(FP), Y10
	VMOVDQU high_142+4864(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 8 to 9 outputs
	VMOVDQU (R10)(R11*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_144+2304(FP), Y10
	VMOVDQU high_144+4896(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_146+2336(FP), Y10
	VMOVDQU high_146+4928(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_148+2368(FP), Y10
	VMOVDQU high_148+4960(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_150+2400(FP), Y10
	VMOVDQU high_150+4992(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_152+2432(FP), Y10
	VMOVDQU high_152+5024(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_154+2464(FP), Y10
	VMOVDQU high_154+5056(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_156+2496(FP), Y10
	VMOVDQU high_156+5088(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_158+2528(FP), Y10
	VMOVDQU high_158+5120(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_160+2560(FP), Y10
	VMOVDQU high_160+5152(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+5400(FP), R12
	VMOVDQU Y0, (R12)(R11*1)
	MOVQ    out_1_base+5424(FP), R12
	VMOVDQU Y1, (R12)(R11*1)
	MOVQ    out_2_base+5448(FP), R12
	VMOVDQU Y2, (R12)(R11*1)
	MOVQ    out_3_base+5472(FP), R12
	VMOVDQU Y3, (R12)(R11*1)
	MOVQ    out_4_base+5496(FP), R12
	VMOVDQU Y4, (R12)(R11*1)
	MOVQ    out_5_base+5520(FP), R12
	VMOVDQU Y5, (R12)(R11*1)
	MOVQ    out_6_base+5544(FP), R12
	VMOVDQU Y6, (R12)(R11*1)
	MOVQ    out_7_base+5568(FP), R12
	VMOVDQU Y7, (R12)(R11*1)
	MOVQ    out_8_base+5592(FP), R12
	VMOVDQU Y8, (R12)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_9x9_loop
	VZEROUPPER

mulAvxTwo_9x9_end:
	RET

// func mulAvxTwo_9x10(low [180][16]byte, high [180][16]byte, in [9][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_9x10(SB), $0-6216
	// Loading no tables to registers
	// Full registers estimated 195 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5768(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_9x10_end
	MOVQ         in_0_base+5760(FP), CX
	MOVQ         in_1_base+5784(FP), DX
	MOVQ         in_2_base+5808(FP), BX
	MOVQ         in_3_base+5832(FP), BP
	MOVQ         in_4_base+5856(FP), SI
	MOVQ         in_5_base+5880(FP), DI
	MOVQ         in_6_base+5904(FP), R8
	MOVQ         in_7_base+5928(FP), R9
	MOVQ         in_8_base+5952(FP), R10
	MOVQ         $0x0000000f, R11
	MOVQ         R11, X10
	VPBROADCASTB X10, Y10
	XORQ         R11, R11

mulAvxTwo_9x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+2880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+2912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+2944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+2976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+3008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+3040(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+3072(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+3104(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+3136(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+3168(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+3200(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+3232(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+3264(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+3296(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+3328(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+3360(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+3392(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+3424(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+3456(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+3488(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+3520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+3552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+3584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+3616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+3648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+3680(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+3712(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+3744(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+3776(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+3808(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+3840(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+3872(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+3904(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+3936(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+3968(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+4000(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+4032(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+4064(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+4096(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+4128(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+4160(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+4192(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+4224(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+4256(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+4288(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+4320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+4352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+4384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+4416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+4448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+4480(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+4512(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+4544(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+4576(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+4608(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+4640(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+4672(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+4704(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+4736(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+4768(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 6 to 10 outputs
	VMOVDQU (R8)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_120+1920(FP), Y11
	VMOVDQU high_120+4800(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_122+1952(FP), Y11
	VMOVDQU high_122+4832(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_124+1984(FP), Y11
	VMOVDQU high_124+4864(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_126+2016(FP), Y11
	VMOVDQU high_126+4896(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_128+2048(FP), Y11
	VMOVDQU high_128+4928(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_130+2080(FP), Y11
	VMOVDQU high_130+4960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_132+2112(FP), Y11
	VMOVDQU high_132+4992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_134+2144(FP), Y11
	VMOVDQU high_134+5024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_136+2176(FP), Y11
	VMOVDQU high_136+5056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_138+2208(FP), Y11
	VMOVDQU high_138+5088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 7 to 10 outputs
	VMOVDQU (R9)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_140+2240(FP), Y11
	VMOVDQU high_140+5120(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_142+2272(FP), Y11
	VMOVDQU high_142+5152(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_144+2304(FP), Y11
	VMOVDQU high_144+5184(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_146+2336(FP), Y11
	VMOVDQU high_146+5216(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_148+2368(FP), Y11
	VMOVDQU high_148+5248(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_150+2400(FP), Y11
	VMOVDQU high_150+5280(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_152+2432(FP), Y11
	VMOVDQU high_152+5312(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_154+2464(FP), Y11
	VMOVDQU high_154+5344(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_156+2496(FP), Y11
	VMOVDQU high_156+5376(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_158+2528(FP), Y11
	VMOVDQU high_158+5408(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 8 to 10 outputs
	VMOVDQU (R10)(R11*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_160+2560(FP), Y11
	VMOVDQU high_160+5440(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_162+2592(FP), Y11
	VMOVDQU high_162+5472(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_164+2624(FP), Y11
	VMOVDQU high_164+5504(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_166+2656(FP), Y11
	VMOVDQU high_166+5536(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_168+2688(FP), Y11
	VMOVDQU high_168+5568(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_170+2720(FP), Y11
	VMOVDQU high_170+5600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_172+2752(FP), Y11
	VMOVDQU high_172+5632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_174+2784(FP), Y11
	VMOVDQU high_174+5664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_176+2816(FP), Y11
	VMOVDQU high_176+5696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_178+2848(FP), Y11
	VMOVDQU high_178+5728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+5976(FP), R12
	VMOVDQU Y0, (R12)(R11*1)
	MOVQ    out_1_base+6000(FP), R12
	VMOVDQU Y1, (R12)(R11*1)
	MOVQ    out_2_base+6024(FP), R12
	VMOVDQU Y2, (R12)(R11*1)
	MOVQ    out_3_base+6048(FP), R12
	VMOVDQU Y3, (R12)(R11*1)
	MOVQ    out_4_base+6072(FP), R12
	VMOVDQU Y4, (R12)(R11*1)
	MOVQ    out_5_base+6096(FP), R12
	VMOVDQU Y5, (R12)(R11*1)
	MOVQ    out_6_base+6120(FP), R12
	VMOVDQU Y6, (R12)(R11*1)
	MOVQ    out_7_base+6144(FP), R12
	VMOVDQU Y7, (R12)(R11*1)
	MOVQ    out_8_base+6168(FP), R12
	VMOVDQU Y8, (R12)(R11*1)
	MOVQ    out_9_base+6192(FP), R12
	VMOVDQU Y9, (R12)(R11*1)

	// Prepare for next loop
	ADDQ $0x20, R11
	DECQ AX
	JNZ  mulAvxTwo_9x10_loop
	VZEROUPPER

mulAvxTwo_9x10_end:
	RET

// func mulAvxTwo_10x1(low [20][16]byte, high [20][16]byte, in [10][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x1(SB), $0-904
	// Loading no tables to registers
	// Full registers estimated 24 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+648(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x1_end
	MOVQ         out_0_base+880(FP), CX
	MOVQ         in_0_base+640(FP), DX
	MOVQ         in_1_base+664(FP), BX
	MOVQ         in_2_base+688(FP), BP
	MOVQ         in_3_base+712(FP), SI
	MOVQ         in_4_base+736(FP), DI
	MOVQ         in_5_base+760(FP), R8
	MOVQ         in_6_base+784(FP), R9
	MOVQ         in_7_base+808(FP), R10
	MOVQ         in_8_base+832(FP), R11
	MOVQ         in_9_base+856(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X1
	VPBROADCASTB X1, Y1
	XORQ         R13, R13

mulAvxTwo_10x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_0+0(FP), Y2
	VMOVDQU high_0+320(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_2+32(FP), Y2
	VMOVDQU high_2+352(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_4+64(FP), Y2
	VMOVDQU high_4+384(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_6+96(FP), Y2
	VMOVDQU high_6+416(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_8+128(FP), Y2
	VMOVDQU high_8+448(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_10+160(FP), Y2
	VMOVDQU high_10+480(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 6 to 1 outputs
	VMOVDQU (R9)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_12+192(FP), Y2
	VMOVDQU high_12+512(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 7 to 1 outputs
	VMOVDQU (R10)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_14+224(FP), Y2
	VMOVDQU high_14+544(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 8 to 1 outputs
	VMOVDQU (R11)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_16+256(FP), Y2
	VMOVDQU high_16+576(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 9 to 1 outputs
	VMOVDQU (R12)(R13*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_18+288(FP), Y2
	VMOVDQU high_18+608(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_10x1_loop
	VZEROUPPER

mulAvxTwo_10x1_end:
	RET

// func mulAvxTwo_10x2(low [40][16]byte, high [40][16]byte, in [10][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x2(SB), $0-1568
	// Loading no tables to registers
	// Full registers estimated 47 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1288(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x2_end
	MOVQ         out_0_base+1520(FP), CX
	MOVQ         out_1_base+1544(FP), DX
	MOVQ         in_0_base+1280(FP), BX
	MOVQ         in_1_base+1304(FP), BP
	MOVQ         in_2_base+1328(FP), SI
	MOVQ         in_3_base+1352(FP), DI
	MOVQ         in_4_base+1376(FP), R8
	MOVQ         in_5_base+1400(FP), R9
	MOVQ         in_6_base+1424(FP), R10
	MOVQ         in_7_base+1448(FP), R11
	MOVQ         in_8_base+1472(FP), R12
	MOVQ         in_9_base+1496(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X2
	VPBROADCASTB X2, Y2
	XORQ         R14, R14

mulAvxTwo_10x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+640(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+672(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+704(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+736(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+768(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+800(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+832(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+864(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+896(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+928(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (R9)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+960(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+992(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 6 to 2 outputs
	VMOVDQU (R10)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_24+384(FP), Y3
	VMOVDQU high_24+1024(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_26+416(FP), Y3
	VMOVDQU high_26+1056(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 7 to 2 outputs
	VMOVDQU (R11)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_28+448(FP), Y3
	VMOVDQU high_28+1088(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_30+480(FP), Y3
	VMOVDQU high_30+1120(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 8 to 2 outputs
	VMOVDQU (R12)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_32+512(FP), Y3
	VMOVDQU high_32+1152(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_34+544(FP), Y3
	VMOVDQU high_34+1184(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 9 to 2 outputs
	VMOVDQU (R13)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_36+576(FP), Y3
	VMOVDQU high_36+1216(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_38+608(FP), Y3
	VMOVDQU high_38+1248(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R14*1)
	VMOVDQU Y1, (DX)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_10x2_loop
	VZEROUPPER

mulAvxTwo_10x2_end:
	RET

// func mulAvxTwo_10x3(low [60][16]byte, high [60][16]byte, in [10][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x3(SB), $0-2232
	// Loading no tables to registers
	// Full registers estimated 68 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1928(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x3_end
	MOVQ         out_0_base+2160(FP), CX
	MOVQ         out_1_base+2184(FP), DX
	MOVQ         out_2_base+2208(FP), BX
	MOVQ         in_0_base+1920(FP), BP
	MOVQ         in_1_base+1944(FP), SI
	MOVQ         in_2_base+1968(FP), DI
	MOVQ         in_3_base+1992(FP), R8
	MOVQ         in_4_base+2016(FP), R9
	MOVQ         in_5_base+2040(FP), R10
	MOVQ         in_6_base+2064(FP), R11
	MOVQ         in_7_base+2088(FP), R12
	MOVQ         in_8_base+2112(FP), R13
	MOVQ         in_9_base+2136(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X3
	VPBROADCASTB X3, Y3
	XORQ         R15, R15

mulAvxTwo_10x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+960(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+992(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+1024(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (SI)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+1056(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+1088(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+1120(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (DI)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+1152(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+1184(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+1216(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (R8)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+1248(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+1280(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+1312(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (R9)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+1344(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+1376(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1408(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (R10)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1440(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1472(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1504(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 6 to 3 outputs
	VMOVDQU (R11)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_36+576(FP), Y4
	VMOVDQU high_36+1536(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_38+608(FP), Y4
	VMOVDQU high_38+1568(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_40+640(FP), Y4
	VMOVDQU high_40+1600(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 7 to 3 outputs
	VMOVDQU (R12)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_42+672(FP), Y4
	VMOVDQU high_42+1632(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_44+704(FP), Y4
	VMOVDQU high_44+1664(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_46+736(FP), Y4
	VMOVDQU high_46+1696(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 8 to 3 outputs
	VMOVDQU (R13)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_48+768(FP), Y4
	VMOVDQU high_48+1728(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_50+800(FP), Y4
	VMOVDQU high_50+1760(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_52+832(FP), Y4
	VMOVDQU high_52+1792(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 9 to 3 outputs
	VMOVDQU (R14)(R15*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_54+864(FP), Y4
	VMOVDQU high_54+1824(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_56+896(FP), Y4
	VMOVDQU high_56+1856(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_58+928(FP), Y4
	VMOVDQU high_58+1888(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)
	VMOVDQU Y2, (BX)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_10x3_loop
	VZEROUPPER

mulAvxTwo_10x3_end:
	RET

// func mulAvxTwo_10x4(low [80][16]byte, high [80][16]byte, in [10][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x4(SB), $0-2896
	// Loading no tables to registers
	// Full registers estimated 89 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2568(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x4_end
	MOVQ         in_0_base+2560(FP), CX
	MOVQ         in_1_base+2584(FP), DX
	MOVQ         in_2_base+2608(FP), BX
	MOVQ         in_3_base+2632(FP), BP
	MOVQ         in_4_base+2656(FP), SI
	MOVQ         in_5_base+2680(FP), DI
	MOVQ         in_6_base+2704(FP), R8
	MOVQ         in_7_base+2728(FP), R9
	MOVQ         in_8_base+2752(FP), R10
	MOVQ         in_9_base+2776(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X4
	VPBROADCASTB X4, Y4
	XORQ         R12, R12

mulAvxTwo_10x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (CX)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+1280(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+1312(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+1344(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+1376(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DX)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+1408(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+1440(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+1472(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+1504(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (BX)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1536(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1568(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1600(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1632(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (BP)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1664(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1696(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1728(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1760(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (SI)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1792(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1824(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1856(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+1888(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (DI)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+1920(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+1952(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+1984(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+2016(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 6 to 4 outputs
	VMOVDQU (R8)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_48+768(FP), Y5
	VMOVDQU high_48+2048(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_50+800(FP), Y5
	VMOVDQU high_50+2080(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_52+832(FP), Y5
	VMOVDQU high_52+2112(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_54+864(FP), Y5
	VMOVDQU high_54+2144(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 7 to 4 outputs
	VMOVDQU (R9)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_56+896(FP), Y5
	VMOVDQU high_56+2176(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_58+928(FP), Y5
	VMOVDQU high_58+2208(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_60+960(FP), Y5
	VMOVDQU high_60+2240(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_62+992(FP), Y5
	VMOVDQU high_62+2272(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 8 to 4 outputs
	VMOVDQU (R10)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_64+1024(FP), Y5
	VMOVDQU high_64+2304(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_66+1056(FP), Y5
	VMOVDQU high_66+2336(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_68+1088(FP), Y5
	VMOVDQU high_68+2368(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_70+1120(FP), Y5
	VMOVDQU high_70+2400(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 9 to 4 outputs
	VMOVDQU (R11)(R12*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_72+1152(FP), Y5
	VMOVDQU high_72+2432(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_74+1184(FP), Y5
	VMOVDQU high_74+2464(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_76+1216(FP), Y5
	VMOVDQU high_76+2496(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_78+1248(FP), Y5
	VMOVDQU high_78+2528(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	MOVQ    out_0_base+2800(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+2824(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+2848(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+2872(FP), R13
	VMOVDQU Y3, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x4_loop
	VZEROUPPER

mulAvxTwo_10x4_end:
	RET

// func mulAvxTwo_10x5(low [100][16]byte, high [100][16]byte, in [10][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x5(SB), $0-3560
	// Loading no tables to registers
	// Full registers estimated 110 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3208(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x5_end
	MOVQ         in_0_base+3200(FP), CX
	MOVQ         in_1_base+3224(FP), DX
	MOVQ         in_2_base+3248(FP), BX
	MOVQ         in_3_base+3272(FP), BP
	MOVQ         in_4_base+3296(FP), SI
	MOVQ         in_5_base+3320(FP), DI
	MOVQ         in_6_base+3344(FP), R8
	MOVQ         in_7_base+3368(FP), R9
	MOVQ         in_8_base+3392(FP), R10
	MOVQ         in_9_base+3416(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X5
	VPBROADCASTB X5, Y5
	XORQ         R12, R12

mulAvxTwo_10x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (CX)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+1600(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+1632(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1664(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+1696(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+1728(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (DX)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+1760(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+1792(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1824(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+1856(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+1888(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (BX)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+1920(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+1952(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+1984(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+2016(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+2048(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (BP)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+2080(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+2112(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+2144(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+2176(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+2208(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (SI)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+2240(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+2272(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+2304(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+2336(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+2368(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (DI)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+2400(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+2432(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+2464(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+2496(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+2528(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 6 to 5 outputs
	VMOVDQU (R8)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_60+960(FP), Y6
	VMOVDQU high_60+2560(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_62+992(FP), Y6
	VMOVDQU high_62+2592(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_64+1024(FP), Y6
	VMOVDQU high_64+2624(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_66+1056(FP), Y6
	VMOVDQU high_66+2656(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_68+1088(FP), Y6
	VMOVDQU high_68+2688(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 7 to 5 outputs
	VMOVDQU (R9)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_70+1120(FP), Y6
	VMOVDQU high_70+2720(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_72+1152(FP), Y6
	VMOVDQU high_72+2752(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_74+1184(FP), Y6
	VMOVDQU high_74+2784(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_76+1216(FP), Y6
	VMOVDQU high_76+2816(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_78+1248(FP), Y6
	VMOVDQU high_78+2848(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 8 to 5 outputs
	VMOVDQU (R10)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_80+1280(FP), Y6
	VMOVDQU high_80+2880(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_82+1312(FP), Y6
	VMOVDQU high_82+2912(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_84+1344(FP), Y6
	VMOVDQU high_84+2944(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_86+1376(FP), Y6
	VMOVDQU high_86+2976(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_88+1408(FP), Y6
	VMOVDQU high_88+3008(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 9 to 5 outputs
	VMOVDQU (R11)(R12*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_90+1440(FP), Y6
	VMOVDQU high_90+3040(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_92+1472(FP), Y6
	VMOVDQU high_92+3072(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_94+1504(FP), Y6
	VMOVDQU high_94+3104(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_96+1536(FP), Y6
	VMOVDQU high_96+3136(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_98+1568(FP), Y6
	VMOVDQU high_98+3168(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	MOVQ    out_0_base+3440(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+3464(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+3488(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+3512(FP), R13
	VMOVDQU Y3, (R13)(R12*1)
	MOVQ    out_4_base+3536(FP), R13
	VMOVDQU Y4, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x5_loop
	VZEROUPPER

mulAvxTwo_10x5_end:
	RET

// func mulAvxTwo_10x6(low [120][16]byte, high [120][16]byte, in [10][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x6(SB), $0-4224
	// Loading no tables to registers
	// Full registers estimated 131 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3848(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x6_end
	MOVQ         in_0_base+3840(FP), CX
	MOVQ         in_1_base+3864(FP), DX
	MOVQ         in_2_base+3888(FP), BX
	MOVQ         in_3_base+3912(FP), BP
	MOVQ         in_4_base+3936(FP), SI
	MOVQ         in_5_base+3960(FP), DI
	MOVQ         in_6_base+3984(FP), R8
	MOVQ         in_7_base+4008(FP), R9
	MOVQ         in_8_base+4032(FP), R10
	MOVQ         in_9_base+4056(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X6
	VPBROADCASTB X6, Y6
	XORQ         R12, R12

mulAvxTwo_10x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (CX)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+1920(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+1952(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+1984(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+2016(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+2048(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+2080(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (DX)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+2112(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+2144(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+2176(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+2208(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+2240(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+2272(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (BX)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+2304(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+2336(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+2368(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+2400(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+2432(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+2464(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (BP)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+2496(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+2528(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+2560(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+2592(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+2624(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+2656(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (SI)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+2688(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+2720(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+2752(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+2784(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+2816(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+2848(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (DI)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+2880(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+2912(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+2944(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+2976(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+3008(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+3040(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 6 to 6 outputs
	VMOVDQU (R8)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_72+1152(FP), Y7
	VMOVDQU high_72+3072(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_74+1184(FP), Y7
	VMOVDQU high_74+3104(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_76+1216(FP), Y7
	VMOVDQU high_76+3136(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_78+1248(FP), Y7
	VMOVDQU high_78+3168(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_80+1280(FP), Y7
	VMOVDQU high_80+3200(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_82+1312(FP), Y7
	VMOVDQU high_82+3232(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 7 to 6 outputs
	VMOVDQU (R9)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_84+1344(FP), Y7
	VMOVDQU high_84+3264(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_86+1376(FP), Y7
	VMOVDQU high_86+3296(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_88+1408(FP), Y7
	VMOVDQU high_88+3328(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_90+1440(FP), Y7
	VMOVDQU high_90+3360(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_92+1472(FP), Y7
	VMOVDQU high_92+3392(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_94+1504(FP), Y7
	VMOVDQU high_94+3424(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 8 to 6 outputs
	VMOVDQU (R10)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_96+1536(FP), Y7
	VMOVDQU high_96+3456(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_98+1568(FP), Y7
	VMOVDQU high_98+3488(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_100+1600(FP), Y7
	VMOVDQU high_100+3520(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_102+1632(FP), Y7
	VMOVDQU high_102+3552(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_104+1664(FP), Y7
	VMOVDQU high_104+3584(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_106+1696(FP), Y7
	VMOVDQU high_106+3616(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 9 to 6 outputs
	VMOVDQU (R11)(R12*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_108+1728(FP), Y7
	VMOVDQU high_108+3648(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_110+1760(FP), Y7
	VMOVDQU high_110+3680(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_112+1792(FP), Y7
	VMOVDQU high_112+3712(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_114+1824(FP), Y7
	VMOVDQU high_114+3744(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_116+1856(FP), Y7
	VMOVDQU high_116+3776(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_118+1888(FP), Y7
	VMOVDQU high_118+3808(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	MOVQ    out_0_base+4080(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+4104(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+4128(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+4152(FP), R13
	VMOVDQU Y3, (R13)(R12*1)
	MOVQ    out_4_base+4176(FP), R13
	VMOVDQU Y4, (R13)(R12*1)
	MOVQ    out_5_base+4200(FP), R13
	VMOVDQU Y5, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x6_loop
	VZEROUPPER

mulAvxTwo_10x6_end:
	RET

// func mulAvxTwo_10x7(low [140][16]byte, high [140][16]byte, in [10][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x7(SB), $0-4888
	// Loading no tables to registers
	// Full registers estimated 152 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4488(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x7_end
	MOVQ         in_0_base+4480(FP), CX
	MOVQ         in_1_base+4504(FP), DX
	MOVQ         in_2_base+4528(FP), BX
	MOVQ         in_3_base+4552(FP), BP
	MOVQ         in_4_base+4576(FP), SI
	MOVQ         in_5_base+4600(FP), DI
	MOVQ         in_6_base+4624(FP), R8
	MOVQ         in_7_base+4648(FP), R9
	MOVQ         in_8_base+4672(FP), R10
	MOVQ         in_9_base+4696(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X7
	VPBROADCASTB X7, Y7
	XORQ         R12, R12

mulAvxTwo_10x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (CX)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+2240(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+2272(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+2304(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+2336(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+2368(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+2400(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+2432(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (DX)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+2464(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+2496(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+2528(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+2560(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+2592(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+2624(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+2656(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (BX)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+2688(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+2720(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+2752(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+2784(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+2816(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+2848(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+2880(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (BP)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+2912(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+2944(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+2976(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+3008(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+3040(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+3072(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+3104(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (SI)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+3136(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+3168(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+3200(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+3232(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+3264(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+3296(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+3328(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (DI)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+3360(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+3392(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+3424(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+3456(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+3488(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+3520(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+3552(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 6 to 7 outputs
	VMOVDQU (R8)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_84+1344(FP), Y8
	VMOVDQU high_84+3584(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_86+1376(FP), Y8
	VMOVDQU high_86+3616(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_88+1408(FP), Y8
	VMOVDQU high_88+3648(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_90+1440(FP), Y8
	VMOVDQU high_90+3680(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_92+1472(FP), Y8
	VMOVDQU high_92+3712(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_94+1504(FP), Y8
	VMOVDQU high_94+3744(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_96+1536(FP), Y8
	VMOVDQU high_96+3776(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 7 to 7 outputs
	VMOVDQU (R9)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_98+1568(FP), Y8
	VMOVDQU high_98+3808(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_100+1600(FP), Y8
	VMOVDQU high_100+3840(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_102+1632(FP), Y8
	VMOVDQU high_102+3872(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_104+1664(FP), Y8
	VMOVDQU high_104+3904(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_106+1696(FP), Y8
	VMOVDQU high_106+3936(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_108+1728(FP), Y8
	VMOVDQU high_108+3968(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_110+1760(FP), Y8
	VMOVDQU high_110+4000(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 8 to 7 outputs
	VMOVDQU (R10)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_112+1792(FP), Y8
	VMOVDQU high_112+4032(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_114+1824(FP), Y8
	VMOVDQU high_114+4064(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_116+1856(FP), Y8
	VMOVDQU high_116+4096(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_118+1888(FP), Y8
	VMOVDQU high_118+4128(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_120+1920(FP), Y8
	VMOVDQU high_120+4160(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_122+1952(FP), Y8
	VMOVDQU high_122+4192(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_124+1984(FP), Y8
	VMOVDQU high_124+4224(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 9 to 7 outputs
	VMOVDQU (R11)(R12*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_126+2016(FP), Y8
	VMOVDQU high_126+4256(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_128+2048(FP), Y8
	VMOVDQU high_128+4288(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_130+2080(FP), Y8
	VMOVDQU high_130+4320(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_132+2112(FP), Y8
	VMOVDQU high_132+4352(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_134+2144(FP), Y8
	VMOVDQU high_134+4384(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_136+2176(FP), Y8
	VMOVDQU high_136+4416(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_138+2208(FP), Y8
	VMOVDQU high_138+4448(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	MOVQ    out_0_base+4720(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+4744(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+4768(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+4792(FP), R13
	VMOVDQU Y3, (R13)(R12*1)
	MOVQ    out_4_base+4816(FP), R13
	VMOVDQU Y4, (R13)(R12*1)
	MOVQ    out_5_base+4840(FP), R13
	VMOVDQU Y5, (R13)(R12*1)
	MOVQ    out_6_base+4864(FP), R13
	VMOVDQU Y6, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x7_loop
	VZEROUPPER

mulAvxTwo_10x7_end:
	RET

// func mulAvxTwo_10x8(low [160][16]byte, high [160][16]byte, in [10][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x8(SB), $0-5552
	// Loading no tables to registers
	// Full registers estimated 173 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5128(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x8_end
	MOVQ         in_0_base+5120(FP), CX
	MOVQ         in_1_base+5144(FP), DX
	MOVQ         in_2_base+5168(FP), BX
	MOVQ         in_3_base+5192(FP), BP
	MOVQ         in_4_base+5216(FP), SI
	MOVQ         in_5_base+5240(FP), DI
	MOVQ         in_6_base+5264(FP), R8
	MOVQ         in_7_base+5288(FP), R9
	MOVQ         in_8_base+5312(FP), R10
	MOVQ         in_9_base+5336(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X8
	VPBROADCASTB X8, Y8
	XORQ         R12, R12

mulAvxTwo_10x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+2560(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+2592(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+2624(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+2656(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+2688(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+2720(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+2752(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+2784(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+2816(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+2848(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+2880(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+2912(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+2944(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+2976(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+3008(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+3040(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+3072(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+3104(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+3136(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+3168(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+3200(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+3232(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+3264(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+3296(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+3328(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+3360(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+3392(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+3424(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+3456(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+3488(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+3520(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+3552(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+3584(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+3616(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+3648(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+3680(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+3712(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+3744(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+3776(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+3808(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+3840(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+3872(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+3904(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+3936(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+3968(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+4000(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+4032(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+4064(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 6 to 8 outputs
	VMOVDQU (R8)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_96+1536(FP), Y9
	VMOVDQU high_96+4096(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_98+1568(FP), Y9
	VMOVDQU high_98+4128(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_100+1600(FP), Y9
	VMOVDQU high_100+4160(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_102+1632(FP), Y9
	VMOVDQU high_102+4192(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_104+1664(FP), Y9
	VMOVDQU high_104+4224(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_106+1696(FP), Y9
	VMOVDQU high_106+4256(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_108+1728(FP), Y9
	VMOVDQU high_108+4288(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_110+1760(FP), Y9
	VMOVDQU high_110+4320(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 7 to 8 outputs
	VMOVDQU (R9)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_112+1792(FP), Y9
	VMOVDQU high_112+4352(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_114+1824(FP), Y9
	VMOVDQU high_114+4384(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_116+1856(FP), Y9
	VMOVDQU high_116+4416(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_118+1888(FP), Y9
	VMOVDQU high_118+4448(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_120+1920(FP), Y9
	VMOVDQU high_120+4480(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_122+1952(FP), Y9
	VMOVDQU high_122+4512(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_124+1984(FP), Y9
	VMOVDQU high_124+4544(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_126+2016(FP), Y9
	VMOVDQU high_126+4576(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 8 to 8 outputs
	VMOVDQU (R10)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_128+2048(FP), Y9
	VMOVDQU high_128+4608(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_130+2080(FP), Y9
	VMOVDQU high_130+4640(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_132+2112(FP), Y9
	VMOVDQU high_132+4672(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_134+2144(FP), Y9
	VMOVDQU high_134+4704(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_136+2176(FP), Y9
	VMOVDQU high_136+4736(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_138+2208(FP), Y9
	VMOVDQU high_138+4768(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_140+2240(FP), Y9
	VMOVDQU high_140+4800(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_142+2272(FP), Y9
	VMOVDQU high_142+4832(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 9 to 8 outputs
	VMOVDQU (R11)(R12*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_144+2304(FP), Y9
	VMOVDQU high_144+4864(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_146+2336(FP), Y9
	VMOVDQU high_146+4896(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_148+2368(FP), Y9
	VMOVDQU high_148+4928(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_150+2400(FP), Y9
	VMOVDQU high_150+4960(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_152+2432(FP), Y9
	VMOVDQU high_152+4992(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_154+2464(FP), Y9
	VMOVDQU high_154+5024(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_156+2496(FP), Y9
	VMOVDQU high_156+5056(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_158+2528(FP), Y9
	VMOVDQU high_158+5088(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+5360(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+5384(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+5408(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+5432(FP), R13
	VMOVDQU Y3, (R13)(R12*1)
	MOVQ    out_4_base+5456(FP), R13
	VMOVDQU Y4, (R13)(R12*1)
	MOVQ    out_5_base+5480(FP), R13
	VMOVDQU Y5, (R13)(R12*1)
	MOVQ    out_6_base+5504(FP), R13
	VMOVDQU Y6, (R13)(R12*1)
	MOVQ    out_7_base+5528(FP), R13
	VMOVDQU Y7, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x8_loop
	VZEROUPPER

mulAvxTwo_10x8_end:
	RET

// func mulAvxTwo_10x9(low [180][16]byte, high [180][16]byte, in [10][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x9(SB), $0-6216
	// Loading no tables to registers
	// Full registers estimated 194 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5768(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x9_end
	MOVQ         in_0_base+5760(FP), CX
	MOVQ         in_1_base+5784(FP), DX
	MOVQ         in_2_base+5808(FP), BX
	MOVQ         in_3_base+5832(FP), BP
	MOVQ         in_4_base+5856(FP), SI
	MOVQ         in_5_base+5880(FP), DI
	MOVQ         in_6_base+5904(FP), R8
	MOVQ         in_7_base+5928(FP), R9
	MOVQ         in_8_base+5952(FP), R10
	MOVQ         in_9_base+5976(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X9
	VPBROADCASTB X9, Y9
	XORQ         R12, R12

mulAvxTwo_10x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+2880(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+2912(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+2944(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+2976(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+3008(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+3040(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+3072(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+3104(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+3136(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+3168(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+3200(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+3232(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+3264(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+3296(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+3328(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+3360(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+3392(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+3424(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+3456(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+3488(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+3520(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+3552(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+3584(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+3616(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+3648(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+3680(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+3712(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+3744(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+3776(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+3808(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+3840(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+3872(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+3904(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+3936(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+3968(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+4000(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+4032(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+4064(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+4096(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+4128(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+4160(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+4192(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+4224(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+4256(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+4288(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+4320(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+4352(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+4384(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+4416(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+4448(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+4480(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+4512(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+4544(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+4576(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 6 to 9 outputs
	VMOVDQU (R8)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_108+1728(FP), Y10
	VMOVDQU high_108+4608(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_110+1760(FP), Y10
	VMOVDQU high_110+4640(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_112+1792(FP), Y10
	VMOVDQU high_112+4672(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_114+1824(FP), Y10
	VMOVDQU high_114+4704(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_116+1856(FP), Y10
	VMOVDQU high_116+4736(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_118+1888(FP), Y10
	VMOVDQU high_118+4768(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_120+1920(FP), Y10
	VMOVDQU high_120+4800(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_122+1952(FP), Y10
	VMOVDQU high_122+4832(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_124+1984(FP), Y10
	VMOVDQU high_124+4864(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 7 to 9 outputs
	VMOVDQU (R9)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_126+2016(FP), Y10
	VMOVDQU high_126+4896(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_128+2048(FP), Y10
	VMOVDQU high_128+4928(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_130+2080(FP), Y10
	VMOVDQU high_130+4960(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_132+2112(FP), Y10
	VMOVDQU high_132+4992(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_134+2144(FP), Y10
	VMOVDQU high_134+5024(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_136+2176(FP), Y10
	VMOVDQU high_136+5056(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_138+2208(FP), Y10
	VMOVDQU high_138+5088(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_140+2240(FP), Y10
	VMOVDQU high_140+5120(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_142+2272(FP), Y10
	VMOVDQU high_142+5152(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 8 to 9 outputs
	VMOVDQU (R10)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_144+2304(FP), Y10
	VMOVDQU high_144+5184(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_146+2336(FP), Y10
	VMOVDQU high_146+5216(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_148+2368(FP), Y10
	VMOVDQU high_148+5248(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_150+2400(FP), Y10
	VMOVDQU high_150+5280(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_152+2432(FP), Y10
	VMOVDQU high_152+5312(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_154+2464(FP), Y10
	VMOVDQU high_154+5344(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_156+2496(FP), Y10
	VMOVDQU high_156+5376(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_158+2528(FP), Y10
	VMOVDQU high_158+5408(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_160+2560(FP), Y10
	VMOVDQU high_160+5440(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 9 to 9 outputs
	VMOVDQU (R11)(R12*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_162+2592(FP), Y10
	VMOVDQU high_162+5472(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_164+2624(FP), Y10
	VMOVDQU high_164+5504(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_166+2656(FP), Y10
	VMOVDQU high_166+5536(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_168+2688(FP), Y10
	VMOVDQU high_168+5568(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_170+2720(FP), Y10
	VMOVDQU high_170+5600(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_172+2752(FP), Y10
	VMOVDQU high_172+5632(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_174+2784(FP), Y10
	VMOVDQU high_174+5664(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_176+2816(FP), Y10
	VMOVDQU high_176+5696(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_178+2848(FP), Y10
	VMOVDQU high_178+5728(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+6000(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+6024(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+6048(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+6072(FP), R13
	VMOVDQU Y3, (R13)(R12*1)
	MOVQ    out_4_base+6096(FP), R13
	VMOVDQU Y4, (R13)(R12*1)
	MOVQ    out_5_base+6120(FP), R13
	VMOVDQU Y5, (R13)(R12*1)
	MOVQ    out_6_base+6144(FP), R13
	VMOVDQU Y6, (R13)(R12*1)
	MOVQ    out_7_base+6168(FP), R13
	VMOVDQU Y7, (R13)(R12*1)
	MOVQ    out_8_base+6192(FP), R13
	VMOVDQU Y8, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x9_loop
	VZEROUPPER

mulAvxTwo_10x9_end:
	RET

// func mulAvxTwo_10x10(low [200][16]byte, high [200][16]byte, in [10][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_10x10(SB), $0-6880
	// Loading no tables to registers
	// Full registers estimated 215 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+6408(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_10x10_end
	MOVQ         in_0_base+6400(FP), CX
	MOVQ         in_1_base+6424(FP), DX
	MOVQ         in_2_base+6448(FP), BX
	MOVQ         in_3_base+6472(FP), BP
	MOVQ         in_4_base+6496(FP), SI
	MOVQ         in_5_base+6520(FP), DI
	MOVQ         in_6_base+6544(FP), R8
	MOVQ         in_7_base+6568(FP), R9
	MOVQ         in_8_base+6592(FP), R10
	MOVQ         in_9_base+6616(FP), R11
	MOVQ         $0x0000000f, R12
	MOVQ         R12, X10
	VPBROADCASTB X10, Y10
	XORQ         R12, R12

mulAvxTwo_10x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+3200(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+3232(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+3264(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+3296(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+3328(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+3360(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+3392(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+3424(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+3456(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+3488(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+3520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+3552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+3584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+3616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+3648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+3680(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+3712(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+3744(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+3776(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+3808(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+3840(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+3872(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+3904(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+3936(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+3968(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+4000(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+4032(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+4064(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+4096(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+4128(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+4160(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+4192(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+4224(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+4256(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+4288(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+4320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+4352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+4384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+4416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+4448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+4480(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+4512(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+4544(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+4576(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+4608(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+4640(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+4672(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+4704(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+4736(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+4768(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+4800(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+4832(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+4864(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+4896(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+4928(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+4960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+4992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+5024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+5056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+5088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 6 to 10 outputs
	VMOVDQU (R8)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_120+1920(FP), Y11
	VMOVDQU high_120+5120(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_122+1952(FP), Y11
	VMOVDQU high_122+5152(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_124+1984(FP), Y11
	VMOVDQU high_124+5184(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_126+2016(FP), Y11
	VMOVDQU high_126+5216(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_128+2048(FP), Y11
	VMOVDQU high_128+5248(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_130+2080(FP), Y11
	VMOVDQU high_130+5280(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_132+2112(FP), Y11
	VMOVDQU high_132+5312(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_134+2144(FP), Y11
	VMOVDQU high_134+5344(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_136+2176(FP), Y11
	VMOVDQU high_136+5376(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_138+2208(FP), Y11
	VMOVDQU high_138+5408(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 7 to 10 outputs
	VMOVDQU (R9)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_140+2240(FP), Y11
	VMOVDQU high_140+5440(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_142+2272(FP), Y11
	VMOVDQU high_142+5472(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_144+2304(FP), Y11
	VMOVDQU high_144+5504(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_146+2336(FP), Y11
	VMOVDQU high_146+5536(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_148+2368(FP), Y11
	VMOVDQU high_148+5568(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_150+2400(FP), Y11
	VMOVDQU high_150+5600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_152+2432(FP), Y11
	VMOVDQU high_152+5632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_154+2464(FP), Y11
	VMOVDQU high_154+5664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_156+2496(FP), Y11
	VMOVDQU high_156+5696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_158+2528(FP), Y11
	VMOVDQU high_158+5728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 8 to 10 outputs
	VMOVDQU (R10)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_160+2560(FP), Y11
	VMOVDQU high_160+5760(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_162+2592(FP), Y11
	VMOVDQU high_162+5792(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_164+2624(FP), Y11
	VMOVDQU high_164+5824(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_166+2656(FP), Y11
	VMOVDQU high_166+5856(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_168+2688(FP), Y11
	VMOVDQU high_168+5888(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_170+2720(FP), Y11
	VMOVDQU high_170+5920(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_172+2752(FP), Y11
	VMOVDQU high_172+5952(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_174+2784(FP), Y11
	VMOVDQU high_174+5984(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_176+2816(FP), Y11
	VMOVDQU high_176+6016(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_178+2848(FP), Y11
	VMOVDQU high_178+6048(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 9 to 10 outputs
	VMOVDQU (R11)(R12*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_180+2880(FP), Y11
	VMOVDQU high_180+6080(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_182+2912(FP), Y11
	VMOVDQU high_182+6112(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_184+2944(FP), Y11
	VMOVDQU high_184+6144(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_186+2976(FP), Y11
	VMOVDQU high_186+6176(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_188+3008(FP), Y11
	VMOVDQU high_188+6208(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_190+3040(FP), Y11
	VMOVDQU high_190+6240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_192+3072(FP), Y11
	VMOVDQU high_192+6272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_194+3104(FP), Y11
	VMOVDQU high_194+6304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_196+3136(FP), Y11
	VMOVDQU high_196+6336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_198+3168(FP), Y11
	VMOVDQU high_198+6368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+6640(FP), R13
	VMOVDQU Y0, (R13)(R12*1)
	MOVQ    out_1_base+6664(FP), R13
	VMOVDQU Y1, (R13)(R12*1)
	MOVQ    out_2_base+6688(FP), R13
	VMOVDQU Y2, (R13)(R12*1)
	MOVQ    out_3_base+6712(FP), R13
	VMOVDQU Y3, (R13)(R12*1)
	MOVQ    out_4_base+6736(FP), R13
	VMOVDQU Y4, (R13)(R12*1)
	MOVQ    out_5_base+6760(FP), R13
	VMOVDQU Y5, (R13)(R12*1)
	MOVQ    out_6_base+6784(FP), R13
	VMOVDQU Y6, (R13)(R12*1)
	MOVQ    out_7_base+6808(FP), R13
	VMOVDQU Y7, (R13)(R12*1)
	MOVQ    out_8_base+6832(FP), R13
	VMOVDQU Y8, (R13)(R12*1)
	MOVQ    out_9_base+6856(FP), R13
	VMOVDQU Y9, (R13)(R12*1)

	// Prepare for next loop
	ADDQ $0x20, R12
	DECQ AX
	JNZ  mulAvxTwo_10x10_loop
	VZEROUPPER

mulAvxTwo_10x10_end:
	RET

// func mulAvxTwo_11x1(low [22][16]byte, high [22][16]byte, in [11][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x1(SB), $0-992
	// Loading no tables to registers
	// Full registers estimated 26 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+712(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x1_end
	MOVQ         out_0_base+968(FP), CX
	MOVQ         in_0_base+704(FP), DX
	MOVQ         in_1_base+728(FP), BX
	MOVQ         in_2_base+752(FP), BP
	MOVQ         in_3_base+776(FP), SI
	MOVQ         in_4_base+800(FP), DI
	MOVQ         in_5_base+824(FP), R8
	MOVQ         in_6_base+848(FP), R9
	MOVQ         in_7_base+872(FP), R10
	MOVQ         in_8_base+896(FP), R11
	MOVQ         in_9_base+920(FP), R12
	MOVQ         in_10_base+944(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X1
	VPBROADCASTB X1, Y1
	XORQ         R14, R14

mulAvxTwo_11x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_0+0(FP), Y2
	VMOVDQU high_0+352(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_2+32(FP), Y2
	VMOVDQU high_2+384(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_4+64(FP), Y2
	VMOVDQU high_4+416(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_6+96(FP), Y2
	VMOVDQU high_6+448(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_8+128(FP), Y2
	VMOVDQU high_8+480(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_10+160(FP), Y2
	VMOVDQU high_10+512(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 6 to 1 outputs
	VMOVDQU (R9)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_12+192(FP), Y2
	VMOVDQU high_12+544(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 7 to 1 outputs
	VMOVDQU (R10)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_14+224(FP), Y2
	VMOVDQU high_14+576(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 8 to 1 outputs
	VMOVDQU (R11)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_16+256(FP), Y2
	VMOVDQU high_16+608(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 9 to 1 outputs
	VMOVDQU (R12)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_18+288(FP), Y2
	VMOVDQU high_18+640(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 10 to 1 outputs
	VMOVDQU (R13)(R14*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_20+320(FP), Y2
	VMOVDQU high_20+672(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_11x1_loop
	VZEROUPPER

mulAvxTwo_11x1_end:
	RET

// func mulAvxTwo_11x2(low [44][16]byte, high [44][16]byte, in [11][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x2(SB), $0-1720
	// Loading no tables to registers
	// Full registers estimated 51 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1416(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x2_end
	MOVQ         out_0_base+1672(FP), CX
	MOVQ         out_1_base+1696(FP), DX
	MOVQ         in_0_base+1408(FP), BX
	MOVQ         in_1_base+1432(FP), BP
	MOVQ         in_2_base+1456(FP), SI
	MOVQ         in_3_base+1480(FP), DI
	MOVQ         in_4_base+1504(FP), R8
	MOVQ         in_5_base+1528(FP), R9
	MOVQ         in_6_base+1552(FP), R10
	MOVQ         in_7_base+1576(FP), R11
	MOVQ         in_8_base+1600(FP), R12
	MOVQ         in_9_base+1624(FP), R13
	MOVQ         in_10_base+1648(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X2
	VPBROADCASTB X2, Y2
	XORQ         R15, R15

mulAvxTwo_11x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+704(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+736(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+768(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+800(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (SI)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+832(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+864(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (DI)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+896(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+928(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (R8)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+960(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+992(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (R9)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+1024(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+1056(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 6 to 2 outputs
	VMOVDQU (R10)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_24+384(FP), Y3
	VMOVDQU high_24+1088(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_26+416(FP), Y3
	VMOVDQU high_26+1120(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 7 to 2 outputs
	VMOVDQU (R11)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_28+448(FP), Y3
	VMOVDQU high_28+1152(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_30+480(FP), Y3
	VMOVDQU high_30+1184(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 8 to 2 outputs
	VMOVDQU (R12)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_32+512(FP), Y3
	VMOVDQU high_32+1216(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_34+544(FP), Y3
	VMOVDQU high_34+1248(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 9 to 2 outputs
	VMOVDQU (R13)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_36+576(FP), Y3
	VMOVDQU high_36+1280(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_38+608(FP), Y3
	VMOVDQU high_38+1312(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 10 to 2 outputs
	VMOVDQU (R14)(R15*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_40+640(FP), Y3
	VMOVDQU high_40+1344(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_42+672(FP), Y3
	VMOVDQU high_42+1376(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(R15*1)
	VMOVDQU Y1, (DX)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_11x2_loop
	VZEROUPPER

mulAvxTwo_11x2_end:
	RET

// func mulAvxTwo_11x3(low [66][16]byte, high [66][16]byte, in [11][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x3(SB), $0-2448
	// Loading no tables to registers
	// Full registers estimated 74 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2120(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x3_end
	MOVQ         in_0_base+2112(FP), CX
	MOVQ         in_1_base+2136(FP), DX
	MOVQ         in_2_base+2160(FP), BX
	MOVQ         in_3_base+2184(FP), BP
	MOVQ         in_4_base+2208(FP), SI
	MOVQ         in_5_base+2232(FP), DI
	MOVQ         in_6_base+2256(FP), R8
	MOVQ         in_7_base+2280(FP), R9
	MOVQ         in_8_base+2304(FP), R10
	MOVQ         in_9_base+2328(FP), R11
	MOVQ         in_10_base+2352(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X3
	VPBROADCASTB X3, Y3
	XORQ         R13, R13

mulAvxTwo_11x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (CX)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+1056(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+1088(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+1120(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (DX)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+1152(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+1184(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+1216(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (BX)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+1248(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+1280(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+1312(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (BP)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+1344(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+1376(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+1408(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (SI)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+1440(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+1472(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1504(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (DI)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1536(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1568(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1600(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 6 to 3 outputs
	VMOVDQU (R8)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_36+576(FP), Y4
	VMOVDQU high_36+1632(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_38+608(FP), Y4
	VMOVDQU high_38+1664(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_40+640(FP), Y4
	VMOVDQU high_40+1696(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 7 to 3 outputs
	VMOVDQU (R9)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_42+672(FP), Y4
	VMOVDQU high_42+1728(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_44+704(FP), Y4
	VMOVDQU high_44+1760(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_46+736(FP), Y4
	VMOVDQU high_46+1792(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 8 to 3 outputs
	VMOVDQU (R10)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_48+768(FP), Y4
	VMOVDQU high_48+1824(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_50+800(FP), Y4
	VMOVDQU high_50+1856(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_52+832(FP), Y4
	VMOVDQU high_52+1888(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 9 to 3 outputs
	VMOVDQU (R11)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_54+864(FP), Y4
	VMOVDQU high_54+1920(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_56+896(FP), Y4
	VMOVDQU high_56+1952(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_58+928(FP), Y4
	VMOVDQU high_58+1984(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 10 to 3 outputs
	VMOVDQU (R12)(R13*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_60+960(FP), Y4
	VMOVDQU high_60+2016(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_62+992(FP), Y4
	VMOVDQU high_62+2048(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_64+1024(FP), Y4
	VMOVDQU high_64+2080(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	MOVQ    out_0_base+2376(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+2400(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+2424(FP), R14
	VMOVDQU Y2, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x3_loop
	VZEROUPPER

mulAvxTwo_11x3_end:
	RET

// func mulAvxTwo_11x4(low [88][16]byte, high [88][16]byte, in [11][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x4(SB), $0-3176
	// Loading no tables to registers
	// Full registers estimated 97 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2824(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x4_end
	MOVQ         in_0_base+2816(FP), CX
	MOVQ         in_1_base+2840(FP), DX
	MOVQ         in_2_base+2864(FP), BX
	MOVQ         in_3_base+2888(FP), BP
	MOVQ         in_4_base+2912(FP), SI
	MOVQ         in_5_base+2936(FP), DI
	MOVQ         in_6_base+2960(FP), R8
	MOVQ         in_7_base+2984(FP), R9
	MOVQ         in_8_base+3008(FP), R10
	MOVQ         in_9_base+3032(FP), R11
	MOVQ         in_10_base+3056(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X4
	VPBROADCASTB X4, Y4
	XORQ         R13, R13

mulAvxTwo_11x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (CX)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+1408(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+1440(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+1472(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+1504(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DX)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+1536(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+1568(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+1600(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+1632(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (BX)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1664(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1696(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1728(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1760(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (BP)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1792(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1824(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1856(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+1888(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (SI)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+1920(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+1952(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+1984(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+2016(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (DI)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+2048(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+2080(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+2112(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+2144(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 6 to 4 outputs
	VMOVDQU (R8)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_48+768(FP), Y5
	VMOVDQU high_48+2176(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_50+800(FP), Y5
	VMOVDQU high_50+2208(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_52+832(FP), Y5
	VMOVDQU high_52+2240(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_54+864(FP), Y5
	VMOVDQU high_54+2272(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 7 to 4 outputs
	VMOVDQU (R9)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_56+896(FP), Y5
	VMOVDQU high_56+2304(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_58+928(FP), Y5
	VMOVDQU high_58+2336(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_60+960(FP), Y5
	VMOVDQU high_60+2368(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_62+992(FP), Y5
	VMOVDQU high_62+2400(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 8 to 4 outputs
	VMOVDQU (R10)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_64+1024(FP), Y5
	VMOVDQU high_64+2432(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_66+1056(FP), Y5
	VMOVDQU high_66+2464(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_68+1088(FP), Y5
	VMOVDQU high_68+2496(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_70+1120(FP), Y5
	VMOVDQU high_70+2528(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 9 to 4 outputs
	VMOVDQU (R11)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_72+1152(FP), Y5
	VMOVDQU high_72+2560(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_74+1184(FP), Y5
	VMOVDQU high_74+2592(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_76+1216(FP), Y5
	VMOVDQU high_76+2624(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_78+1248(FP), Y5
	VMOVDQU high_78+2656(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 10 to 4 outputs
	VMOVDQU (R12)(R13*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_80+1280(FP), Y5
	VMOVDQU high_80+2688(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_82+1312(FP), Y5
	VMOVDQU high_82+2720(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_84+1344(FP), Y5
	VMOVDQU high_84+2752(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_86+1376(FP), Y5
	VMOVDQU high_86+2784(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	MOVQ    out_0_base+3080(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+3104(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+3128(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+3152(FP), R14
	VMOVDQU Y3, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x4_loop
	VZEROUPPER

mulAvxTwo_11x4_end:
	RET

// func mulAvxTwo_11x5(low [110][16]byte, high [110][16]byte, in [11][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x5(SB), $0-3904
	// Loading no tables to registers
	// Full registers estimated 120 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3528(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x5_end
	MOVQ         in_0_base+3520(FP), CX
	MOVQ         in_1_base+3544(FP), DX
	MOVQ         in_2_base+3568(FP), BX
	MOVQ         in_3_base+3592(FP), BP
	MOVQ         in_4_base+3616(FP), SI
	MOVQ         in_5_base+3640(FP), DI
	MOVQ         in_6_base+3664(FP), R8
	MOVQ         in_7_base+3688(FP), R9
	MOVQ         in_8_base+3712(FP), R10
	MOVQ         in_9_base+3736(FP), R11
	MOVQ         in_10_base+3760(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X5
	VPBROADCASTB X5, Y5
	XORQ         R13, R13

mulAvxTwo_11x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (CX)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+1760(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+1792(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1824(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+1856(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+1888(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (DX)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+1920(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+1952(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+1984(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+2016(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+2048(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (BX)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+2080(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+2112(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+2144(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+2176(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+2208(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (BP)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+2240(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+2272(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+2304(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+2336(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+2368(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (SI)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+2400(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+2432(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+2464(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+2496(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+2528(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (DI)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+2560(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+2592(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+2624(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+2656(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+2688(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 6 to 5 outputs
	VMOVDQU (R8)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_60+960(FP), Y6
	VMOVDQU high_60+2720(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_62+992(FP), Y6
	VMOVDQU high_62+2752(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_64+1024(FP), Y6
	VMOVDQU high_64+2784(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_66+1056(FP), Y6
	VMOVDQU high_66+2816(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_68+1088(FP), Y6
	VMOVDQU high_68+2848(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 7 to 5 outputs
	VMOVDQU (R9)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_70+1120(FP), Y6
	VMOVDQU high_70+2880(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_72+1152(FP), Y6
	VMOVDQU high_72+2912(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_74+1184(FP), Y6
	VMOVDQU high_74+2944(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_76+1216(FP), Y6
	VMOVDQU high_76+2976(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_78+1248(FP), Y6
	VMOVDQU high_78+3008(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 8 to 5 outputs
	VMOVDQU (R10)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_80+1280(FP), Y6
	VMOVDQU high_80+3040(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_82+1312(FP), Y6
	VMOVDQU high_82+3072(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_84+1344(FP), Y6
	VMOVDQU high_84+3104(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_86+1376(FP), Y6
	VMOVDQU high_86+3136(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_88+1408(FP), Y6
	VMOVDQU high_88+3168(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 9 to 5 outputs
	VMOVDQU (R11)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_90+1440(FP), Y6
	VMOVDQU high_90+3200(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_92+1472(FP), Y6
	VMOVDQU high_92+3232(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_94+1504(FP), Y6
	VMOVDQU high_94+3264(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_96+1536(FP), Y6
	VMOVDQU high_96+3296(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_98+1568(FP), Y6
	VMOVDQU high_98+3328(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 10 to 5 outputs
	VMOVDQU (R12)(R13*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_100+1600(FP), Y6
	VMOVDQU high_100+3360(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_102+1632(FP), Y6
	VMOVDQU high_102+3392(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_104+1664(FP), Y6
	VMOVDQU high_104+3424(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_106+1696(FP), Y6
	VMOVDQU high_106+3456(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_108+1728(FP), Y6
	VMOVDQU high_108+3488(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	MOVQ    out_0_base+3784(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+3808(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+3832(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+3856(FP), R14
	VMOVDQU Y3, (R14)(R13*1)
	MOVQ    out_4_base+3880(FP), R14
	VMOVDQU Y4, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x5_loop
	VZEROUPPER

mulAvxTwo_11x5_end:
	RET

// func mulAvxTwo_11x6(low [132][16]byte, high [132][16]byte, in [11][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x6(SB), $0-4632
	// Loading no tables to registers
	// Full registers estimated 143 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4232(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x6_end
	MOVQ         in_0_base+4224(FP), CX
	MOVQ         in_1_base+4248(FP), DX
	MOVQ         in_2_base+4272(FP), BX
	MOVQ         in_3_base+4296(FP), BP
	MOVQ         in_4_base+4320(FP), SI
	MOVQ         in_5_base+4344(FP), DI
	MOVQ         in_6_base+4368(FP), R8
	MOVQ         in_7_base+4392(FP), R9
	MOVQ         in_8_base+4416(FP), R10
	MOVQ         in_9_base+4440(FP), R11
	MOVQ         in_10_base+4464(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X6
	VPBROADCASTB X6, Y6
	XORQ         R13, R13

mulAvxTwo_11x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (CX)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+2112(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+2144(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+2176(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+2208(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+2240(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+2272(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (DX)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+2304(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+2336(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+2368(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+2400(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+2432(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+2464(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (BX)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+2496(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+2528(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+2560(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+2592(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+2624(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+2656(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (BP)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+2688(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+2720(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+2752(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+2784(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+2816(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+2848(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (SI)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+2880(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+2912(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+2944(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+2976(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+3008(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+3040(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (DI)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+3072(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+3104(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+3136(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+3168(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+3200(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+3232(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 6 to 6 outputs
	VMOVDQU (R8)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_72+1152(FP), Y7
	VMOVDQU high_72+3264(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_74+1184(FP), Y7
	VMOVDQU high_74+3296(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_76+1216(FP), Y7
	VMOVDQU high_76+3328(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_78+1248(FP), Y7
	VMOVDQU high_78+3360(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_80+1280(FP), Y7
	VMOVDQU high_80+3392(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_82+1312(FP), Y7
	VMOVDQU high_82+3424(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 7 to 6 outputs
	VMOVDQU (R9)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_84+1344(FP), Y7
	VMOVDQU high_84+3456(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_86+1376(FP), Y7
	VMOVDQU high_86+3488(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_88+1408(FP), Y7
	VMOVDQU high_88+3520(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_90+1440(FP), Y7
	VMOVDQU high_90+3552(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_92+1472(FP), Y7
	VMOVDQU high_92+3584(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_94+1504(FP), Y7
	VMOVDQU high_94+3616(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 8 to 6 outputs
	VMOVDQU (R10)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_96+1536(FP), Y7
	VMOVDQU high_96+3648(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_98+1568(FP), Y7
	VMOVDQU high_98+3680(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_100+1600(FP), Y7
	VMOVDQU high_100+3712(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_102+1632(FP), Y7
	VMOVDQU high_102+3744(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_104+1664(FP), Y7
	VMOVDQU high_104+3776(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_106+1696(FP), Y7
	VMOVDQU high_106+3808(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 9 to 6 outputs
	VMOVDQU (R11)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_108+1728(FP), Y7
	VMOVDQU high_108+3840(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_110+1760(FP), Y7
	VMOVDQU high_110+3872(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_112+1792(FP), Y7
	VMOVDQU high_112+3904(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_114+1824(FP), Y7
	VMOVDQU high_114+3936(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_116+1856(FP), Y7
	VMOVDQU high_116+3968(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_118+1888(FP), Y7
	VMOVDQU high_118+4000(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 10 to 6 outputs
	VMOVDQU (R12)(R13*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_120+1920(FP), Y7
	VMOVDQU high_120+4032(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_122+1952(FP), Y7
	VMOVDQU high_122+4064(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_124+1984(FP), Y7
	VMOVDQU high_124+4096(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_126+2016(FP), Y7
	VMOVDQU high_126+4128(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_128+2048(FP), Y7
	VMOVDQU high_128+4160(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_130+2080(FP), Y7
	VMOVDQU high_130+4192(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	MOVQ    out_0_base+4488(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+4512(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+4536(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+4560(FP), R14
	VMOVDQU Y3, (R14)(R13*1)
	MOVQ    out_4_base+4584(FP), R14
	VMOVDQU Y4, (R14)(R13*1)
	MOVQ    out_5_base+4608(FP), R14
	VMOVDQU Y5, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x6_loop
	VZEROUPPER

mulAvxTwo_11x6_end:
	RET

// func mulAvxTwo_11x7(low [154][16]byte, high [154][16]byte, in [11][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x7(SB), $0-5360
	// Loading no tables to registers
	// Full registers estimated 166 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4936(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x7_end
	MOVQ         in_0_base+4928(FP), CX
	MOVQ         in_1_base+4952(FP), DX
	MOVQ         in_2_base+4976(FP), BX
	MOVQ         in_3_base+5000(FP), BP
	MOVQ         in_4_base+5024(FP), SI
	MOVQ         in_5_base+5048(FP), DI
	MOVQ         in_6_base+5072(FP), R8
	MOVQ         in_7_base+5096(FP), R9
	MOVQ         in_8_base+5120(FP), R10
	MOVQ         in_9_base+5144(FP), R11
	MOVQ         in_10_base+5168(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X7
	VPBROADCASTB X7, Y7
	XORQ         R13, R13

mulAvxTwo_11x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (CX)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+2464(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+2496(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+2528(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+2560(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+2592(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+2624(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+2656(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (DX)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+2688(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+2720(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+2752(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+2784(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+2816(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+2848(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+2880(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (BX)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+2912(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+2944(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+2976(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+3008(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+3040(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+3072(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+3104(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (BP)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+3136(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+3168(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+3200(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+3232(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+3264(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+3296(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+3328(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (SI)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+3360(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+3392(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+3424(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+3456(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+3488(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+3520(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+3552(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (DI)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+3584(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+3616(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+3648(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+3680(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+3712(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+3744(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+3776(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 6 to 7 outputs
	VMOVDQU (R8)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_84+1344(FP), Y8
	VMOVDQU high_84+3808(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_86+1376(FP), Y8
	VMOVDQU high_86+3840(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_88+1408(FP), Y8
	VMOVDQU high_88+3872(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_90+1440(FP), Y8
	VMOVDQU high_90+3904(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_92+1472(FP), Y8
	VMOVDQU high_92+3936(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_94+1504(FP), Y8
	VMOVDQU high_94+3968(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_96+1536(FP), Y8
	VMOVDQU high_96+4000(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 7 to 7 outputs
	VMOVDQU (R9)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_98+1568(FP), Y8
	VMOVDQU high_98+4032(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_100+1600(FP), Y8
	VMOVDQU high_100+4064(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_102+1632(FP), Y8
	VMOVDQU high_102+4096(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_104+1664(FP), Y8
	VMOVDQU high_104+4128(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_106+1696(FP), Y8
	VMOVDQU high_106+4160(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_108+1728(FP), Y8
	VMOVDQU high_108+4192(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_110+1760(FP), Y8
	VMOVDQU high_110+4224(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 8 to 7 outputs
	VMOVDQU (R10)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_112+1792(FP), Y8
	VMOVDQU high_112+4256(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_114+1824(FP), Y8
	VMOVDQU high_114+4288(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_116+1856(FP), Y8
	VMOVDQU high_116+4320(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_118+1888(FP), Y8
	VMOVDQU high_118+4352(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_120+1920(FP), Y8
	VMOVDQU high_120+4384(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_122+1952(FP), Y8
	VMOVDQU high_122+4416(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_124+1984(FP), Y8
	VMOVDQU high_124+4448(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 9 to 7 outputs
	VMOVDQU (R11)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_126+2016(FP), Y8
	VMOVDQU high_126+4480(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_128+2048(FP), Y8
	VMOVDQU high_128+4512(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_130+2080(FP), Y8
	VMOVDQU high_130+4544(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_132+2112(FP), Y8
	VMOVDQU high_132+4576(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_134+2144(FP), Y8
	VMOVDQU high_134+4608(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_136+2176(FP), Y8
	VMOVDQU high_136+4640(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_138+2208(FP), Y8
	VMOVDQU high_138+4672(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 10 to 7 outputs
	VMOVDQU (R12)(R13*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_140+2240(FP), Y8
	VMOVDQU high_140+4704(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_142+2272(FP), Y8
	VMOVDQU high_142+4736(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_144+2304(FP), Y8
	VMOVDQU high_144+4768(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_146+2336(FP), Y8
	VMOVDQU high_146+4800(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_148+2368(FP), Y8
	VMOVDQU high_148+4832(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_150+2400(FP), Y8
	VMOVDQU high_150+4864(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_152+2432(FP), Y8
	VMOVDQU high_152+4896(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	MOVQ    out_0_base+5192(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+5216(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+5240(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+5264(FP), R14
	VMOVDQU Y3, (R14)(R13*1)
	MOVQ    out_4_base+5288(FP), R14
	VMOVDQU Y4, (R14)(R13*1)
	MOVQ    out_5_base+5312(FP), R14
	VMOVDQU Y5, (R14)(R13*1)
	MOVQ    out_6_base+5336(FP), R14
	VMOVDQU Y6, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x7_loop
	VZEROUPPER

mulAvxTwo_11x7_end:
	RET

// func mulAvxTwo_11x8(low [176][16]byte, high [176][16]byte, in [11][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x8(SB), $0-6088
	// Loading no tables to registers
	// Full registers estimated 189 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5640(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x8_end
	MOVQ         in_0_base+5632(FP), CX
	MOVQ         in_1_base+5656(FP), DX
	MOVQ         in_2_base+5680(FP), BX
	MOVQ         in_3_base+5704(FP), BP
	MOVQ         in_4_base+5728(FP), SI
	MOVQ         in_5_base+5752(FP), DI
	MOVQ         in_6_base+5776(FP), R8
	MOVQ         in_7_base+5800(FP), R9
	MOVQ         in_8_base+5824(FP), R10
	MOVQ         in_9_base+5848(FP), R11
	MOVQ         in_10_base+5872(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X8
	VPBROADCASTB X8, Y8
	XORQ         R13, R13

mulAvxTwo_11x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+2816(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+2848(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+2880(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+2912(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+2944(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+2976(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+3008(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+3040(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+3072(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+3104(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+3136(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+3168(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+3200(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+3232(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+3264(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+3296(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+3328(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+3360(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+3392(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+3424(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+3456(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+3488(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+3520(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+3552(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+3584(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+3616(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+3648(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+3680(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+3712(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+3744(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+3776(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+3808(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+3840(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+3872(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+3904(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+3936(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+3968(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+4000(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+4032(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+4064(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+4096(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+4128(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+4160(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+4192(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+4224(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+4256(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+4288(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+4320(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 6 to 8 outputs
	VMOVDQU (R8)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_96+1536(FP), Y9
	VMOVDQU high_96+4352(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_98+1568(FP), Y9
	VMOVDQU high_98+4384(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_100+1600(FP), Y9
	VMOVDQU high_100+4416(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_102+1632(FP), Y9
	VMOVDQU high_102+4448(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_104+1664(FP), Y9
	VMOVDQU high_104+4480(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_106+1696(FP), Y9
	VMOVDQU high_106+4512(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_108+1728(FP), Y9
	VMOVDQU high_108+4544(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_110+1760(FP), Y9
	VMOVDQU high_110+4576(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 7 to 8 outputs
	VMOVDQU (R9)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_112+1792(FP), Y9
	VMOVDQU high_112+4608(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_114+1824(FP), Y9
	VMOVDQU high_114+4640(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_116+1856(FP), Y9
	VMOVDQU high_116+4672(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_118+1888(FP), Y9
	VMOVDQU high_118+4704(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_120+1920(FP), Y9
	VMOVDQU high_120+4736(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_122+1952(FP), Y9
	VMOVDQU high_122+4768(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_124+1984(FP), Y9
	VMOVDQU high_124+4800(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_126+2016(FP), Y9
	VMOVDQU high_126+4832(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 8 to 8 outputs
	VMOVDQU (R10)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_128+2048(FP), Y9
	VMOVDQU high_128+4864(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_130+2080(FP), Y9
	VMOVDQU high_130+4896(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_132+2112(FP), Y9
	VMOVDQU high_132+4928(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_134+2144(FP), Y9
	VMOVDQU high_134+4960(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_136+2176(FP), Y9
	VMOVDQU high_136+4992(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_138+2208(FP), Y9
	VMOVDQU high_138+5024(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_140+2240(FP), Y9
	VMOVDQU high_140+5056(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_142+2272(FP), Y9
	VMOVDQU high_142+5088(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 9 to 8 outputs
	VMOVDQU (R11)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_144+2304(FP), Y9
	VMOVDQU high_144+5120(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_146+2336(FP), Y9
	VMOVDQU high_146+5152(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_148+2368(FP), Y9
	VMOVDQU high_148+5184(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_150+2400(FP), Y9
	VMOVDQU high_150+5216(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_152+2432(FP), Y9
	VMOVDQU high_152+5248(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_154+2464(FP), Y9
	VMOVDQU high_154+5280(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_156+2496(FP), Y9
	VMOVDQU high_156+5312(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_158+2528(FP), Y9
	VMOVDQU high_158+5344(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 10 to 8 outputs
	VMOVDQU (R12)(R13*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_160+2560(FP), Y9
	VMOVDQU high_160+5376(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_162+2592(FP), Y9
	VMOVDQU high_162+5408(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_164+2624(FP), Y9
	VMOVDQU high_164+5440(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_166+2656(FP), Y9
	VMOVDQU high_166+5472(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_168+2688(FP), Y9
	VMOVDQU high_168+5504(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_170+2720(FP), Y9
	VMOVDQU high_170+5536(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_172+2752(FP), Y9
	VMOVDQU high_172+5568(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_174+2784(FP), Y9
	VMOVDQU high_174+5600(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+5896(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+5920(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+5944(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+5968(FP), R14
	VMOVDQU Y3, (R14)(R13*1)
	MOVQ    out_4_base+5992(FP), R14
	VMOVDQU Y4, (R14)(R13*1)
	MOVQ    out_5_base+6016(FP), R14
	VMOVDQU Y5, (R14)(R13*1)
	MOVQ    out_6_base+6040(FP), R14
	VMOVDQU Y6, (R14)(R13*1)
	MOVQ    out_7_base+6064(FP), R14
	VMOVDQU Y7, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x8_loop
	VZEROUPPER

mulAvxTwo_11x8_end:
	RET

// func mulAvxTwo_11x9(low [198][16]byte, high [198][16]byte, in [11][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x9(SB), $0-6816
	// Loading no tables to registers
	// Full registers estimated 212 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+6344(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x9_end
	MOVQ         in_0_base+6336(FP), CX
	MOVQ         in_1_base+6360(FP), DX
	MOVQ         in_2_base+6384(FP), BX
	MOVQ         in_3_base+6408(FP), BP
	MOVQ         in_4_base+6432(FP), SI
	MOVQ         in_5_base+6456(FP), DI
	MOVQ         in_6_base+6480(FP), R8
	MOVQ         in_7_base+6504(FP), R9
	MOVQ         in_8_base+6528(FP), R10
	MOVQ         in_9_base+6552(FP), R11
	MOVQ         in_10_base+6576(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X9
	VPBROADCASTB X9, Y9
	XORQ         R13, R13

mulAvxTwo_11x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+3168(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+3200(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+3232(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+3264(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+3296(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+3328(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+3360(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+3392(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+3424(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+3456(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+3488(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+3520(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+3552(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+3584(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+3616(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+3648(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+3680(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+3712(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+3744(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+3776(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+3808(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+3840(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+3872(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+3904(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+3936(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+3968(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+4000(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+4032(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+4064(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+4096(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+4128(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+4160(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+4192(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+4224(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+4256(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+4288(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+4320(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+4352(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+4384(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+4416(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+4448(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+4480(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+4512(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+4544(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+4576(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+4608(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+4640(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+4672(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+4704(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+4736(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+4768(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+4800(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+4832(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+4864(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 6 to 9 outputs
	VMOVDQU (R8)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_108+1728(FP), Y10
	VMOVDQU high_108+4896(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_110+1760(FP), Y10
	VMOVDQU high_110+4928(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_112+1792(FP), Y10
	VMOVDQU high_112+4960(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_114+1824(FP), Y10
	VMOVDQU high_114+4992(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_116+1856(FP), Y10
	VMOVDQU high_116+5024(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_118+1888(FP), Y10
	VMOVDQU high_118+5056(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_120+1920(FP), Y10
	VMOVDQU high_120+5088(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_122+1952(FP), Y10
	VMOVDQU high_122+5120(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_124+1984(FP), Y10
	VMOVDQU high_124+5152(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 7 to 9 outputs
	VMOVDQU (R9)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_126+2016(FP), Y10
	VMOVDQU high_126+5184(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_128+2048(FP), Y10
	VMOVDQU high_128+5216(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_130+2080(FP), Y10
	VMOVDQU high_130+5248(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_132+2112(FP), Y10
	VMOVDQU high_132+5280(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_134+2144(FP), Y10
	VMOVDQU high_134+5312(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_136+2176(FP), Y10
	VMOVDQU high_136+5344(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_138+2208(FP), Y10
	VMOVDQU high_138+5376(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_140+2240(FP), Y10
	VMOVDQU high_140+5408(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_142+2272(FP), Y10
	VMOVDQU high_142+5440(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 8 to 9 outputs
	VMOVDQU (R10)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_144+2304(FP), Y10
	VMOVDQU high_144+5472(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_146+2336(FP), Y10
	VMOVDQU high_146+5504(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_148+2368(FP), Y10
	VMOVDQU high_148+5536(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_150+2400(FP), Y10
	VMOVDQU high_150+5568(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_152+2432(FP), Y10
	VMOVDQU high_152+5600(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_154+2464(FP), Y10
	VMOVDQU high_154+5632(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_156+2496(FP), Y10
	VMOVDQU high_156+5664(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_158+2528(FP), Y10
	VMOVDQU high_158+5696(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_160+2560(FP), Y10
	VMOVDQU high_160+5728(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 9 to 9 outputs
	VMOVDQU (R11)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_162+2592(FP), Y10
	VMOVDQU high_162+5760(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_164+2624(FP), Y10
	VMOVDQU high_164+5792(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_166+2656(FP), Y10
	VMOVDQU high_166+5824(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_168+2688(FP), Y10
	VMOVDQU high_168+5856(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_170+2720(FP), Y10
	VMOVDQU high_170+5888(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_172+2752(FP), Y10
	VMOVDQU high_172+5920(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_174+2784(FP), Y10
	VMOVDQU high_174+5952(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_176+2816(FP), Y10
	VMOVDQU high_176+5984(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_178+2848(FP), Y10
	VMOVDQU high_178+6016(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 10 to 9 outputs
	VMOVDQU (R12)(R13*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_180+2880(FP), Y10
	VMOVDQU high_180+6048(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_182+2912(FP), Y10
	VMOVDQU high_182+6080(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_184+2944(FP), Y10
	VMOVDQU high_184+6112(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_186+2976(FP), Y10
	VMOVDQU high_186+6144(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_188+3008(FP), Y10
	VMOVDQU high_188+6176(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_190+3040(FP), Y10
	VMOVDQU high_190+6208(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_192+3072(FP), Y10
	VMOVDQU high_192+6240(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_194+3104(FP), Y10
	VMOVDQU high_194+6272(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_196+3136(FP), Y10
	VMOVDQU high_196+6304(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+6600(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+6624(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+6648(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+6672(FP), R14
	VMOVDQU Y3, (R14)(R13*1)
	MOVQ    out_4_base+6696(FP), R14
	VMOVDQU Y4, (R14)(R13*1)
	MOVQ    out_5_base+6720(FP), R14
	VMOVDQU Y5, (R14)(R13*1)
	MOVQ    out_6_base+6744(FP), R14
	VMOVDQU Y6, (R14)(R13*1)
	MOVQ    out_7_base+6768(FP), R14
	VMOVDQU Y7, (R14)(R13*1)
	MOVQ    out_8_base+6792(FP), R14
	VMOVDQU Y8, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x9_loop
	VZEROUPPER

mulAvxTwo_11x9_end:
	RET

// func mulAvxTwo_11x10(low [220][16]byte, high [220][16]byte, in [11][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_11x10(SB), $0-7544
	// Loading no tables to registers
	// Full registers estimated 235 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+7048(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_11x10_end
	MOVQ         in_0_base+7040(FP), CX
	MOVQ         in_1_base+7064(FP), DX
	MOVQ         in_2_base+7088(FP), BX
	MOVQ         in_3_base+7112(FP), BP
	MOVQ         in_4_base+7136(FP), SI
	MOVQ         in_5_base+7160(FP), DI
	MOVQ         in_6_base+7184(FP), R8
	MOVQ         in_7_base+7208(FP), R9
	MOVQ         in_8_base+7232(FP), R10
	MOVQ         in_9_base+7256(FP), R11
	MOVQ         in_10_base+7280(FP), R12
	MOVQ         $0x0000000f, R13
	MOVQ         R13, X10
	VPBROADCASTB X10, Y10
	XORQ         R13, R13

mulAvxTwo_11x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+3520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+3552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+3584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+3616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+3648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+3680(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+3712(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+3744(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+3776(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+3808(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+3840(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+3872(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+3904(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+3936(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+3968(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+4000(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+4032(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+4064(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+4096(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+4128(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+4160(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+4192(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+4224(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+4256(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+4288(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+4320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+4352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+4384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+4416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+4448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+4480(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+4512(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+4544(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+4576(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+4608(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+4640(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+4672(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+4704(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+4736(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+4768(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+4800(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+4832(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+4864(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+4896(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+4928(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+4960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+4992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+5024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+5056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+5088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+5120(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+5152(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+5184(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+5216(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+5248(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+5280(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+5312(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+5344(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+5376(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+5408(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 6 to 10 outputs
	VMOVDQU (R8)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_120+1920(FP), Y11
	VMOVDQU high_120+5440(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_122+1952(FP), Y11
	VMOVDQU high_122+5472(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_124+1984(FP), Y11
	VMOVDQU high_124+5504(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_126+2016(FP), Y11
	VMOVDQU high_126+5536(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_128+2048(FP), Y11
	VMOVDQU high_128+5568(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_130+2080(FP), Y11
	VMOVDQU high_130+5600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_132+2112(FP), Y11
	VMOVDQU high_132+5632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_134+2144(FP), Y11
	VMOVDQU high_134+5664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_136+2176(FP), Y11
	VMOVDQU high_136+5696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_138+2208(FP), Y11
	VMOVDQU high_138+5728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 7 to 10 outputs
	VMOVDQU (R9)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_140+2240(FP), Y11
	VMOVDQU high_140+5760(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_142+2272(FP), Y11
	VMOVDQU high_142+5792(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_144+2304(FP), Y11
	VMOVDQU high_144+5824(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_146+2336(FP), Y11
	VMOVDQU high_146+5856(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_148+2368(FP), Y11
	VMOVDQU high_148+5888(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_150+2400(FP), Y11
	VMOVDQU high_150+5920(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_152+2432(FP), Y11
	VMOVDQU high_152+5952(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_154+2464(FP), Y11
	VMOVDQU high_154+5984(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_156+2496(FP), Y11
	VMOVDQU high_156+6016(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_158+2528(FP), Y11
	VMOVDQU high_158+6048(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 8 to 10 outputs
	VMOVDQU (R10)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_160+2560(FP), Y11
	VMOVDQU high_160+6080(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_162+2592(FP), Y11
	VMOVDQU high_162+6112(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_164+2624(FP), Y11
	VMOVDQU high_164+6144(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_166+2656(FP), Y11
	VMOVDQU high_166+6176(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_168+2688(FP), Y11
	VMOVDQU high_168+6208(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_170+2720(FP), Y11
	VMOVDQU high_170+6240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_172+2752(FP), Y11
	VMOVDQU high_172+6272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_174+2784(FP), Y11
	VMOVDQU high_174+6304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_176+2816(FP), Y11
	VMOVDQU high_176+6336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_178+2848(FP), Y11
	VMOVDQU high_178+6368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 9 to 10 outputs
	VMOVDQU (R11)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_180+2880(FP), Y11
	VMOVDQU high_180+6400(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_182+2912(FP), Y11
	VMOVDQU high_182+6432(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_184+2944(FP), Y11
	VMOVDQU high_184+6464(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_186+2976(FP), Y11
	VMOVDQU high_186+6496(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_188+3008(FP), Y11
	VMOVDQU high_188+6528(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_190+3040(FP), Y11
	VMOVDQU high_190+6560(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_192+3072(FP), Y11
	VMOVDQU high_192+6592(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_194+3104(FP), Y11
	VMOVDQU high_194+6624(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_196+3136(FP), Y11
	VMOVDQU high_196+6656(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_198+3168(FP), Y11
	VMOVDQU high_198+6688(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 10 to 10 outputs
	VMOVDQU (R12)(R13*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_200+3200(FP), Y11
	VMOVDQU high_200+6720(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_202+3232(FP), Y11
	VMOVDQU high_202+6752(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_204+3264(FP), Y11
	VMOVDQU high_204+6784(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_206+3296(FP), Y11
	VMOVDQU high_206+6816(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_208+3328(FP), Y11
	VMOVDQU high_208+6848(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_210+3360(FP), Y11
	VMOVDQU high_210+6880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_212+3392(FP), Y11
	VMOVDQU high_212+6912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_214+3424(FP), Y11
	VMOVDQU high_214+6944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_216+3456(FP), Y11
	VMOVDQU high_216+6976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_218+3488(FP), Y11
	VMOVDQU high_218+7008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+7304(FP), R14
	VMOVDQU Y0, (R14)(R13*1)
	MOVQ    out_1_base+7328(FP), R14
	VMOVDQU Y1, (R14)(R13*1)
	MOVQ    out_2_base+7352(FP), R14
	VMOVDQU Y2, (R14)(R13*1)
	MOVQ    out_3_base+7376(FP), R14
	VMOVDQU Y3, (R14)(R13*1)
	MOVQ    out_4_base+7400(FP), R14
	VMOVDQU Y4, (R14)(R13*1)
	MOVQ    out_5_base+7424(FP), R14
	VMOVDQU Y5, (R14)(R13*1)
	MOVQ    out_6_base+7448(FP), R14
	VMOVDQU Y6, (R14)(R13*1)
	MOVQ    out_7_base+7472(FP), R14
	VMOVDQU Y7, (R14)(R13*1)
	MOVQ    out_8_base+7496(FP), R14
	VMOVDQU Y8, (R14)(R13*1)
	MOVQ    out_9_base+7520(FP), R14
	VMOVDQU Y9, (R14)(R13*1)

	// Prepare for next loop
	ADDQ $0x20, R13
	DECQ AX
	JNZ  mulAvxTwo_11x10_loop
	VZEROUPPER

mulAvxTwo_11x10_end:
	RET

// func mulAvxTwo_12x1(low [24][16]byte, high [24][16]byte, in [12][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x1(SB), $0-1080
	// Loading no tables to registers
	// Full registers estimated 28 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+776(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x1_end
	MOVQ         out_0_base+1056(FP), CX
	MOVQ         in_0_base+768(FP), DX
	MOVQ         in_1_base+792(FP), BX
	MOVQ         in_2_base+816(FP), BP
	MOVQ         in_3_base+840(FP), SI
	MOVQ         in_4_base+864(FP), DI
	MOVQ         in_5_base+888(FP), R8
	MOVQ         in_6_base+912(FP), R9
	MOVQ         in_7_base+936(FP), R10
	MOVQ         in_8_base+960(FP), R11
	MOVQ         in_9_base+984(FP), R12
	MOVQ         in_10_base+1008(FP), R13
	MOVQ         in_11_base+1032(FP), R14
	MOVQ         $0x0000000f, R15
	MOVQ         R15, X1
	VPBROADCASTB X1, Y1
	XORQ         R15, R15

mulAvxTwo_12x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_0+0(FP), Y2
	VMOVDQU high_0+384(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_2+32(FP), Y2
	VMOVDQU high_2+416(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_4+64(FP), Y2
	VMOVDQU high_4+448(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_6+96(FP), Y2
	VMOVDQU high_6+480(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_8+128(FP), Y2
	VMOVDQU high_8+512(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_10+160(FP), Y2
	VMOVDQU high_10+544(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 6 to 1 outputs
	VMOVDQU (R9)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_12+192(FP), Y2
	VMOVDQU high_12+576(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 7 to 1 outputs
	VMOVDQU (R10)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_14+224(FP), Y2
	VMOVDQU high_14+608(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 8 to 1 outputs
	VMOVDQU (R11)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_16+256(FP), Y2
	VMOVDQU high_16+640(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 9 to 1 outputs
	VMOVDQU (R12)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_18+288(FP), Y2
	VMOVDQU high_18+672(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 10 to 1 outputs
	VMOVDQU (R13)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_20+320(FP), Y2
	VMOVDQU high_20+704(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Load and process 32 bytes from input 11 to 1 outputs
	VMOVDQU (R14)(R15*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y1, Y4, Y4
	VPAND   Y1, Y5, Y5
	VMOVDQU low_22+352(FP), Y2
	VMOVDQU high_22+736(FP), Y3
	VPSHUFB Y4, Y2, Y2
	VPSHUFB Y5, Y3, Y3
	VPXOR   Y2, Y3, Y2
	VPXOR   Y2, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R15*1)

	// Prepare for next loop
	ADDQ $0x20, R15
	DECQ AX
	JNZ  mulAvxTwo_12x1_loop
	VZEROUPPER

mulAvxTwo_12x1_end:
	RET

// func mulAvxTwo_12x2(low [48][16]byte, high [48][16]byte, in [12][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x2(SB), $0-1872
	// Loading no tables to registers
	// Full registers estimated 55 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+1544(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x2_end
	MOVQ         in_0_base+1536(FP), CX
	MOVQ         in_1_base+1560(FP), DX
	MOVQ         in_2_base+1584(FP), BX
	MOVQ         in_3_base+1608(FP), BP
	MOVQ         in_4_base+1632(FP), SI
	MOVQ         in_5_base+1656(FP), DI
	MOVQ         in_6_base+1680(FP), R8
	MOVQ         in_7_base+1704(FP), R9
	MOVQ         in_8_base+1728(FP), R10
	MOVQ         in_9_base+1752(FP), R11
	MOVQ         in_10_base+1776(FP), R12
	MOVQ         in_11_base+1800(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X2
	VPBROADCASTB X2, Y2
	XORQ         R14, R14

mulAvxTwo_12x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (CX)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_0+0(FP), Y3
	VMOVDQU high_0+768(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_2+32(FP), Y3
	VMOVDQU high_2+800(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (DX)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_4+64(FP), Y3
	VMOVDQU high_4+832(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_6+96(FP), Y3
	VMOVDQU high_6+864(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 2 to 2 outputs
	VMOVDQU (BX)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_8+128(FP), Y3
	VMOVDQU high_8+896(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_10+160(FP), Y3
	VMOVDQU high_10+928(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 3 to 2 outputs
	VMOVDQU (BP)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_12+192(FP), Y3
	VMOVDQU high_12+960(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_14+224(FP), Y3
	VMOVDQU high_14+992(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 4 to 2 outputs
	VMOVDQU (SI)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_16+256(FP), Y3
	VMOVDQU high_16+1024(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_18+288(FP), Y3
	VMOVDQU high_18+1056(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 5 to 2 outputs
	VMOVDQU (DI)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_20+320(FP), Y3
	VMOVDQU high_20+1088(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_22+352(FP), Y3
	VMOVDQU high_22+1120(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 6 to 2 outputs
	VMOVDQU (R8)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_24+384(FP), Y3
	VMOVDQU high_24+1152(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_26+416(FP), Y3
	VMOVDQU high_26+1184(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 7 to 2 outputs
	VMOVDQU (R9)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_28+448(FP), Y3
	VMOVDQU high_28+1216(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_30+480(FP), Y3
	VMOVDQU high_30+1248(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 8 to 2 outputs
	VMOVDQU (R10)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_32+512(FP), Y3
	VMOVDQU high_32+1280(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_34+544(FP), Y3
	VMOVDQU high_34+1312(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 9 to 2 outputs
	VMOVDQU (R11)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_36+576(FP), Y3
	VMOVDQU high_36+1344(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_38+608(FP), Y3
	VMOVDQU high_38+1376(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 10 to 2 outputs
	VMOVDQU (R12)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_40+640(FP), Y3
	VMOVDQU high_40+1408(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_42+672(FP), Y3
	VMOVDQU high_42+1440(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Load and process 32 bytes from input 11 to 2 outputs
	VMOVDQU (R13)(R14*1), Y5
	VPSRLQ  $0x04, Y5, Y6
	VPAND   Y2, Y5, Y5
	VPAND   Y2, Y6, Y6
	VMOVDQU low_44+704(FP), Y3
	VMOVDQU high_44+1472(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y0, Y0
	VMOVDQU low_46+736(FP), Y3
	VMOVDQU high_46+1504(FP), Y4
	VPSHUFB Y5, Y3, Y3
	VPSHUFB Y6, Y4, Y4
	VPXOR   Y3, Y4, Y3
	VPXOR   Y3, Y1, Y1

	// Store 2 outputs
	MOVQ    out_0_base+1824(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+1848(FP), R15
	VMOVDQU Y1, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x2_loop
	VZEROUPPER

mulAvxTwo_12x2_end:
	RET

// func mulAvxTwo_12x3(low [72][16]byte, high [72][16]byte, in [12][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x3(SB), $0-2664
	// Loading no tables to registers
	// Full registers estimated 80 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+2312(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x3_end
	MOVQ         in_0_base+2304(FP), CX
	MOVQ         in_1_base+2328(FP), DX
	MOVQ         in_2_base+2352(FP), BX
	MOVQ         in_3_base+2376(FP), BP
	MOVQ         in_4_base+2400(FP), SI
	MOVQ         in_5_base+2424(FP), DI
	MOVQ         in_6_base+2448(FP), R8
	MOVQ         in_7_base+2472(FP), R9
	MOVQ         in_8_base+2496(FP), R10
	MOVQ         in_9_base+2520(FP), R11
	MOVQ         in_10_base+2544(FP), R12
	MOVQ         in_11_base+2568(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X3
	VPBROADCASTB X3, Y3
	XORQ         R14, R14

mulAvxTwo_12x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (CX)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_0+0(FP), Y4
	VMOVDQU high_0+1152(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_2+32(FP), Y4
	VMOVDQU high_2+1184(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_4+64(FP), Y4
	VMOVDQU high_4+1216(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 1 to 3 outputs
	VMOVDQU (DX)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_6+96(FP), Y4
	VMOVDQU high_6+1248(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_8+128(FP), Y4
	VMOVDQU high_8+1280(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_10+160(FP), Y4
	VMOVDQU high_10+1312(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 2 to 3 outputs
	VMOVDQU (BX)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_12+192(FP), Y4
	VMOVDQU high_12+1344(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_14+224(FP), Y4
	VMOVDQU high_14+1376(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_16+256(FP), Y4
	VMOVDQU high_16+1408(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 3 to 3 outputs
	VMOVDQU (BP)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_18+288(FP), Y4
	VMOVDQU high_18+1440(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_20+320(FP), Y4
	VMOVDQU high_20+1472(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_22+352(FP), Y4
	VMOVDQU high_22+1504(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 4 to 3 outputs
	VMOVDQU (SI)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_24+384(FP), Y4
	VMOVDQU high_24+1536(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_26+416(FP), Y4
	VMOVDQU high_26+1568(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_28+448(FP), Y4
	VMOVDQU high_28+1600(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 5 to 3 outputs
	VMOVDQU (DI)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_30+480(FP), Y4
	VMOVDQU high_30+1632(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_32+512(FP), Y4
	VMOVDQU high_32+1664(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_34+544(FP), Y4
	VMOVDQU high_34+1696(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 6 to 3 outputs
	VMOVDQU (R8)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_36+576(FP), Y4
	VMOVDQU high_36+1728(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_38+608(FP), Y4
	VMOVDQU high_38+1760(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_40+640(FP), Y4
	VMOVDQU high_40+1792(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 7 to 3 outputs
	VMOVDQU (R9)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_42+672(FP), Y4
	VMOVDQU high_42+1824(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_44+704(FP), Y4
	VMOVDQU high_44+1856(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_46+736(FP), Y4
	VMOVDQU high_46+1888(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 8 to 3 outputs
	VMOVDQU (R10)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_48+768(FP), Y4
	VMOVDQU high_48+1920(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_50+800(FP), Y4
	VMOVDQU high_50+1952(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_52+832(FP), Y4
	VMOVDQU high_52+1984(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 9 to 3 outputs
	VMOVDQU (R11)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_54+864(FP), Y4
	VMOVDQU high_54+2016(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_56+896(FP), Y4
	VMOVDQU high_56+2048(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_58+928(FP), Y4
	VMOVDQU high_58+2080(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 10 to 3 outputs
	VMOVDQU (R12)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_60+960(FP), Y4
	VMOVDQU high_60+2112(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_62+992(FP), Y4
	VMOVDQU high_62+2144(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_64+1024(FP), Y4
	VMOVDQU high_64+2176(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Load and process 32 bytes from input 11 to 3 outputs
	VMOVDQU (R13)(R14*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y3, Y6, Y6
	VPAND   Y3, Y7, Y7
	VMOVDQU low_66+1056(FP), Y4
	VMOVDQU high_66+2208(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0
	VMOVDQU low_68+1088(FP), Y4
	VMOVDQU high_68+2240(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y1, Y1
	VMOVDQU low_70+1120(FP), Y4
	VMOVDQU high_70+2272(FP), Y5
	VPSHUFB Y6, Y4, Y4
	VPSHUFB Y7, Y5, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y2, Y2

	// Store 3 outputs
	MOVQ    out_0_base+2592(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+2616(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+2640(FP), R15
	VMOVDQU Y2, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x3_loop
	VZEROUPPER

mulAvxTwo_12x3_end:
	RET

// func mulAvxTwo_12x4(low [96][16]byte, high [96][16]byte, in [12][]byte, out [4][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x4(SB), $0-3456
	// Loading no tables to registers
	// Full registers estimated 105 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3080(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x4_end
	MOVQ         in_0_base+3072(FP), CX
	MOVQ         in_1_base+3096(FP), DX
	MOVQ         in_2_base+3120(FP), BX
	MOVQ         in_3_base+3144(FP), BP
	MOVQ         in_4_base+3168(FP), SI
	MOVQ         in_5_base+3192(FP), DI
	MOVQ         in_6_base+3216(FP), R8
	MOVQ         in_7_base+3240(FP), R9
	MOVQ         in_8_base+3264(FP), R10
	MOVQ         in_9_base+3288(FP), R11
	MOVQ         in_10_base+3312(FP), R12
	MOVQ         in_11_base+3336(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X4
	VPBROADCASTB X4, Y4
	XORQ         R14, R14

mulAvxTwo_12x4_loop:
	// Clear 4 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3

	// Load and process 32 bytes from input 0 to 4 outputs
	VMOVDQU (CX)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_0+0(FP), Y5
	VMOVDQU high_0+1536(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_2+32(FP), Y5
	VMOVDQU high_2+1568(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_4+64(FP), Y5
	VMOVDQU high_4+1600(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_6+96(FP), Y5
	VMOVDQU high_6+1632(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 1 to 4 outputs
	VMOVDQU (DX)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_8+128(FP), Y5
	VMOVDQU high_8+1664(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_10+160(FP), Y5
	VMOVDQU high_10+1696(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_12+192(FP), Y5
	VMOVDQU high_12+1728(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_14+224(FP), Y5
	VMOVDQU high_14+1760(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 2 to 4 outputs
	VMOVDQU (BX)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_16+256(FP), Y5
	VMOVDQU high_16+1792(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_18+288(FP), Y5
	VMOVDQU high_18+1824(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_20+320(FP), Y5
	VMOVDQU high_20+1856(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_22+352(FP), Y5
	VMOVDQU high_22+1888(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 3 to 4 outputs
	VMOVDQU (BP)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_24+384(FP), Y5
	VMOVDQU high_24+1920(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_26+416(FP), Y5
	VMOVDQU high_26+1952(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_28+448(FP), Y5
	VMOVDQU high_28+1984(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_30+480(FP), Y5
	VMOVDQU high_30+2016(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 4 to 4 outputs
	VMOVDQU (SI)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_32+512(FP), Y5
	VMOVDQU high_32+2048(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_34+544(FP), Y5
	VMOVDQU high_34+2080(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_36+576(FP), Y5
	VMOVDQU high_36+2112(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_38+608(FP), Y5
	VMOVDQU high_38+2144(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 5 to 4 outputs
	VMOVDQU (DI)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_40+640(FP), Y5
	VMOVDQU high_40+2176(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_42+672(FP), Y5
	VMOVDQU high_42+2208(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_44+704(FP), Y5
	VMOVDQU high_44+2240(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_46+736(FP), Y5
	VMOVDQU high_46+2272(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 6 to 4 outputs
	VMOVDQU (R8)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_48+768(FP), Y5
	VMOVDQU high_48+2304(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_50+800(FP), Y5
	VMOVDQU high_50+2336(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_52+832(FP), Y5
	VMOVDQU high_52+2368(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_54+864(FP), Y5
	VMOVDQU high_54+2400(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 7 to 4 outputs
	VMOVDQU (R9)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_56+896(FP), Y5
	VMOVDQU high_56+2432(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_58+928(FP), Y5
	VMOVDQU high_58+2464(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_60+960(FP), Y5
	VMOVDQU high_60+2496(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_62+992(FP), Y5
	VMOVDQU high_62+2528(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 8 to 4 outputs
	VMOVDQU (R10)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_64+1024(FP), Y5
	VMOVDQU high_64+2560(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_66+1056(FP), Y5
	VMOVDQU high_66+2592(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_68+1088(FP), Y5
	VMOVDQU high_68+2624(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_70+1120(FP), Y5
	VMOVDQU high_70+2656(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 9 to 4 outputs
	VMOVDQU (R11)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_72+1152(FP), Y5
	VMOVDQU high_72+2688(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_74+1184(FP), Y5
	VMOVDQU high_74+2720(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_76+1216(FP), Y5
	VMOVDQU high_76+2752(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_78+1248(FP), Y5
	VMOVDQU high_78+2784(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 10 to 4 outputs
	VMOVDQU (R12)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_80+1280(FP), Y5
	VMOVDQU high_80+2816(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_82+1312(FP), Y5
	VMOVDQU high_82+2848(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_84+1344(FP), Y5
	VMOVDQU high_84+2880(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_86+1376(FP), Y5
	VMOVDQU high_86+2912(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Load and process 32 bytes from input 11 to 4 outputs
	VMOVDQU (R13)(R14*1), Y7
	VPSRLQ  $0x04, Y7, Y8
	VPAND   Y4, Y7, Y7
	VPAND   Y4, Y8, Y8
	VMOVDQU low_88+1408(FP), Y5
	VMOVDQU high_88+2944(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y0, Y0
	VMOVDQU low_90+1440(FP), Y5
	VMOVDQU high_90+2976(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y1, Y1
	VMOVDQU low_92+1472(FP), Y5
	VMOVDQU high_92+3008(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y2, Y2
	VMOVDQU low_94+1504(FP), Y5
	VMOVDQU high_94+3040(FP), Y6
	VPSHUFB Y7, Y5, Y5
	VPSHUFB Y8, Y6, Y6
	VPXOR   Y5, Y6, Y5
	VPXOR   Y5, Y3, Y3

	// Store 4 outputs
	MOVQ    out_0_base+3360(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+3384(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+3408(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+3432(FP), R15
	VMOVDQU Y3, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x4_loop
	VZEROUPPER

mulAvxTwo_12x4_end:
	RET

// func mulAvxTwo_12x5(low [120][16]byte, high [120][16]byte, in [12][]byte, out [5][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x5(SB), $0-4248
	// Loading no tables to registers
	// Full registers estimated 130 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+3848(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x5_end
	MOVQ         in_0_base+3840(FP), CX
	MOVQ         in_1_base+3864(FP), DX
	MOVQ         in_2_base+3888(FP), BX
	MOVQ         in_3_base+3912(FP), BP
	MOVQ         in_4_base+3936(FP), SI
	MOVQ         in_5_base+3960(FP), DI
	MOVQ         in_6_base+3984(FP), R8
	MOVQ         in_7_base+4008(FP), R9
	MOVQ         in_8_base+4032(FP), R10
	MOVQ         in_9_base+4056(FP), R11
	MOVQ         in_10_base+4080(FP), R12
	MOVQ         in_11_base+4104(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X5
	VPBROADCASTB X5, Y5
	XORQ         R14, R14

mulAvxTwo_12x5_loop:
	// Clear 5 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4

	// Load and process 32 bytes from input 0 to 5 outputs
	VMOVDQU (CX)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_0+0(FP), Y6
	VMOVDQU high_0+1920(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_2+32(FP), Y6
	VMOVDQU high_2+1952(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_4+64(FP), Y6
	VMOVDQU high_4+1984(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_6+96(FP), Y6
	VMOVDQU high_6+2016(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_8+128(FP), Y6
	VMOVDQU high_8+2048(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 1 to 5 outputs
	VMOVDQU (DX)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_10+160(FP), Y6
	VMOVDQU high_10+2080(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_12+192(FP), Y6
	VMOVDQU high_12+2112(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_14+224(FP), Y6
	VMOVDQU high_14+2144(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_16+256(FP), Y6
	VMOVDQU high_16+2176(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_18+288(FP), Y6
	VMOVDQU high_18+2208(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 2 to 5 outputs
	VMOVDQU (BX)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_20+320(FP), Y6
	VMOVDQU high_20+2240(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_22+352(FP), Y6
	VMOVDQU high_22+2272(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_24+384(FP), Y6
	VMOVDQU high_24+2304(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_26+416(FP), Y6
	VMOVDQU high_26+2336(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_28+448(FP), Y6
	VMOVDQU high_28+2368(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 3 to 5 outputs
	VMOVDQU (BP)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_30+480(FP), Y6
	VMOVDQU high_30+2400(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_32+512(FP), Y6
	VMOVDQU high_32+2432(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_34+544(FP), Y6
	VMOVDQU high_34+2464(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_36+576(FP), Y6
	VMOVDQU high_36+2496(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_38+608(FP), Y6
	VMOVDQU high_38+2528(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 4 to 5 outputs
	VMOVDQU (SI)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_40+640(FP), Y6
	VMOVDQU high_40+2560(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_42+672(FP), Y6
	VMOVDQU high_42+2592(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_44+704(FP), Y6
	VMOVDQU high_44+2624(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_46+736(FP), Y6
	VMOVDQU high_46+2656(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_48+768(FP), Y6
	VMOVDQU high_48+2688(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 5 to 5 outputs
	VMOVDQU (DI)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_50+800(FP), Y6
	VMOVDQU high_50+2720(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_52+832(FP), Y6
	VMOVDQU high_52+2752(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_54+864(FP), Y6
	VMOVDQU high_54+2784(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_56+896(FP), Y6
	VMOVDQU high_56+2816(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_58+928(FP), Y6
	VMOVDQU high_58+2848(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 6 to 5 outputs
	VMOVDQU (R8)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_60+960(FP), Y6
	VMOVDQU high_60+2880(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_62+992(FP), Y6
	VMOVDQU high_62+2912(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_64+1024(FP), Y6
	VMOVDQU high_64+2944(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_66+1056(FP), Y6
	VMOVDQU high_66+2976(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_68+1088(FP), Y6
	VMOVDQU high_68+3008(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 7 to 5 outputs
	VMOVDQU (R9)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_70+1120(FP), Y6
	VMOVDQU high_70+3040(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_72+1152(FP), Y6
	VMOVDQU high_72+3072(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_74+1184(FP), Y6
	VMOVDQU high_74+3104(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_76+1216(FP), Y6
	VMOVDQU high_76+3136(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_78+1248(FP), Y6
	VMOVDQU high_78+3168(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 8 to 5 outputs
	VMOVDQU (R10)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_80+1280(FP), Y6
	VMOVDQU high_80+3200(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_82+1312(FP), Y6
	VMOVDQU high_82+3232(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_84+1344(FP), Y6
	VMOVDQU high_84+3264(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_86+1376(FP), Y6
	VMOVDQU high_86+3296(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_88+1408(FP), Y6
	VMOVDQU high_88+3328(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 9 to 5 outputs
	VMOVDQU (R11)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_90+1440(FP), Y6
	VMOVDQU high_90+3360(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_92+1472(FP), Y6
	VMOVDQU high_92+3392(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_94+1504(FP), Y6
	VMOVDQU high_94+3424(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_96+1536(FP), Y6
	VMOVDQU high_96+3456(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_98+1568(FP), Y6
	VMOVDQU high_98+3488(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 10 to 5 outputs
	VMOVDQU (R12)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_100+1600(FP), Y6
	VMOVDQU high_100+3520(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_102+1632(FP), Y6
	VMOVDQU high_102+3552(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_104+1664(FP), Y6
	VMOVDQU high_104+3584(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_106+1696(FP), Y6
	VMOVDQU high_106+3616(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_108+1728(FP), Y6
	VMOVDQU high_108+3648(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Load and process 32 bytes from input 11 to 5 outputs
	VMOVDQU (R13)(R14*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y5, Y8, Y8
	VPAND   Y5, Y9, Y9
	VMOVDQU low_110+1760(FP), Y6
	VMOVDQU high_110+3680(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0
	VMOVDQU low_112+1792(FP), Y6
	VMOVDQU high_112+3712(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y1, Y1
	VMOVDQU low_114+1824(FP), Y6
	VMOVDQU high_114+3744(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y2, Y2
	VMOVDQU low_116+1856(FP), Y6
	VMOVDQU high_116+3776(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y3, Y3
	VMOVDQU low_118+1888(FP), Y6
	VMOVDQU high_118+3808(FP), Y7
	VPSHUFB Y8, Y6, Y6
	VPSHUFB Y9, Y7, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y4, Y4

	// Store 5 outputs
	MOVQ    out_0_base+4128(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+4152(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+4176(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+4200(FP), R15
	VMOVDQU Y3, (R15)(R14*1)
	MOVQ    out_4_base+4224(FP), R15
	VMOVDQU Y4, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x5_loop
	VZEROUPPER

mulAvxTwo_12x5_end:
	RET

// func mulAvxTwo_12x6(low [144][16]byte, high [144][16]byte, in [12][]byte, out [6][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x6(SB), $0-5040
	// Loading no tables to registers
	// Full registers estimated 155 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+4616(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x6_end
	MOVQ         in_0_base+4608(FP), CX
	MOVQ         in_1_base+4632(FP), DX
	MOVQ         in_2_base+4656(FP), BX
	MOVQ         in_3_base+4680(FP), BP
	MOVQ         in_4_base+4704(FP), SI
	MOVQ         in_5_base+4728(FP), DI
	MOVQ         in_6_base+4752(FP), R8
	MOVQ         in_7_base+4776(FP), R9
	MOVQ         in_8_base+4800(FP), R10
	MOVQ         in_9_base+4824(FP), R11
	MOVQ         in_10_base+4848(FP), R12
	MOVQ         in_11_base+4872(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X6
	VPBROADCASTB X6, Y6
	XORQ         R14, R14

mulAvxTwo_12x6_loop:
	// Clear 6 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5

	// Load and process 32 bytes from input 0 to 6 outputs
	VMOVDQU (CX)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_0+0(FP), Y7
	VMOVDQU high_0+2304(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_2+32(FP), Y7
	VMOVDQU high_2+2336(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_4+64(FP), Y7
	VMOVDQU high_4+2368(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_6+96(FP), Y7
	VMOVDQU high_6+2400(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_8+128(FP), Y7
	VMOVDQU high_8+2432(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_10+160(FP), Y7
	VMOVDQU high_10+2464(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 1 to 6 outputs
	VMOVDQU (DX)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_12+192(FP), Y7
	VMOVDQU high_12+2496(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_14+224(FP), Y7
	VMOVDQU high_14+2528(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_16+256(FP), Y7
	VMOVDQU high_16+2560(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_18+288(FP), Y7
	VMOVDQU high_18+2592(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_20+320(FP), Y7
	VMOVDQU high_20+2624(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_22+352(FP), Y7
	VMOVDQU high_22+2656(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 2 to 6 outputs
	VMOVDQU (BX)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_24+384(FP), Y7
	VMOVDQU high_24+2688(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_26+416(FP), Y7
	VMOVDQU high_26+2720(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_28+448(FP), Y7
	VMOVDQU high_28+2752(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_30+480(FP), Y7
	VMOVDQU high_30+2784(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_32+512(FP), Y7
	VMOVDQU high_32+2816(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_34+544(FP), Y7
	VMOVDQU high_34+2848(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 3 to 6 outputs
	VMOVDQU (BP)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_36+576(FP), Y7
	VMOVDQU high_36+2880(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_38+608(FP), Y7
	VMOVDQU high_38+2912(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_40+640(FP), Y7
	VMOVDQU high_40+2944(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_42+672(FP), Y7
	VMOVDQU high_42+2976(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_44+704(FP), Y7
	VMOVDQU high_44+3008(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_46+736(FP), Y7
	VMOVDQU high_46+3040(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 4 to 6 outputs
	VMOVDQU (SI)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_48+768(FP), Y7
	VMOVDQU high_48+3072(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_50+800(FP), Y7
	VMOVDQU high_50+3104(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_52+832(FP), Y7
	VMOVDQU high_52+3136(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_54+864(FP), Y7
	VMOVDQU high_54+3168(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_56+896(FP), Y7
	VMOVDQU high_56+3200(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_58+928(FP), Y7
	VMOVDQU high_58+3232(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 5 to 6 outputs
	VMOVDQU (DI)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_60+960(FP), Y7
	VMOVDQU high_60+3264(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_62+992(FP), Y7
	VMOVDQU high_62+3296(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_64+1024(FP), Y7
	VMOVDQU high_64+3328(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_66+1056(FP), Y7
	VMOVDQU high_66+3360(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_68+1088(FP), Y7
	VMOVDQU high_68+3392(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_70+1120(FP), Y7
	VMOVDQU high_70+3424(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 6 to 6 outputs
	VMOVDQU (R8)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_72+1152(FP), Y7
	VMOVDQU high_72+3456(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_74+1184(FP), Y7
	VMOVDQU high_74+3488(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_76+1216(FP), Y7
	VMOVDQU high_76+3520(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_78+1248(FP), Y7
	VMOVDQU high_78+3552(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_80+1280(FP), Y7
	VMOVDQU high_80+3584(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_82+1312(FP), Y7
	VMOVDQU high_82+3616(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 7 to 6 outputs
	VMOVDQU (R9)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_84+1344(FP), Y7
	VMOVDQU high_84+3648(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_86+1376(FP), Y7
	VMOVDQU high_86+3680(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_88+1408(FP), Y7
	VMOVDQU high_88+3712(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_90+1440(FP), Y7
	VMOVDQU high_90+3744(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_92+1472(FP), Y7
	VMOVDQU high_92+3776(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_94+1504(FP), Y7
	VMOVDQU high_94+3808(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 8 to 6 outputs
	VMOVDQU (R10)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_96+1536(FP), Y7
	VMOVDQU high_96+3840(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_98+1568(FP), Y7
	VMOVDQU high_98+3872(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_100+1600(FP), Y7
	VMOVDQU high_100+3904(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_102+1632(FP), Y7
	VMOVDQU high_102+3936(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_104+1664(FP), Y7
	VMOVDQU high_104+3968(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_106+1696(FP), Y7
	VMOVDQU high_106+4000(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 9 to 6 outputs
	VMOVDQU (R11)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_108+1728(FP), Y7
	VMOVDQU high_108+4032(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_110+1760(FP), Y7
	VMOVDQU high_110+4064(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_112+1792(FP), Y7
	VMOVDQU high_112+4096(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_114+1824(FP), Y7
	VMOVDQU high_114+4128(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_116+1856(FP), Y7
	VMOVDQU high_116+4160(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_118+1888(FP), Y7
	VMOVDQU high_118+4192(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 10 to 6 outputs
	VMOVDQU (R12)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_120+1920(FP), Y7
	VMOVDQU high_120+4224(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_122+1952(FP), Y7
	VMOVDQU high_122+4256(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_124+1984(FP), Y7
	VMOVDQU high_124+4288(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_126+2016(FP), Y7
	VMOVDQU high_126+4320(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_128+2048(FP), Y7
	VMOVDQU high_128+4352(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_130+2080(FP), Y7
	VMOVDQU high_130+4384(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Load and process 32 bytes from input 11 to 6 outputs
	VMOVDQU (R13)(R14*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VMOVDQU low_132+2112(FP), Y7
	VMOVDQU high_132+4416(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VMOVDQU low_134+2144(FP), Y7
	VMOVDQU high_134+4448(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1
	VMOVDQU low_136+2176(FP), Y7
	VMOVDQU high_136+4480(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y2, Y2
	VMOVDQU low_138+2208(FP), Y7
	VMOVDQU high_138+4512(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y3, Y3
	VMOVDQU low_140+2240(FP), Y7
	VMOVDQU high_140+4544(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y4, Y4
	VMOVDQU low_142+2272(FP), Y7
	VMOVDQU high_142+4576(FP), Y8
	VPSHUFB Y9, Y7, Y7
	VPSHUFB Y10, Y8, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y5, Y5

	// Store 6 outputs
	MOVQ    out_0_base+4896(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+4920(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+4944(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+4968(FP), R15
	VMOVDQU Y3, (R15)(R14*1)
	MOVQ    out_4_base+4992(FP), R15
	VMOVDQU Y4, (R15)(R14*1)
	MOVQ    out_5_base+5016(FP), R15
	VMOVDQU Y5, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x6_loop
	VZEROUPPER

mulAvxTwo_12x6_end:
	RET

// func mulAvxTwo_12x7(low [168][16]byte, high [168][16]byte, in [12][]byte, out [7][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x7(SB), $0-5832
	// Loading no tables to registers
	// Full registers estimated 180 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+5384(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x7_end
	MOVQ         in_0_base+5376(FP), CX
	MOVQ         in_1_base+5400(FP), DX
	MOVQ         in_2_base+5424(FP), BX
	MOVQ         in_3_base+5448(FP), BP
	MOVQ         in_4_base+5472(FP), SI
	MOVQ         in_5_base+5496(FP), DI
	MOVQ         in_6_base+5520(FP), R8
	MOVQ         in_7_base+5544(FP), R9
	MOVQ         in_8_base+5568(FP), R10
	MOVQ         in_9_base+5592(FP), R11
	MOVQ         in_10_base+5616(FP), R12
	MOVQ         in_11_base+5640(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X7
	VPBROADCASTB X7, Y7
	XORQ         R14, R14

mulAvxTwo_12x7_loop:
	// Clear 7 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6

	// Load and process 32 bytes from input 0 to 7 outputs
	VMOVDQU (CX)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_0+0(FP), Y8
	VMOVDQU high_0+2688(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_2+32(FP), Y8
	VMOVDQU high_2+2720(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_4+64(FP), Y8
	VMOVDQU high_4+2752(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_6+96(FP), Y8
	VMOVDQU high_6+2784(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_8+128(FP), Y8
	VMOVDQU high_8+2816(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_10+160(FP), Y8
	VMOVDQU high_10+2848(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_12+192(FP), Y8
	VMOVDQU high_12+2880(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 1 to 7 outputs
	VMOVDQU (DX)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_14+224(FP), Y8
	VMOVDQU high_14+2912(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_16+256(FP), Y8
	VMOVDQU high_16+2944(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_18+288(FP), Y8
	VMOVDQU high_18+2976(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_20+320(FP), Y8
	VMOVDQU high_20+3008(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_22+352(FP), Y8
	VMOVDQU high_22+3040(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_24+384(FP), Y8
	VMOVDQU high_24+3072(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_26+416(FP), Y8
	VMOVDQU high_26+3104(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 2 to 7 outputs
	VMOVDQU (BX)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_28+448(FP), Y8
	VMOVDQU high_28+3136(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_30+480(FP), Y8
	VMOVDQU high_30+3168(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_32+512(FP), Y8
	VMOVDQU high_32+3200(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_34+544(FP), Y8
	VMOVDQU high_34+3232(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_36+576(FP), Y8
	VMOVDQU high_36+3264(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_38+608(FP), Y8
	VMOVDQU high_38+3296(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_40+640(FP), Y8
	VMOVDQU high_40+3328(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 3 to 7 outputs
	VMOVDQU (BP)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_42+672(FP), Y8
	VMOVDQU high_42+3360(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_44+704(FP), Y8
	VMOVDQU high_44+3392(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_46+736(FP), Y8
	VMOVDQU high_46+3424(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_48+768(FP), Y8
	VMOVDQU high_48+3456(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_50+800(FP), Y8
	VMOVDQU high_50+3488(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_52+832(FP), Y8
	VMOVDQU high_52+3520(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_54+864(FP), Y8
	VMOVDQU high_54+3552(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 4 to 7 outputs
	VMOVDQU (SI)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_56+896(FP), Y8
	VMOVDQU high_56+3584(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_58+928(FP), Y8
	VMOVDQU high_58+3616(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_60+960(FP), Y8
	VMOVDQU high_60+3648(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_62+992(FP), Y8
	VMOVDQU high_62+3680(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_64+1024(FP), Y8
	VMOVDQU high_64+3712(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_66+1056(FP), Y8
	VMOVDQU high_66+3744(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_68+1088(FP), Y8
	VMOVDQU high_68+3776(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 5 to 7 outputs
	VMOVDQU (DI)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_70+1120(FP), Y8
	VMOVDQU high_70+3808(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_72+1152(FP), Y8
	VMOVDQU high_72+3840(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_74+1184(FP), Y8
	VMOVDQU high_74+3872(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_76+1216(FP), Y8
	VMOVDQU high_76+3904(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_78+1248(FP), Y8
	VMOVDQU high_78+3936(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_80+1280(FP), Y8
	VMOVDQU high_80+3968(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_82+1312(FP), Y8
	VMOVDQU high_82+4000(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 6 to 7 outputs
	VMOVDQU (R8)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_84+1344(FP), Y8
	VMOVDQU high_84+4032(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_86+1376(FP), Y8
	VMOVDQU high_86+4064(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_88+1408(FP), Y8
	VMOVDQU high_88+4096(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_90+1440(FP), Y8
	VMOVDQU high_90+4128(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_92+1472(FP), Y8
	VMOVDQU high_92+4160(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_94+1504(FP), Y8
	VMOVDQU high_94+4192(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_96+1536(FP), Y8
	VMOVDQU high_96+4224(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 7 to 7 outputs
	VMOVDQU (R9)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_98+1568(FP), Y8
	VMOVDQU high_98+4256(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_100+1600(FP), Y8
	VMOVDQU high_100+4288(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_102+1632(FP), Y8
	VMOVDQU high_102+4320(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_104+1664(FP), Y8
	VMOVDQU high_104+4352(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_106+1696(FP), Y8
	VMOVDQU high_106+4384(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_108+1728(FP), Y8
	VMOVDQU high_108+4416(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_110+1760(FP), Y8
	VMOVDQU high_110+4448(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 8 to 7 outputs
	VMOVDQU (R10)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_112+1792(FP), Y8
	VMOVDQU high_112+4480(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_114+1824(FP), Y8
	VMOVDQU high_114+4512(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_116+1856(FP), Y8
	VMOVDQU high_116+4544(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_118+1888(FP), Y8
	VMOVDQU high_118+4576(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_120+1920(FP), Y8
	VMOVDQU high_120+4608(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_122+1952(FP), Y8
	VMOVDQU high_122+4640(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_124+1984(FP), Y8
	VMOVDQU high_124+4672(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 9 to 7 outputs
	VMOVDQU (R11)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_126+2016(FP), Y8
	VMOVDQU high_126+4704(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_128+2048(FP), Y8
	VMOVDQU high_128+4736(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_130+2080(FP), Y8
	VMOVDQU high_130+4768(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_132+2112(FP), Y8
	VMOVDQU high_132+4800(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_134+2144(FP), Y8
	VMOVDQU high_134+4832(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_136+2176(FP), Y8
	VMOVDQU high_136+4864(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_138+2208(FP), Y8
	VMOVDQU high_138+4896(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 10 to 7 outputs
	VMOVDQU (R12)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_140+2240(FP), Y8
	VMOVDQU high_140+4928(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_142+2272(FP), Y8
	VMOVDQU high_142+4960(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_144+2304(FP), Y8
	VMOVDQU high_144+4992(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_146+2336(FP), Y8
	VMOVDQU high_146+5024(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_148+2368(FP), Y8
	VMOVDQU high_148+5056(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_150+2400(FP), Y8
	VMOVDQU high_150+5088(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_152+2432(FP), Y8
	VMOVDQU high_152+5120(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Load and process 32 bytes from input 11 to 7 outputs
	VMOVDQU (R13)(R14*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y7, Y10, Y10
	VPAND   Y7, Y11, Y11
	VMOVDQU low_154+2464(FP), Y8
	VMOVDQU high_154+5152(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0
	VMOVDQU low_156+2496(FP), Y8
	VMOVDQU high_156+5184(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y1, Y1
	VMOVDQU low_158+2528(FP), Y8
	VMOVDQU high_158+5216(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y2, Y2
	VMOVDQU low_160+2560(FP), Y8
	VMOVDQU high_160+5248(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y3, Y3
	VMOVDQU low_162+2592(FP), Y8
	VMOVDQU high_162+5280(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y4, Y4
	VMOVDQU low_164+2624(FP), Y8
	VMOVDQU high_164+5312(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y5, Y5
	VMOVDQU low_166+2656(FP), Y8
	VMOVDQU high_166+5344(FP), Y9
	VPSHUFB Y10, Y8, Y8
	VPSHUFB Y11, Y9, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y6, Y6

	// Store 7 outputs
	MOVQ    out_0_base+5664(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+5688(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+5712(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+5736(FP), R15
	VMOVDQU Y3, (R15)(R14*1)
	MOVQ    out_4_base+5760(FP), R15
	VMOVDQU Y4, (R15)(R14*1)
	MOVQ    out_5_base+5784(FP), R15
	VMOVDQU Y5, (R15)(R14*1)
	MOVQ    out_6_base+5808(FP), R15
	VMOVDQU Y6, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x7_loop
	VZEROUPPER

mulAvxTwo_12x7_end:
	RET

// func mulAvxTwo_12x8(low [192][16]byte, high [192][16]byte, in [12][]byte, out [8][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x8(SB), $0-6624
	// Loading no tables to registers
	// Full registers estimated 205 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+6152(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x8_end
	MOVQ         in_0_base+6144(FP), CX
	MOVQ         in_1_base+6168(FP), DX
	MOVQ         in_2_base+6192(FP), BX
	MOVQ         in_3_base+6216(FP), BP
	MOVQ         in_4_base+6240(FP), SI
	MOVQ         in_5_base+6264(FP), DI
	MOVQ         in_6_base+6288(FP), R8
	MOVQ         in_7_base+6312(FP), R9
	MOVQ         in_8_base+6336(FP), R10
	MOVQ         in_9_base+6360(FP), R11
	MOVQ         in_10_base+6384(FP), R12
	MOVQ         in_11_base+6408(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X8
	VPBROADCASTB X8, Y8
	XORQ         R14, R14

mulAvxTwo_12x8_loop:
	// Clear 8 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7

	// Load and process 32 bytes from input 0 to 8 outputs
	VMOVDQU (CX)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_0+0(FP), Y9
	VMOVDQU high_0+3072(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_2+32(FP), Y9
	VMOVDQU high_2+3104(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_4+64(FP), Y9
	VMOVDQU high_4+3136(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_6+96(FP), Y9
	VMOVDQU high_6+3168(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_8+128(FP), Y9
	VMOVDQU high_8+3200(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_10+160(FP), Y9
	VMOVDQU high_10+3232(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_12+192(FP), Y9
	VMOVDQU high_12+3264(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_14+224(FP), Y9
	VMOVDQU high_14+3296(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 1 to 8 outputs
	VMOVDQU (DX)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_16+256(FP), Y9
	VMOVDQU high_16+3328(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_18+288(FP), Y9
	VMOVDQU high_18+3360(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_20+320(FP), Y9
	VMOVDQU high_20+3392(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_22+352(FP), Y9
	VMOVDQU high_22+3424(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_24+384(FP), Y9
	VMOVDQU high_24+3456(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_26+416(FP), Y9
	VMOVDQU high_26+3488(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_28+448(FP), Y9
	VMOVDQU high_28+3520(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_30+480(FP), Y9
	VMOVDQU high_30+3552(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 2 to 8 outputs
	VMOVDQU (BX)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_32+512(FP), Y9
	VMOVDQU high_32+3584(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_34+544(FP), Y9
	VMOVDQU high_34+3616(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_36+576(FP), Y9
	VMOVDQU high_36+3648(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_38+608(FP), Y9
	VMOVDQU high_38+3680(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_40+640(FP), Y9
	VMOVDQU high_40+3712(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_42+672(FP), Y9
	VMOVDQU high_42+3744(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_44+704(FP), Y9
	VMOVDQU high_44+3776(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_46+736(FP), Y9
	VMOVDQU high_46+3808(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 3 to 8 outputs
	VMOVDQU (BP)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_48+768(FP), Y9
	VMOVDQU high_48+3840(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_50+800(FP), Y9
	VMOVDQU high_50+3872(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_52+832(FP), Y9
	VMOVDQU high_52+3904(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_54+864(FP), Y9
	VMOVDQU high_54+3936(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_56+896(FP), Y9
	VMOVDQU high_56+3968(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_58+928(FP), Y9
	VMOVDQU high_58+4000(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_60+960(FP), Y9
	VMOVDQU high_60+4032(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_62+992(FP), Y9
	VMOVDQU high_62+4064(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 4 to 8 outputs
	VMOVDQU (SI)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_64+1024(FP), Y9
	VMOVDQU high_64+4096(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_66+1056(FP), Y9
	VMOVDQU high_66+4128(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_68+1088(FP), Y9
	VMOVDQU high_68+4160(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_70+1120(FP), Y9
	VMOVDQU high_70+4192(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_72+1152(FP), Y9
	VMOVDQU high_72+4224(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_74+1184(FP), Y9
	VMOVDQU high_74+4256(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_76+1216(FP), Y9
	VMOVDQU high_76+4288(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_78+1248(FP), Y9
	VMOVDQU high_78+4320(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 5 to 8 outputs
	VMOVDQU (DI)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_80+1280(FP), Y9
	VMOVDQU high_80+4352(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_82+1312(FP), Y9
	VMOVDQU high_82+4384(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_84+1344(FP), Y9
	VMOVDQU high_84+4416(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_86+1376(FP), Y9
	VMOVDQU high_86+4448(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_88+1408(FP), Y9
	VMOVDQU high_88+4480(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_90+1440(FP), Y9
	VMOVDQU high_90+4512(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_92+1472(FP), Y9
	VMOVDQU high_92+4544(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_94+1504(FP), Y9
	VMOVDQU high_94+4576(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 6 to 8 outputs
	VMOVDQU (R8)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_96+1536(FP), Y9
	VMOVDQU high_96+4608(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_98+1568(FP), Y9
	VMOVDQU high_98+4640(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_100+1600(FP), Y9
	VMOVDQU high_100+4672(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_102+1632(FP), Y9
	VMOVDQU high_102+4704(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_104+1664(FP), Y9
	VMOVDQU high_104+4736(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_106+1696(FP), Y9
	VMOVDQU high_106+4768(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_108+1728(FP), Y9
	VMOVDQU high_108+4800(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_110+1760(FP), Y9
	VMOVDQU high_110+4832(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 7 to 8 outputs
	VMOVDQU (R9)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_112+1792(FP), Y9
	VMOVDQU high_112+4864(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_114+1824(FP), Y9
	VMOVDQU high_114+4896(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_116+1856(FP), Y9
	VMOVDQU high_116+4928(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_118+1888(FP), Y9
	VMOVDQU high_118+4960(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_120+1920(FP), Y9
	VMOVDQU high_120+4992(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_122+1952(FP), Y9
	VMOVDQU high_122+5024(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_124+1984(FP), Y9
	VMOVDQU high_124+5056(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_126+2016(FP), Y9
	VMOVDQU high_126+5088(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 8 to 8 outputs
	VMOVDQU (R10)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_128+2048(FP), Y9
	VMOVDQU high_128+5120(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_130+2080(FP), Y9
	VMOVDQU high_130+5152(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_132+2112(FP), Y9
	VMOVDQU high_132+5184(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_134+2144(FP), Y9
	VMOVDQU high_134+5216(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_136+2176(FP), Y9
	VMOVDQU high_136+5248(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_138+2208(FP), Y9
	VMOVDQU high_138+5280(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_140+2240(FP), Y9
	VMOVDQU high_140+5312(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_142+2272(FP), Y9
	VMOVDQU high_142+5344(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 9 to 8 outputs
	VMOVDQU (R11)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_144+2304(FP), Y9
	VMOVDQU high_144+5376(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_146+2336(FP), Y9
	VMOVDQU high_146+5408(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_148+2368(FP), Y9
	VMOVDQU high_148+5440(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_150+2400(FP), Y9
	VMOVDQU high_150+5472(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_152+2432(FP), Y9
	VMOVDQU high_152+5504(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_154+2464(FP), Y9
	VMOVDQU high_154+5536(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_156+2496(FP), Y9
	VMOVDQU high_156+5568(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_158+2528(FP), Y9
	VMOVDQU high_158+5600(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 10 to 8 outputs
	VMOVDQU (R12)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_160+2560(FP), Y9
	VMOVDQU high_160+5632(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_162+2592(FP), Y9
	VMOVDQU high_162+5664(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_164+2624(FP), Y9
	VMOVDQU high_164+5696(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_166+2656(FP), Y9
	VMOVDQU high_166+5728(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_168+2688(FP), Y9
	VMOVDQU high_168+5760(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_170+2720(FP), Y9
	VMOVDQU high_170+5792(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_172+2752(FP), Y9
	VMOVDQU high_172+5824(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_174+2784(FP), Y9
	VMOVDQU high_174+5856(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Load and process 32 bytes from input 11 to 8 outputs
	VMOVDQU (R13)(R14*1), Y11
	VPSRLQ  $0x04, Y11, Y12
	VPAND   Y8, Y11, Y11
	VPAND   Y8, Y12, Y12
	VMOVDQU low_176+2816(FP), Y9
	VMOVDQU high_176+5888(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y0, Y0
	VMOVDQU low_178+2848(FP), Y9
	VMOVDQU high_178+5920(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y1, Y1
	VMOVDQU low_180+2880(FP), Y9
	VMOVDQU high_180+5952(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y2, Y2
	VMOVDQU low_182+2912(FP), Y9
	VMOVDQU high_182+5984(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y3, Y3
	VMOVDQU low_184+2944(FP), Y9
	VMOVDQU high_184+6016(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y4, Y4
	VMOVDQU low_186+2976(FP), Y9
	VMOVDQU high_186+6048(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y5, Y5
	VMOVDQU low_188+3008(FP), Y9
	VMOVDQU high_188+6080(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y6, Y6
	VMOVDQU low_190+3040(FP), Y9
	VMOVDQU high_190+6112(FP), Y10
	VPSHUFB Y11, Y9, Y9
	VPSHUFB Y12, Y10, Y10
	VPXOR   Y9, Y10, Y9
	VPXOR   Y9, Y7, Y7

	// Store 8 outputs
	MOVQ    out_0_base+6432(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+6456(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+6480(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+6504(FP), R15
	VMOVDQU Y3, (R15)(R14*1)
	MOVQ    out_4_base+6528(FP), R15
	VMOVDQU Y4, (R15)(R14*1)
	MOVQ    out_5_base+6552(FP), R15
	VMOVDQU Y5, (R15)(R14*1)
	MOVQ    out_6_base+6576(FP), R15
	VMOVDQU Y6, (R15)(R14*1)
	MOVQ    out_7_base+6600(FP), R15
	VMOVDQU Y7, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x8_loop
	VZEROUPPER

mulAvxTwo_12x8_end:
	RET

// func mulAvxTwo_12x9(low [216][16]byte, high [216][16]byte, in [12][]byte, out [9][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x9(SB), $0-7416
	// Loading no tables to registers
	// Full registers estimated 230 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+6920(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x9_end
	MOVQ         in_0_base+6912(FP), CX
	MOVQ         in_1_base+6936(FP), DX
	MOVQ         in_2_base+6960(FP), BX
	MOVQ         in_3_base+6984(FP), BP
	MOVQ         in_4_base+7008(FP), SI
	MOVQ         in_5_base+7032(FP), DI
	MOVQ         in_6_base+7056(FP), R8
	MOVQ         in_7_base+7080(FP), R9
	MOVQ         in_8_base+7104(FP), R10
	MOVQ         in_9_base+7128(FP), R11
	MOVQ         in_10_base+7152(FP), R12
	MOVQ         in_11_base+7176(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X9
	VPBROADCASTB X9, Y9
	XORQ         R14, R14

mulAvxTwo_12x9_loop:
	// Clear 9 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8

	// Load and process 32 bytes from input 0 to 9 outputs
	VMOVDQU (CX)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_0+0(FP), Y10
	VMOVDQU high_0+3456(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_2+32(FP), Y10
	VMOVDQU high_2+3488(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_4+64(FP), Y10
	VMOVDQU high_4+3520(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_6+96(FP), Y10
	VMOVDQU high_6+3552(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_8+128(FP), Y10
	VMOVDQU high_8+3584(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_10+160(FP), Y10
	VMOVDQU high_10+3616(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_12+192(FP), Y10
	VMOVDQU high_12+3648(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_14+224(FP), Y10
	VMOVDQU high_14+3680(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_16+256(FP), Y10
	VMOVDQU high_16+3712(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 1 to 9 outputs
	VMOVDQU (DX)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_18+288(FP), Y10
	VMOVDQU high_18+3744(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_20+320(FP), Y10
	VMOVDQU high_20+3776(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_22+352(FP), Y10
	VMOVDQU high_22+3808(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_24+384(FP), Y10
	VMOVDQU high_24+3840(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_26+416(FP), Y10
	VMOVDQU high_26+3872(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_28+448(FP), Y10
	VMOVDQU high_28+3904(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_30+480(FP), Y10
	VMOVDQU high_30+3936(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_32+512(FP), Y10
	VMOVDQU high_32+3968(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_34+544(FP), Y10
	VMOVDQU high_34+4000(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 2 to 9 outputs
	VMOVDQU (BX)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_36+576(FP), Y10
	VMOVDQU high_36+4032(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_38+608(FP), Y10
	VMOVDQU high_38+4064(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_40+640(FP), Y10
	VMOVDQU high_40+4096(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_42+672(FP), Y10
	VMOVDQU high_42+4128(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_44+704(FP), Y10
	VMOVDQU high_44+4160(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_46+736(FP), Y10
	VMOVDQU high_46+4192(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_48+768(FP), Y10
	VMOVDQU high_48+4224(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_50+800(FP), Y10
	VMOVDQU high_50+4256(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_52+832(FP), Y10
	VMOVDQU high_52+4288(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 3 to 9 outputs
	VMOVDQU (BP)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_54+864(FP), Y10
	VMOVDQU high_54+4320(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_56+896(FP), Y10
	VMOVDQU high_56+4352(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_58+928(FP), Y10
	VMOVDQU high_58+4384(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_60+960(FP), Y10
	VMOVDQU high_60+4416(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_62+992(FP), Y10
	VMOVDQU high_62+4448(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_64+1024(FP), Y10
	VMOVDQU high_64+4480(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_66+1056(FP), Y10
	VMOVDQU high_66+4512(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_68+1088(FP), Y10
	VMOVDQU high_68+4544(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_70+1120(FP), Y10
	VMOVDQU high_70+4576(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 4 to 9 outputs
	VMOVDQU (SI)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_72+1152(FP), Y10
	VMOVDQU high_72+4608(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_74+1184(FP), Y10
	VMOVDQU high_74+4640(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_76+1216(FP), Y10
	VMOVDQU high_76+4672(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_78+1248(FP), Y10
	VMOVDQU high_78+4704(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_80+1280(FP), Y10
	VMOVDQU high_80+4736(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_82+1312(FP), Y10
	VMOVDQU high_82+4768(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_84+1344(FP), Y10
	VMOVDQU high_84+4800(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_86+1376(FP), Y10
	VMOVDQU high_86+4832(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_88+1408(FP), Y10
	VMOVDQU high_88+4864(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 5 to 9 outputs
	VMOVDQU (DI)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_90+1440(FP), Y10
	VMOVDQU high_90+4896(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_92+1472(FP), Y10
	VMOVDQU high_92+4928(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_94+1504(FP), Y10
	VMOVDQU high_94+4960(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_96+1536(FP), Y10
	VMOVDQU high_96+4992(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_98+1568(FP), Y10
	VMOVDQU high_98+5024(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_100+1600(FP), Y10
	VMOVDQU high_100+5056(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_102+1632(FP), Y10
	VMOVDQU high_102+5088(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_104+1664(FP), Y10
	VMOVDQU high_104+5120(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_106+1696(FP), Y10
	VMOVDQU high_106+5152(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 6 to 9 outputs
	VMOVDQU (R8)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_108+1728(FP), Y10
	VMOVDQU high_108+5184(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_110+1760(FP), Y10
	VMOVDQU high_110+5216(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_112+1792(FP), Y10
	VMOVDQU high_112+5248(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_114+1824(FP), Y10
	VMOVDQU high_114+5280(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_116+1856(FP), Y10
	VMOVDQU high_116+5312(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_118+1888(FP), Y10
	VMOVDQU high_118+5344(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_120+1920(FP), Y10
	VMOVDQU high_120+5376(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_122+1952(FP), Y10
	VMOVDQU high_122+5408(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_124+1984(FP), Y10
	VMOVDQU high_124+5440(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 7 to 9 outputs
	VMOVDQU (R9)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_126+2016(FP), Y10
	VMOVDQU high_126+5472(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_128+2048(FP), Y10
	VMOVDQU high_128+5504(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_130+2080(FP), Y10
	VMOVDQU high_130+5536(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_132+2112(FP), Y10
	VMOVDQU high_132+5568(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_134+2144(FP), Y10
	VMOVDQU high_134+5600(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_136+2176(FP), Y10
	VMOVDQU high_136+5632(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_138+2208(FP), Y10
	VMOVDQU high_138+5664(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_140+2240(FP), Y10
	VMOVDQU high_140+5696(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_142+2272(FP), Y10
	VMOVDQU high_142+5728(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 8 to 9 outputs
	VMOVDQU (R10)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_144+2304(FP), Y10
	VMOVDQU high_144+5760(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_146+2336(FP), Y10
	VMOVDQU high_146+5792(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_148+2368(FP), Y10
	VMOVDQU high_148+5824(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_150+2400(FP), Y10
	VMOVDQU high_150+5856(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_152+2432(FP), Y10
	VMOVDQU high_152+5888(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_154+2464(FP), Y10
	VMOVDQU high_154+5920(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_156+2496(FP), Y10
	VMOVDQU high_156+5952(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_158+2528(FP), Y10
	VMOVDQU high_158+5984(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_160+2560(FP), Y10
	VMOVDQU high_160+6016(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 9 to 9 outputs
	VMOVDQU (R11)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_162+2592(FP), Y10
	VMOVDQU high_162+6048(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_164+2624(FP), Y10
	VMOVDQU high_164+6080(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_166+2656(FP), Y10
	VMOVDQU high_166+6112(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_168+2688(FP), Y10
	VMOVDQU high_168+6144(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_170+2720(FP), Y10
	VMOVDQU high_170+6176(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_172+2752(FP), Y10
	VMOVDQU high_172+6208(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_174+2784(FP), Y10
	VMOVDQU high_174+6240(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_176+2816(FP), Y10
	VMOVDQU high_176+6272(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_178+2848(FP), Y10
	VMOVDQU high_178+6304(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 10 to 9 outputs
	VMOVDQU (R12)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_180+2880(FP), Y10
	VMOVDQU high_180+6336(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_182+2912(FP), Y10
	VMOVDQU high_182+6368(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_184+2944(FP), Y10
	VMOVDQU high_184+6400(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_186+2976(FP), Y10
	VMOVDQU high_186+6432(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_188+3008(FP), Y10
	VMOVDQU high_188+6464(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_190+3040(FP), Y10
	VMOVDQU high_190+6496(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_192+3072(FP), Y10
	VMOVDQU high_192+6528(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_194+3104(FP), Y10
	VMOVDQU high_194+6560(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_196+3136(FP), Y10
	VMOVDQU high_196+6592(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Load and process 32 bytes from input 11 to 9 outputs
	VMOVDQU (R13)(R14*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VMOVDQU low_198+3168(FP), Y10
	VMOVDQU high_198+6624(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VMOVDQU low_200+3200(FP), Y10
	VMOVDQU high_200+6656(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VMOVDQU low_202+3232(FP), Y10
	VMOVDQU high_202+6688(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2
	VMOVDQU low_204+3264(FP), Y10
	VMOVDQU high_204+6720(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y3, Y3
	VMOVDQU low_206+3296(FP), Y10
	VMOVDQU high_206+6752(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y4, Y4
	VMOVDQU low_208+3328(FP), Y10
	VMOVDQU high_208+6784(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y5, Y5
	VMOVDQU low_210+3360(FP), Y10
	VMOVDQU high_210+6816(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y6, Y6
	VMOVDQU low_212+3392(FP), Y10
	VMOVDQU high_212+6848(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y7, Y7
	VMOVDQU low_214+3424(FP), Y10
	VMOVDQU high_214+6880(FP), Y11
	VPSHUFB Y12, Y10, Y10
	VPSHUFB Y13, Y11, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y8, Y8

	// Store 9 outputs
	MOVQ    out_0_base+7200(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+7224(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+7248(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+7272(FP), R15
	VMOVDQU Y3, (R15)(R14*1)
	MOVQ    out_4_base+7296(FP), R15
	VMOVDQU Y4, (R15)(R14*1)
	MOVQ    out_5_base+7320(FP), R15
	VMOVDQU Y5, (R15)(R14*1)
	MOVQ    out_6_base+7344(FP), R15
	VMOVDQU Y6, (R15)(R14*1)
	MOVQ    out_7_base+7368(FP), R15
	VMOVDQU Y7, (R15)(R14*1)
	MOVQ    out_8_base+7392(FP), R15
	VMOVDQU Y8, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x9_loop
	VZEROUPPER

mulAvxTwo_12x9_end:
	RET

// func mulAvxTwo_12x10(low [240][16]byte, high [240][16]byte, in [12][]byte, out [10][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_12x10(SB), $0-8208
	// Loading no tables to registers
	// Full registers estimated 255 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+7688(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_12x10_end
	MOVQ         in_0_base+7680(FP), CX
	MOVQ         in_1_base+7704(FP), DX
	MOVQ         in_2_base+7728(FP), BX
	MOVQ         in_3_base+7752(FP), BP
	MOVQ         in_4_base+7776(FP), SI
	MOVQ         in_5_base+7800(FP), DI
	MOVQ         in_6_base+7824(FP), R8
	MOVQ         in_7_base+7848(FP), R9
	MOVQ         in_8_base+7872(FP), R10
	MOVQ         in_9_base+7896(FP), R11
	MOVQ         in_10_base+7920(FP), R12
	MOVQ         in_11_base+7944(FP), R13
	MOVQ         $0x0000000f, R14
	MOVQ         R14, X10
	VPBROADCASTB X10, Y10
	XORQ         R14, R14

mulAvxTwo_12x10_loop:
	// Clear 10 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2
	VPXOR Y3, Y3, Y3
	VPXOR Y4, Y4, Y4
	VPXOR Y5, Y5, Y5
	VPXOR Y6, Y6, Y6
	VPXOR Y7, Y7, Y7
	VPXOR Y8, Y8, Y8
	VPXOR Y9, Y9, Y9

	// Load and process 32 bytes from input 0 to 10 outputs
	VMOVDQU (CX)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_0+0(FP), Y11
	VMOVDQU high_0+3840(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_2+32(FP), Y11
	VMOVDQU high_2+3872(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_4+64(FP), Y11
	VMOVDQU high_4+3904(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_6+96(FP), Y11
	VMOVDQU high_6+3936(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_8+128(FP), Y11
	VMOVDQU high_8+3968(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_10+160(FP), Y11
	VMOVDQU high_10+4000(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_12+192(FP), Y11
	VMOVDQU high_12+4032(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_14+224(FP), Y11
	VMOVDQU high_14+4064(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_16+256(FP), Y11
	VMOVDQU high_16+4096(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_18+288(FP), Y11
	VMOVDQU high_18+4128(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 1 to 10 outputs
	VMOVDQU (DX)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_20+320(FP), Y11
	VMOVDQU high_20+4160(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_22+352(FP), Y11
	VMOVDQU high_22+4192(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_24+384(FP), Y11
	VMOVDQU high_24+4224(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_26+416(FP), Y11
	VMOVDQU high_26+4256(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_28+448(FP), Y11
	VMOVDQU high_28+4288(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_30+480(FP), Y11
	VMOVDQU high_30+4320(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_32+512(FP), Y11
	VMOVDQU high_32+4352(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_34+544(FP), Y11
	VMOVDQU high_34+4384(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_36+576(FP), Y11
	VMOVDQU high_36+4416(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_38+608(FP), Y11
	VMOVDQU high_38+4448(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 2 to 10 outputs
	VMOVDQU (BX)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_40+640(FP), Y11
	VMOVDQU high_40+4480(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_42+672(FP), Y11
	VMOVDQU high_42+4512(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_44+704(FP), Y11
	VMOVDQU high_44+4544(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_46+736(FP), Y11
	VMOVDQU high_46+4576(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_48+768(FP), Y11
	VMOVDQU high_48+4608(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_50+800(FP), Y11
	VMOVDQU high_50+4640(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_52+832(FP), Y11
	VMOVDQU high_52+4672(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_54+864(FP), Y11
	VMOVDQU high_54+4704(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_56+896(FP), Y11
	VMOVDQU high_56+4736(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_58+928(FP), Y11
	VMOVDQU high_58+4768(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 3 to 10 outputs
	VMOVDQU (BP)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_60+960(FP), Y11
	VMOVDQU high_60+4800(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_62+992(FP), Y11
	VMOVDQU high_62+4832(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_64+1024(FP), Y11
	VMOVDQU high_64+4864(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_66+1056(FP), Y11
	VMOVDQU high_66+4896(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_68+1088(FP), Y11
	VMOVDQU high_68+4928(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_70+1120(FP), Y11
	VMOVDQU high_70+4960(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_72+1152(FP), Y11
	VMOVDQU high_72+4992(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_74+1184(FP), Y11
	VMOVDQU high_74+5024(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_76+1216(FP), Y11
	VMOVDQU high_76+5056(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_78+1248(FP), Y11
	VMOVDQU high_78+5088(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 4 to 10 outputs
	VMOVDQU (SI)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_80+1280(FP), Y11
	VMOVDQU high_80+5120(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_82+1312(FP), Y11
	VMOVDQU high_82+5152(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_84+1344(FP), Y11
	VMOVDQU high_84+5184(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_86+1376(FP), Y11
	VMOVDQU high_86+5216(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_88+1408(FP), Y11
	VMOVDQU high_88+5248(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_90+1440(FP), Y11
	VMOVDQU high_90+5280(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_92+1472(FP), Y11
	VMOVDQU high_92+5312(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_94+1504(FP), Y11
	VMOVDQU high_94+5344(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_96+1536(FP), Y11
	VMOVDQU high_96+5376(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_98+1568(FP), Y11
	VMOVDQU high_98+5408(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 5 to 10 outputs
	VMOVDQU (DI)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_100+1600(FP), Y11
	VMOVDQU high_100+5440(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_102+1632(FP), Y11
	VMOVDQU high_102+5472(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_104+1664(FP), Y11
	VMOVDQU high_104+5504(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_106+1696(FP), Y11
	VMOVDQU high_106+5536(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_108+1728(FP), Y11
	VMOVDQU high_108+5568(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_110+1760(FP), Y11
	VMOVDQU high_110+5600(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_112+1792(FP), Y11
	VMOVDQU high_112+5632(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_114+1824(FP), Y11
	VMOVDQU high_114+5664(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_116+1856(FP), Y11
	VMOVDQU high_116+5696(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_118+1888(FP), Y11
	VMOVDQU high_118+5728(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 6 to 10 outputs
	VMOVDQU (R8)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_120+1920(FP), Y11
	VMOVDQU high_120+5760(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_122+1952(FP), Y11
	VMOVDQU high_122+5792(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_124+1984(FP), Y11
	VMOVDQU high_124+5824(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_126+2016(FP), Y11
	VMOVDQU high_126+5856(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_128+2048(FP), Y11
	VMOVDQU high_128+5888(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_130+2080(FP), Y11
	VMOVDQU high_130+5920(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_132+2112(FP), Y11
	VMOVDQU high_132+5952(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_134+2144(FP), Y11
	VMOVDQU high_134+5984(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_136+2176(FP), Y11
	VMOVDQU high_136+6016(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_138+2208(FP), Y11
	VMOVDQU high_138+6048(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 7 to 10 outputs
	VMOVDQU (R9)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_140+2240(FP), Y11
	VMOVDQU high_140+6080(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_142+2272(FP), Y11
	VMOVDQU high_142+6112(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_144+2304(FP), Y11
	VMOVDQU high_144+6144(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_146+2336(FP), Y11
	VMOVDQU high_146+6176(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_148+2368(FP), Y11
	VMOVDQU high_148+6208(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_150+2400(FP), Y11
	VMOVDQU high_150+6240(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_152+2432(FP), Y11
	VMOVDQU high_152+6272(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_154+2464(FP), Y11
	VMOVDQU high_154+6304(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_156+2496(FP), Y11
	VMOVDQU high_156+6336(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_158+2528(FP), Y11
	VMOVDQU high_158+6368(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 8 to 10 outputs
	VMOVDQU (R10)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_160+2560(FP), Y11
	VMOVDQU high_160+6400(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_162+2592(FP), Y11
	VMOVDQU high_162+6432(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_164+2624(FP), Y11
	VMOVDQU high_164+6464(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_166+2656(FP), Y11
	VMOVDQU high_166+6496(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_168+2688(FP), Y11
	VMOVDQU high_168+6528(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_170+2720(FP), Y11
	VMOVDQU high_170+6560(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_172+2752(FP), Y11
	VMOVDQU high_172+6592(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_174+2784(FP), Y11
	VMOVDQU high_174+6624(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_176+2816(FP), Y11
	VMOVDQU high_176+6656(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_178+2848(FP), Y11
	VMOVDQU high_178+6688(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 9 to 10 outputs
	VMOVDQU (R11)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_180+2880(FP), Y11
	VMOVDQU high_180+6720(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_182+2912(FP), Y11
	VMOVDQU high_182+6752(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_184+2944(FP), Y11
	VMOVDQU high_184+6784(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_186+2976(FP), Y11
	VMOVDQU high_186+6816(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_188+3008(FP), Y11
	VMOVDQU high_188+6848(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_190+3040(FP), Y11
	VMOVDQU high_190+6880(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_192+3072(FP), Y11
	VMOVDQU high_192+6912(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_194+3104(FP), Y11
	VMOVDQU high_194+6944(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_196+3136(FP), Y11
	VMOVDQU high_196+6976(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_198+3168(FP), Y11
	VMOVDQU high_198+7008(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 10 to 10 outputs
	VMOVDQU (R12)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_200+3200(FP), Y11
	VMOVDQU high_200+7040(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_202+3232(FP), Y11
	VMOVDQU high_202+7072(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_204+3264(FP), Y11
	VMOVDQU high_204+7104(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_206+3296(FP), Y11
	VMOVDQU high_206+7136(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_208+3328(FP), Y11
	VMOVDQU high_208+7168(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_210+3360(FP), Y11
	VMOVDQU high_210+7200(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_212+3392(FP), Y11
	VMOVDQU high_212+7232(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_214+3424(FP), Y11
	VMOVDQU high_214+7264(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_216+3456(FP), Y11
	VMOVDQU high_216+7296(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_218+3488(FP), Y11
	VMOVDQU high_218+7328(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Load and process 32 bytes from input 11 to 10 outputs
	VMOVDQU (R13)(R14*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VMOVDQU low_220+3520(FP), Y11
	VMOVDQU high_220+7360(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VMOVDQU low_222+3552(FP), Y11
	VMOVDQU high_222+7392(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1
	VMOVDQU low_224+3584(FP), Y11
	VMOVDQU high_224+7424(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y2, Y2
	VMOVDQU low_226+3616(FP), Y11
	VMOVDQU high_226+7456(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y3, Y3
	VMOVDQU low_228+3648(FP), Y11
	VMOVDQU high_228+7488(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y4, Y4
	VMOVDQU low_230+3680(FP), Y11
	VMOVDQU high_230+7520(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y5, Y5
	VMOVDQU low_232+3712(FP), Y11
	VMOVDQU high_232+7552(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y6, Y6
	VMOVDQU low_234+3744(FP), Y11
	VMOVDQU high_234+7584(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y7, Y7
	VMOVDQU low_236+3776(FP), Y11
	VMOVDQU high_236+7616(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y8, Y8
	VMOVDQU low_238+3808(FP), Y11
	VMOVDQU high_238+7648(FP), Y12
	VPSHUFB Y13, Y11, Y11
	VPSHUFB Y14, Y12, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y9, Y9

	// Store 10 outputs
	MOVQ    out_0_base+7968(FP), R15
	VMOVDQU Y0, (R15)(R14*1)
	MOVQ    out_1_base+7992(FP), R15
	VMOVDQU Y1, (R15)(R14*1)
	MOVQ    out_2_base+8016(FP), R15
	VMOVDQU Y2, (R15)(R14*1)
	MOVQ    out_3_base+8040(FP), R15
	VMOVDQU Y3, (R15)(R14*1)
	MOVQ    out_4_base+8064(FP), R15
	VMOVDQU Y4, (R15)(R14*1)
	MOVQ    out_5_base+8088(FP), R15
	VMOVDQU Y5, (R15)(R14*1)
	MOVQ    out_6_base+8112(FP), R15
	VMOVDQU Y6, (R15)(R14*1)
	MOVQ    out_7_base+8136(FP), R15
	VMOVDQU Y7, (R15)(R14*1)
	MOVQ    out_8_base+8160(FP), R15
	VMOVDQU Y8, (R15)(R14*1)
	MOVQ    out_9_base+8184(FP), R15
	VMOVDQU Y9, (R15)(R14*1)

	// Prepare for next loop
	ADDQ $0x20, R14
	DECQ AX
	JNZ  mulAvxTwo_12x10_loop
	VZEROUPPER

mulAvxTwo_12x10_end:
	RET
