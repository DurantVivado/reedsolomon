// Code generated by command: go run gen.go -out galois_gen_amd64.s -stubs galois_gen_amd64.go. DO NOT EDIT.

// +build !appengine
// +build !noasm
// +build gc

// func mulAvxTwoXor_1x1(low [1][16]byte, high [1][16]byte, in [1][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_1x1(SB), $0-80
	// Full registers estimated 6 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+40(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_1x1_end
	MOVQ         out_0_base+56(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+16(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVQ         in_0_base+32(FP), DX
	MOVQ         $0x0000000f, BX
	MOVQ         BX, X3
	VPBROADCASTB X3, Y3
	XORQ         BX, BX

mulAvxTwoXor_1x1_loop:
	// Load 1 outputs
	VMOVDQU (CX)(BX*1), Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(BX*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y3, Y4, Y4
	VPAND   Y3, Y5, Y5
	VPSHUFB Y4, Y1, Y4
	VPSHUFB Y5, Y2, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(BX*1)

	// Prepare for next loop
	ADDQ $0x20, BX
	DECQ AX
	JNZ  mulAvxTwoXor_1x1_loop
	VZEROUPPER

mulAvxTwoXor_1x1_end:
	RET

// func mulAvxTwo_1x1(low [1][16]byte, high [1][16]byte, in [1][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x1(SB), $0-80
	// Full registers estimated 6 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+40(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x1_end
	MOVQ         out_0_base+56(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+16(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVQ         in_0_base+32(FP), DX
	MOVQ         $0x0000000f, BX
	MOVQ         BX, X3
	VPBROADCASTB X3, Y3
	XORQ         BX, BX

mulAvxTwo_1x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(BX*1), Y4
	VPSRLQ  $0x04, Y4, Y5
	VPAND   Y3, Y4, Y4
	VPAND   Y3, Y5, Y5
	VPSHUFB Y4, Y1, Y4
	VPSHUFB Y5, Y2, Y5
	VPXOR   Y4, Y5, Y4
	VPXOR   Y4, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(BX*1)

	// Prepare for next loop
	ADDQ $0x20, BX
	DECQ AX
	JNZ  mulAvxTwo_1x1_loop
	VZEROUPPER

mulAvxTwo_1x1_end:
	RET

// func mulAvxTwoXor_1x2(low [2][16]byte, high [2][16]byte, in [1][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_1x2(SB), $0-136
	// Full registers estimated 11 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+72(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_1x2_end
	MOVQ         out_0_base+88(FP), CX
	MOVQ         out_1_base+112(FP), DX
	MOVOU        low_0+0(FP), X2
	MOVOU        high_0+32(FP), X3
	VINSERTI128  $0x01, X2, Y2, Y2
	VINSERTI128  $0x01, X3, Y3, Y3
	MOVOU        low_1+16(FP), X4
	MOVOU        high_1+48(FP), X5
	VINSERTI128  $0x01, X4, Y4, Y4
	VINSERTI128  $0x01, X5, Y5, Y5
	MOVQ         in_0_base+64(FP), BX
	MOVQ         $0x0000000f, BP
	MOVQ         BP, X6
	VPBROADCASTB X6, Y6
	XORQ         BP, BP

mulAvxTwoXor_1x2_loop:
	// Load 2 outputs
	VMOVDQU (CX)(BP*1), Y0
	VMOVDQU (DX)(BP*1), Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(BP*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VPSHUFB Y9, Y2, Y7
	VPSHUFB Y10, Y3, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VPSHUFB Y9, Y4, Y7
	VPSHUFB Y10, Y5, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(BP*1)
	VMOVDQU Y1, (DX)(BP*1)

	// Prepare for next loop
	ADDQ $0x20, BP
	DECQ AX
	JNZ  mulAvxTwoXor_1x2_loop
	VZEROUPPER

mulAvxTwoXor_1x2_end:
	RET

// func mulAvxTwo_1x2(low [2][16]byte, high [2][16]byte, in [1][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x2(SB), $0-136
	// Full registers estimated 11 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+72(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x2_end
	MOVQ         out_0_base+88(FP), CX
	MOVQ         out_1_base+112(FP), DX
	MOVOU        low_0+0(FP), X2
	MOVOU        high_0+32(FP), X3
	VINSERTI128  $0x01, X2, Y2, Y2
	VINSERTI128  $0x01, X3, Y3, Y3
	MOVOU        low_1+16(FP), X4
	MOVOU        high_1+48(FP), X5
	VINSERTI128  $0x01, X4, Y4, Y4
	VINSERTI128  $0x01, X5, Y5, Y5
	MOVQ         in_0_base+64(FP), BX
	MOVQ         $0x0000000f, BP
	MOVQ         BP, X6
	VPBROADCASTB X6, Y6
	XORQ         BP, BP

mulAvxTwo_1x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(BP*1), Y9
	VPSRLQ  $0x04, Y9, Y10
	VPAND   Y6, Y9, Y9
	VPAND   Y6, Y10, Y10
	VPSHUFB Y9, Y2, Y7
	VPSHUFB Y10, Y3, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y0, Y0
	VPSHUFB Y9, Y4, Y7
	VPSHUFB Y10, Y5, Y8
	VPXOR   Y7, Y8, Y7
	VPXOR   Y7, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(BP*1)
	VMOVDQU Y1, (DX)(BP*1)

	// Prepare for next loop
	ADDQ $0x20, BP
	DECQ AX
	JNZ  mulAvxTwo_1x2_loop
	VZEROUPPER

mulAvxTwo_1x2_end:
	RET

// func mulAvxTwoXor_1x3(low [3][16]byte, high [3][16]byte, in [1][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_1x3(SB), $0-192
	// Full registers estimated 14 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_1x3_end
	MOVQ         out_0_base+120(FP), CX
	MOVQ         out_1_base+144(FP), DX
	MOVQ         out_2_base+168(FP), BX
	MOVOU        low_0+0(FP), X3
	MOVOU        high_0+48(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_1+16(FP), X5
	MOVOU        high_1+64(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_2+32(FP), X7
	MOVOU        high_2+80(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVQ         in_0_base+96(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X9
	VPBROADCASTB X9, Y9
	XORQ         SI, SI

mulAvxTwoXor_1x3_loop:
	// Load 3 outputs
	VMOVDQU (CX)(SI*1), Y0
	VMOVDQU (DX)(SI*1), Y1
	VMOVDQU (BX)(SI*1), Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(SI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VPSHUFB Y12, Y3, Y10
	VPSHUFB Y13, Y4, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VPSHUFB Y12, Y5, Y10
	VPSHUFB Y13, Y6, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VPSHUFB Y12, Y7, Y10
	VPSHUFB Y13, Y8, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(SI*1)
	VMOVDQU Y1, (DX)(SI*1)
	VMOVDQU Y2, (BX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwoXor_1x3_loop
	VZEROUPPER

mulAvxTwoXor_1x3_end:
	RET

// func mulAvxTwo_1x3(low [3][16]byte, high [3][16]byte, in [1][]byte, out [3][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_1x3(SB), $0-192
	// Full registers estimated 14 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_1x3_end
	MOVQ         out_0_base+120(FP), CX
	MOVQ         out_1_base+144(FP), DX
	MOVQ         out_2_base+168(FP), BX
	MOVOU        low_0+0(FP), X3
	MOVOU        high_0+48(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_1+16(FP), X5
	MOVOU        high_1+64(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_2+32(FP), X7
	MOVOU        high_2+80(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVQ         in_0_base+96(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X9
	VPBROADCASTB X9, Y9
	XORQ         SI, SI

mulAvxTwo_1x3_loop:
	// Clear 3 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1
	VPXOR Y2, Y2, Y2

	// Load and process 32 bytes from input 0 to 3 outputs
	VMOVDQU (BP)(SI*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y9, Y12, Y12
	VPAND   Y9, Y13, Y13
	VPSHUFB Y12, Y3, Y10
	VPSHUFB Y13, Y4, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0
	VPSHUFB Y12, Y5, Y10
	VPSHUFB Y13, Y6, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y1, Y1
	VPSHUFB Y12, Y7, Y10
	VPSHUFB Y13, Y8, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y2, Y2

	// Store 3 outputs
	VMOVDQU Y0, (CX)(SI*1)
	VMOVDQU Y1, (DX)(SI*1)
	VMOVDQU Y2, (BX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_1x3_loop
	VZEROUPPER

mulAvxTwo_1x3_end:
	RET

// func mulAvxTwoXor_2x1(low [2][16]byte, high [2][16]byte, in [2][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_2x1(SB), $0-136
	// Full registers estimated 8 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+72(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_2x1_end
	MOVQ         out_0_base+112(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+32(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+48(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVQ         in_0_base+64(FP), DX
	MOVQ         in_1_base+88(FP), BX
	MOVQ         $0x0000000f, BP
	MOVQ         BP, X5
	VPBROADCASTB X5, Y5
	XORQ         BP, BP

mulAvxTwoXor_2x1_loop:
	// Load 1 outputs
	VMOVDQU (CX)(BP*1), Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(BP*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y5, Y6, Y6
	VPAND   Y5, Y7, Y7
	VPSHUFB Y6, Y1, Y6
	VPSHUFB Y7, Y2, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(BP*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y5, Y6, Y6
	VPAND   Y5, Y7, Y7
	VPSHUFB Y6, Y3, Y6
	VPSHUFB Y7, Y4, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(BP*1)

	// Prepare for next loop
	ADDQ $0x20, BP
	DECQ AX
	JNZ  mulAvxTwoXor_2x1_loop
	VZEROUPPER

mulAvxTwoXor_2x1_end:
	RET

// func mulAvxTwo_2x1(low [2][16]byte, high [2][16]byte, in [2][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x1(SB), $0-136
	// Full registers estimated 8 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+72(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x1_end
	MOVQ         out_0_base+112(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+32(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+48(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVQ         in_0_base+64(FP), DX
	MOVQ         in_1_base+88(FP), BX
	MOVQ         $0x0000000f, BP
	MOVQ         BP, X5
	VPBROADCASTB X5, Y5
	XORQ         BP, BP

mulAvxTwo_2x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(BP*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y5, Y6, Y6
	VPAND   Y5, Y7, Y7
	VPSHUFB Y6, Y1, Y6
	VPSHUFB Y7, Y2, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(BP*1), Y6
	VPSRLQ  $0x04, Y6, Y7
	VPAND   Y5, Y6, Y6
	VPAND   Y5, Y7, Y7
	VPSHUFB Y6, Y3, Y6
	VPSHUFB Y7, Y4, Y7
	VPXOR   Y6, Y7, Y6
	VPXOR   Y6, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(BP*1)

	// Prepare for next loop
	ADDQ $0x20, BP
	DECQ AX
	JNZ  mulAvxTwo_2x1_loop
	VZEROUPPER

mulAvxTwo_2x1_end:
	RET

// func mulAvxTwoXor_2x2(low [4][16]byte, high [4][16]byte, in [2][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_2x2(SB), $0-224
	// Full registers estimated 15 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+136(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_2x2_end
	MOVQ         out_0_base+176(FP), CX
	MOVQ         out_1_base+200(FP), DX
	MOVOU        low_0+0(FP), X2
	MOVOU        high_0+64(FP), X3
	VINSERTI128  $0x01, X2, Y2, Y2
	VINSERTI128  $0x01, X3, Y3, Y3
	MOVOU        low_1+16(FP), X4
	MOVOU        high_1+80(FP), X5
	VINSERTI128  $0x01, X4, Y4, Y4
	VINSERTI128  $0x01, X5, Y5, Y5
	MOVOU        low_2+32(FP), X6
	MOVOU        high_2+96(FP), X7
	VINSERTI128  $0x01, X6, Y6, Y6
	VINSERTI128  $0x01, X7, Y7, Y7
	MOVOU        low_3+48(FP), X8
	MOVOU        high_3+112(FP), X9
	VINSERTI128  $0x01, X8, Y8, Y8
	VINSERTI128  $0x01, X9, Y9, Y9
	MOVQ         in_0_base+128(FP), BX
	MOVQ         in_1_base+152(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X10
	VPBROADCASTB X10, Y10
	XORQ         SI, SI

mulAvxTwoXor_2x2_loop:
	// Load 2 outputs
	VMOVDQU (CX)(SI*1), Y0
	VMOVDQU (DX)(SI*1), Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VPSHUFB Y13, Y2, Y11
	VPSHUFB Y14, Y3, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VPSHUFB Y13, Y4, Y11
	VPSHUFB Y14, Y5, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VPSHUFB Y13, Y6, Y11
	VPSHUFB Y14, Y7, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VPSHUFB Y13, Y8, Y11
	VPSHUFB Y14, Y9, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(SI*1)
	VMOVDQU Y1, (DX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwoXor_2x2_loop
	VZEROUPPER

mulAvxTwoXor_2x2_end:
	RET

// func mulAvxTwo_2x2(low [4][16]byte, high [4][16]byte, in [2][]byte, out [2][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_2x2(SB), $0-224
	// Full registers estimated 15 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+136(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_2x2_end
	MOVQ         out_0_base+176(FP), CX
	MOVQ         out_1_base+200(FP), DX
	MOVOU        low_0+0(FP), X2
	MOVOU        high_0+64(FP), X3
	VINSERTI128  $0x01, X2, Y2, Y2
	VINSERTI128  $0x01, X3, Y3, Y3
	MOVOU        low_1+16(FP), X4
	MOVOU        high_1+80(FP), X5
	VINSERTI128  $0x01, X4, Y4, Y4
	VINSERTI128  $0x01, X5, Y5, Y5
	MOVOU        low_2+32(FP), X6
	MOVOU        high_2+96(FP), X7
	VINSERTI128  $0x01, X6, Y6, Y6
	VINSERTI128  $0x01, X7, Y7, Y7
	MOVOU        low_3+48(FP), X8
	MOVOU        high_3+112(FP), X9
	VINSERTI128  $0x01, X8, Y8, Y8
	VINSERTI128  $0x01, X9, Y9, Y9
	MOVQ         in_0_base+128(FP), BX
	MOVQ         in_1_base+152(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X10
	VPBROADCASTB X10, Y10
	XORQ         SI, SI

mulAvxTwo_2x2_loop:
	// Clear 2 outputs
	VPXOR Y0, Y0, Y0
	VPXOR Y1, Y1, Y1

	// Load and process 32 bytes from input 0 to 2 outputs
	VMOVDQU (BX)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VPSHUFB Y13, Y2, Y11
	VPSHUFB Y14, Y3, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VPSHUFB Y13, Y4, Y11
	VPSHUFB Y14, Y5, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1

	// Load and process 32 bytes from input 1 to 2 outputs
	VMOVDQU (BP)(SI*1), Y13
	VPSRLQ  $0x04, Y13, Y14
	VPAND   Y10, Y13, Y13
	VPAND   Y10, Y14, Y14
	VPSHUFB Y13, Y6, Y11
	VPSHUFB Y14, Y7, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y0, Y0
	VPSHUFB Y13, Y8, Y11
	VPSHUFB Y14, Y9, Y12
	VPXOR   Y11, Y12, Y11
	VPXOR   Y11, Y1, Y1

	// Store 2 outputs
	VMOVDQU Y0, (CX)(SI*1)
	VMOVDQU Y1, (DX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_2x2_loop
	VZEROUPPER

mulAvxTwo_2x2_end:
	RET

// func mulAvxTwoXor_3x1(low [3][16]byte, high [3][16]byte, in [3][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_3x1(SB), $0-192
	// Full registers estimated 10 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_3x1_end
	MOVQ         out_0_base+168(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+48(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+64(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+80(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVQ         in_0_base+96(FP), DX
	MOVQ         in_1_base+120(FP), BX
	MOVQ         in_2_base+144(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X7
	VPBROADCASTB X7, Y7
	XORQ         SI, SI

mulAvxTwoXor_3x1_loop:
	// Load 1 outputs
	VMOVDQU (CX)(SI*1), Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y1, Y8
	VPSHUFB Y9, Y2, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y3, Y8
	VPSHUFB Y9, Y4, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y5, Y8
	VPSHUFB Y9, Y6, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwoXor_3x1_loop
	VZEROUPPER

mulAvxTwoXor_3x1_end:
	RET

// func mulAvxTwo_3x1(low [3][16]byte, high [3][16]byte, in [3][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_3x1(SB), $0-192
	// Full registers estimated 10 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+104(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_3x1_end
	MOVQ         out_0_base+168(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+48(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+64(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+80(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVQ         in_0_base+96(FP), DX
	MOVQ         in_1_base+120(FP), BX
	MOVQ         in_2_base+144(FP), BP
	MOVQ         $0x0000000f, SI
	MOVQ         SI, X7
	VPBROADCASTB X7, Y7
	XORQ         SI, SI

mulAvxTwo_3x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y1, Y8
	VPSHUFB Y9, Y2, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y3, Y8
	VPSHUFB Y9, Y4, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(SI*1), Y8
	VPSRLQ  $0x04, Y8, Y9
	VPAND   Y7, Y8, Y8
	VPAND   Y7, Y9, Y9
	VPSHUFB Y8, Y5, Y8
	VPSHUFB Y9, Y6, Y9
	VPXOR   Y8, Y9, Y8
	VPXOR   Y8, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(SI*1)

	// Prepare for next loop
	ADDQ $0x20, SI
	DECQ AX
	JNZ  mulAvxTwo_3x1_loop
	VZEROUPPER

mulAvxTwo_3x1_end:
	RET

// func mulAvxTwoXor_4x1(low [4][16]byte, high [4][16]byte, in [4][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_4x1(SB), $0-248
	// Full registers estimated 12 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+136(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_4x1_end
	MOVQ         out_0_base+224(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+64(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+80(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+96(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+112(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVQ         in_0_base+128(FP), DX
	MOVQ         in_1_base+152(FP), BX
	MOVQ         in_2_base+176(FP), BP
	MOVQ         in_3_base+200(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X9
	VPBROADCASTB X9, Y9
	XORQ         DI, DI

mulAvxTwoXor_4x1_loop:
	// Load 1 outputs
	VMOVDQU (CX)(DI*1), Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y1, Y10
	VPSHUFB Y11, Y2, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y3, Y10
	VPSHUFB Y11, Y4, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y5, Y10
	VPSHUFB Y11, Y6, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y7, Y10
	VPSHUFB Y11, Y8, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwoXor_4x1_loop
	VZEROUPPER

mulAvxTwoXor_4x1_end:
	RET

// func mulAvxTwo_4x1(low [4][16]byte, high [4][16]byte, in [4][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_4x1(SB), $0-248
	// Full registers estimated 12 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+136(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_4x1_end
	MOVQ         out_0_base+224(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+64(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+80(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+96(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+112(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVQ         in_0_base+128(FP), DX
	MOVQ         in_1_base+152(FP), BX
	MOVQ         in_2_base+176(FP), BP
	MOVQ         in_3_base+200(FP), SI
	MOVQ         $0x0000000f, DI
	MOVQ         DI, X9
	VPBROADCASTB X9, Y9
	XORQ         DI, DI

mulAvxTwo_4x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y1, Y10
	VPSHUFB Y11, Y2, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y3, Y10
	VPSHUFB Y11, Y4, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y5, Y10
	VPSHUFB Y11, Y6, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(DI*1), Y10
	VPSRLQ  $0x04, Y10, Y11
	VPAND   Y9, Y10, Y10
	VPAND   Y9, Y11, Y11
	VPSHUFB Y10, Y7, Y10
	VPSHUFB Y11, Y8, Y11
	VPXOR   Y10, Y11, Y10
	VPXOR   Y10, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(DI*1)

	// Prepare for next loop
	ADDQ $0x20, DI
	DECQ AX
	JNZ  mulAvxTwo_4x1_loop
	VZEROUPPER

mulAvxTwo_4x1_end:
	RET

// func mulAvxTwoXor_5x1(low [5][16]byte, high [5][16]byte, in [5][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_5x1(SB), $0-304
	// Full registers estimated 14 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+168(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_5x1_end
	MOVQ         out_0_base+280(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+80(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+96(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+112(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+128(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVOU        low_4+64(FP), X9
	MOVOU        high_4+144(FP), X10
	VINSERTI128  $0x01, X9, Y9, Y9
	VINSERTI128  $0x01, X10, Y10, Y10
	MOVQ         in_0_base+160(FP), DX
	MOVQ         in_1_base+184(FP), BX
	MOVQ         in_2_base+208(FP), BP
	MOVQ         in_3_base+232(FP), SI
	MOVQ         in_4_base+256(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X11
	VPBROADCASTB X11, Y11
	XORQ         R8, R8

mulAvxTwoXor_5x1_loop:
	// Load 1 outputs
	VMOVDQU (CX)(R8*1), Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y1, Y12
	VPSHUFB Y13, Y2, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y3, Y12
	VPSHUFB Y13, Y4, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y5, Y12
	VPSHUFB Y13, Y6, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y7, Y12
	VPSHUFB Y13, Y8, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y9, Y12
	VPSHUFB Y13, Y10, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwoXor_5x1_loop
	VZEROUPPER

mulAvxTwoXor_5x1_end:
	RET

// func mulAvxTwo_5x1(low [5][16]byte, high [5][16]byte, in [5][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_5x1(SB), $0-304
	// Full registers estimated 14 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+168(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_5x1_end
	MOVQ         out_0_base+280(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+80(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+96(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+112(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+128(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVOU        low_4+64(FP), X9
	MOVOU        high_4+144(FP), X10
	VINSERTI128  $0x01, X9, Y9, Y9
	VINSERTI128  $0x01, X10, Y10, Y10
	MOVQ         in_0_base+160(FP), DX
	MOVQ         in_1_base+184(FP), BX
	MOVQ         in_2_base+208(FP), BP
	MOVQ         in_3_base+232(FP), SI
	MOVQ         in_4_base+256(FP), DI
	MOVQ         $0x0000000f, R8
	MOVQ         R8, X11
	VPBROADCASTB X11, Y11
	XORQ         R8, R8

mulAvxTwo_5x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y1, Y12
	VPSHUFB Y13, Y2, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y3, Y12
	VPSHUFB Y13, Y4, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y5, Y12
	VPSHUFB Y13, Y6, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y7, Y12
	VPSHUFB Y13, Y8, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R8*1), Y12
	VPSRLQ  $0x04, Y12, Y13
	VPAND   Y11, Y12, Y12
	VPAND   Y11, Y13, Y13
	VPSHUFB Y12, Y9, Y12
	VPSHUFB Y13, Y10, Y13
	VPXOR   Y12, Y13, Y12
	VPXOR   Y12, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R8*1)

	// Prepare for next loop
	ADDQ $0x20, R8
	DECQ AX
	JNZ  mulAvxTwo_5x1_loop
	VZEROUPPER

mulAvxTwo_5x1_end:
	RET

// func mulAvxTwoXor_6x1(low [6][16]byte, high [6][16]byte, in [6][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwoXor_6x1(SB), $0-360
	// Full registers estimated 16 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+200(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwoXor_6x1_end
	MOVQ         out_0_base+336(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+96(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+112(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+128(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+144(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVOU        low_4+64(FP), X9
	MOVOU        high_4+160(FP), X10
	VINSERTI128  $0x01, X9, Y9, Y9
	VINSERTI128  $0x01, X10, Y10, Y10
	MOVOU        low_5+80(FP), X11
	MOVOU        high_5+176(FP), X12
	VINSERTI128  $0x01, X11, Y11, Y11
	VINSERTI128  $0x01, X12, Y12, Y12
	MOVQ         in_0_base+192(FP), DX
	MOVQ         in_1_base+216(FP), BX
	MOVQ         in_2_base+240(FP), BP
	MOVQ         in_3_base+264(FP), SI
	MOVQ         in_4_base+288(FP), DI
	MOVQ         in_5_base+312(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X13
	VPBROADCASTB X13, Y13
	XORQ         R9, R9

mulAvxTwoXor_6x1_loop:
	// Load 1 outputs
	VMOVDQU (CX)(R9*1), Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y1, Y14
	VPSHUFB Y15, Y2, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y3, Y14
	VPSHUFB Y15, Y4, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y5, Y14
	VPSHUFB Y15, Y6, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y7, Y14
	VPSHUFB Y15, Y8, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y9, Y14
	VPSHUFB Y15, Y10, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y11, Y14
	VPSHUFB Y15, Y12, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwoXor_6x1_loop
	VZEROUPPER

mulAvxTwoXor_6x1_end:
	RET

// func mulAvxTwo_6x1(low [6][16]byte, high [6][16]byte, in [6][]byte, out [1][]byte)
// Requires: AVX, AVX2, SSE2
TEXT ·mulAvxTwo_6x1(SB), $0-360
	// Full registers estimated 16 YMM used
	// Load all tables to registers
	MOVQ         in_0_len+200(FP), AX
	SHRQ         $0x05, AX
	TESTQ        AX, AX
	JZ           mulAvxTwo_6x1_end
	MOVQ         out_0_base+336(FP), CX
	MOVOU        low_0+0(FP), X1
	MOVOU        high_0+96(FP), X2
	VINSERTI128  $0x01, X1, Y1, Y1
	VINSERTI128  $0x01, X2, Y2, Y2
	MOVOU        low_1+16(FP), X3
	MOVOU        high_1+112(FP), X4
	VINSERTI128  $0x01, X3, Y3, Y3
	VINSERTI128  $0x01, X4, Y4, Y4
	MOVOU        low_2+32(FP), X5
	MOVOU        high_2+128(FP), X6
	VINSERTI128  $0x01, X5, Y5, Y5
	VINSERTI128  $0x01, X6, Y6, Y6
	MOVOU        low_3+48(FP), X7
	MOVOU        high_3+144(FP), X8
	VINSERTI128  $0x01, X7, Y7, Y7
	VINSERTI128  $0x01, X8, Y8, Y8
	MOVOU        low_4+64(FP), X9
	MOVOU        high_4+160(FP), X10
	VINSERTI128  $0x01, X9, Y9, Y9
	VINSERTI128  $0x01, X10, Y10, Y10
	MOVOU        low_5+80(FP), X11
	MOVOU        high_5+176(FP), X12
	VINSERTI128  $0x01, X11, Y11, Y11
	VINSERTI128  $0x01, X12, Y12, Y12
	MOVQ         in_0_base+192(FP), DX
	MOVQ         in_1_base+216(FP), BX
	MOVQ         in_2_base+240(FP), BP
	MOVQ         in_3_base+264(FP), SI
	MOVQ         in_4_base+288(FP), DI
	MOVQ         in_5_base+312(FP), R8
	MOVQ         $0x0000000f, R9
	MOVQ         R9, X13
	VPBROADCASTB X13, Y13
	XORQ         R9, R9

mulAvxTwo_6x1_loop:
	// Clear 1 outputs
	VPXOR Y0, Y0, Y0

	// Load and process 32 bytes from input 0 to 1 outputs
	VMOVDQU (DX)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y1, Y14
	VPSHUFB Y15, Y2, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 1 to 1 outputs
	VMOVDQU (BX)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y3, Y14
	VPSHUFB Y15, Y4, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 2 to 1 outputs
	VMOVDQU (BP)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y5, Y14
	VPSHUFB Y15, Y6, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 3 to 1 outputs
	VMOVDQU (SI)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y7, Y14
	VPSHUFB Y15, Y8, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 4 to 1 outputs
	VMOVDQU (DI)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y9, Y14
	VPSHUFB Y15, Y10, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Load and process 32 bytes from input 5 to 1 outputs
	VMOVDQU (R8)(R9*1), Y14
	VPSRLQ  $0x04, Y14, Y15
	VPAND   Y13, Y14, Y14
	VPAND   Y13, Y15, Y15
	VPSHUFB Y14, Y11, Y14
	VPSHUFB Y15, Y12, Y15
	VPXOR   Y14, Y15, Y14
	VPXOR   Y14, Y0, Y0

	// Store 1 outputs
	VMOVDQU Y0, (CX)(R9*1)

	// Prepare for next loop
	ADDQ $0x20, R9
	DECQ AX
	JNZ  mulAvxTwo_6x1_loop
	VZEROUPPER

mulAvxTwo_6x1_end:
	RET
